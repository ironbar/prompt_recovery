{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LLM Prompt Recovery Recover the prompt used to transform a given text Link to the page of the challenge Docs https://ironbar.github.io/prompt_recovery Code structure |_ prometeo: library with code developed for the challenge |_ forum: all the scripts and notebooks taken from the forum |_ logs: folder for storing all kind of stats and logs. For example the score of each model, results from experiments |_ notebooks: jupyter notebooks made during the challenge. They start by number for easier sorting. |_ reports: documents made during the challenge according to CRISP-DM methodology |_ tests: folder with tests for the library |_ data: folder with light data from the challenge |_ rules: the official rules of the challenge |_ scripts: scripts made during the challenge for training, data processing...","title":"LLM Prompt Recovery"},{"location":"#llm-prompt-recovery","text":"Recover the prompt used to transform a given text Link to the page of the challenge","title":"LLM Prompt Recovery"},{"location":"#docs","text":"https://ironbar.github.io/prompt_recovery","title":"Docs"},{"location":"#code-structure","text":"|_ prometeo: library with code developed for the challenge |_ forum: all the scripts and notebooks taken from the forum |_ logs: folder for storing all kind of stats and logs. For example the score of each model, results from experiments |_ notebooks: jupyter notebooks made during the challenge. They start by number for easier sorting. |_ reports: documents made during the challenge according to CRISP-DM methodology |_ tests: folder with tests for the library |_ data: folder with light data from the challenge |_ rules: the official rules of the challenge |_ scripts: scripts made during the challenge for training, data processing...","title":"Code structure"},{"location":"01_Business_Understanding/","text":"Business Understanding Challenge description The challenge is to guess the prompt that was used to transform a text, given the original and the transformed text. The texts were rewritten using Gemma 7b-it-quant LLM, so we can use it to create training data. The test dataset has around 1300 samples and it is hidden, we have to submit code that runs in less than 9 hours . That gives less than 25s to make each inference. Evaluation For each row in the submission and corresponding ground truth, sentence-t5-base is used to calculate corresponding embedding vectors. The score for each predicted / expected pair is calculated using the Sharpened Cosine Similarity , using an exponent of 3. The SCS is used to attenuate the generous score given by embedding vectors for incorrect answers. Do not leave any rewrite_prompt blank as null answers will throw an error. So they compute the cosine similarity of the embedding of the ground truth and the prediction, and that value is elevated to 3. We have to maximize the similarity between the predictions and the ground truth. In the forum it has been said that st5-base is not a good model. Assess situation The timeline is tight, just a month for this challenge. It does not give too much time to try things. I'm also participating on Drivendata's Pose bowl competition . That competition ends on May 14 so there is an extra month and I should apply most of my focus to Kaggle. This competition is a good opportunity to gain hands-on experience with LLMs. I'm going to devote a week to the challenge to learn and if my score is promising I would consider devoting more time. The week of Monday 18 March. Terminology EOS token : End of sentence token GGUF format : GGUF is a file format for storing models for inference with GGML and executors based on GGML. GGUF is a binary format that is designed for fast loading and saving of models, and for ease of reading. GPTQ : Accurate Post-Training Quantization for Generative Pre-trained Transformers. It is a a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. It is implemented on AutoGPTQ and it seems to be used by TheBloke QLoRA : Efficient Finetuning of Quantized LLMs. It quantizes the original model to 4bits and trains a LoRA. The paper claims that achieves state of the art results while requiring much less memory. Nous-Hermes-Llama2-13b : is a state-of-the-art language model fine-tuned on over 300,000 instructions. I have read that it is the state of the art for that size of model. DPO Direct Preference Optimization is a method to fine-tune a model to learn preferences of some output over other output. Goal-conditioned imitation learning Flash attention : Faster Attention with Better Parallelism and Work Partitioning. Flash attention is orthogonal to QLoRA, meaning that you can combine the two. PEFT : Parameter efficient fine-tuning Prefix tuning : Prefix tuning prefixes a series of task-specific vectors to the input sequence that can be learned while keeping the pretrained model frozen. The prefix parameters are inserted in all of the model layers. Special tokens: EOS . End of sentence. UNK . Unknown token PAD . Add this token to have all the inputs in the batch with the same length. BOS . Begin of sentence Questions Does the prompt include the original text? It does not seem to be the case according to this experiment . Making a submission with the original text only received a LB score of 0.38, while submitting a simple prompt such as Improve the text to this. received a score of 0.6. Does rewrite_prompt contain original_text? Forum What is the advantage of using custom tokens for retraining? I know they are used for converting LLM to assistants. If I'm fine-tuning an assistant does it have sense to add new tokens? My intuition says that if I use an specific prompt when fine-tuning the model it should be comparable to adding new tokens but simpler. Project Plan Premises I have to use the biggest model possible. As a rule of thumb larger LLM models give better results. Thus I have to use the biggest model possible that allows to make a successful submission. Fine-tuning is likely to give better results over just prompt engineering. Given the model information about the evaluation metrics is likely to be helpful. It could be as simple as conditioning the generation on the score by giving the desired score in the prompt. Or could it be something more advanced like DPO, reinforcement learning or weighted loss. Plan Find the biggest model that could be used for the submission. Make a few submissions using prompt engineering. Basic fine-tuning of the model on public data Evaluate on public data Submission At this point the next steps could involve: Generate more training data with Gemma Try more advanced training methods that use information from the evaluation metrics","title":"Business Understanding"},{"location":"01_Business_Understanding/#business-understanding","text":"","title":"Business Understanding"},{"location":"01_Business_Understanding/#challenge-description","text":"The challenge is to guess the prompt that was used to transform a text, given the original and the transformed text. The texts were rewritten using Gemma 7b-it-quant LLM, so we can use it to create training data. The test dataset has around 1300 samples and it is hidden, we have to submit code that runs in less than 9 hours . That gives less than 25s to make each inference.","title":"Challenge description"},{"location":"01_Business_Understanding/#evaluation","text":"For each row in the submission and corresponding ground truth, sentence-t5-base is used to calculate corresponding embedding vectors. The score for each predicted / expected pair is calculated using the Sharpened Cosine Similarity , using an exponent of 3. The SCS is used to attenuate the generous score given by embedding vectors for incorrect answers. Do not leave any rewrite_prompt blank as null answers will throw an error. So they compute the cosine similarity of the embedding of the ground truth and the prediction, and that value is elevated to 3. We have to maximize the similarity between the predictions and the ground truth. In the forum it has been said that st5-base is not a good model.","title":"Evaluation"},{"location":"01_Business_Understanding/#assess-situation","text":"The timeline is tight, just a month for this challenge. It does not give too much time to try things. I'm also participating on Drivendata's Pose bowl competition . That competition ends on May 14 so there is an extra month and I should apply most of my focus to Kaggle. This competition is a good opportunity to gain hands-on experience with LLMs. I'm going to devote a week to the challenge to learn and if my score is promising I would consider devoting more time. The week of Monday 18 March.","title":"Assess situation"},{"location":"01_Business_Understanding/#terminology","text":"EOS token : End of sentence token GGUF format : GGUF is a file format for storing models for inference with GGML and executors based on GGML. GGUF is a binary format that is designed for fast loading and saving of models, and for ease of reading. GPTQ : Accurate Post-Training Quantization for Generative Pre-trained Transformers. It is a a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. It is implemented on AutoGPTQ and it seems to be used by TheBloke QLoRA : Efficient Finetuning of Quantized LLMs. It quantizes the original model to 4bits and trains a LoRA. The paper claims that achieves state of the art results while requiring much less memory. Nous-Hermes-Llama2-13b : is a state-of-the-art language model fine-tuned on over 300,000 instructions. I have read that it is the state of the art for that size of model. DPO Direct Preference Optimization is a method to fine-tune a model to learn preferences of some output over other output. Goal-conditioned imitation learning Flash attention : Faster Attention with Better Parallelism and Work Partitioning. Flash attention is orthogonal to QLoRA, meaning that you can combine the two. PEFT : Parameter efficient fine-tuning Prefix tuning : Prefix tuning prefixes a series of task-specific vectors to the input sequence that can be learned while keeping the pretrained model frozen. The prefix parameters are inserted in all of the model layers. Special tokens: EOS . End of sentence. UNK . Unknown token PAD . Add this token to have all the inputs in the batch with the same length. BOS . Begin of sentence","title":"Terminology"},{"location":"01_Business_Understanding/#questions","text":"","title":"Questions"},{"location":"01_Business_Understanding/#does-the-prompt-include-the-original-text","text":"It does not seem to be the case according to this experiment . Making a submission with the original text only received a LB score of 0.38, while submitting a simple prompt such as Improve the text to this. received a score of 0.6. Does rewrite_prompt contain original_text? Forum","title":"Does the prompt include the original text?"},{"location":"01_Business_Understanding/#what-is-the-advantage-of-using-custom-tokens-for-retraining","text":"I know they are used for converting LLM to assistants. If I'm fine-tuning an assistant does it have sense to add new tokens? My intuition says that if I use an specific prompt when fine-tuning the model it should be comparable to adding new tokens but simpler.","title":"What is the advantage of using custom tokens for retraining?"},{"location":"01_Business_Understanding/#project-plan","text":"","title":"Project Plan"},{"location":"01_Business_Understanding/#premises","text":"I have to use the biggest model possible. As a rule of thumb larger LLM models give better results. Thus I have to use the biggest model possible that allows to make a successful submission. Fine-tuning is likely to give better results over just prompt engineering. Given the model information about the evaluation metrics is likely to be helpful. It could be as simple as conditioning the generation on the score by giving the desired score in the prompt. Or could it be something more advanced like DPO, reinforcement learning or weighted loss.","title":"Premises"},{"location":"01_Business_Understanding/#plan","text":"Find the biggest model that could be used for the submission. Make a few submissions using prompt engineering. Basic fine-tuning of the model on public data Evaluate on public data Submission At this point the next steps could involve: Generate more training data with Gemma Try more advanced training methods that use information from the evaluation metrics","title":"Plan"},{"location":"02_Data_Understanding/","text":"Data Understanding Collect initial data No official data is provided for this challenge. External data On this post there is a collection of generated data. Particularly interesting is a dataset created by the host for testing and accidentally leaked. He says it is very different from the hidden one.","title":"Data Understanding"},{"location":"02_Data_Understanding/#data-understanding","text":"","title":"Data Understanding"},{"location":"02_Data_Understanding/#collect-initial-data","text":"No official data is provided for this challenge.","title":"Collect initial data"},{"location":"02_Data_Understanding/#external-data","text":"On this post there is a collection of generated data. Particularly interesting is a dataset created by the host for testing and accidentally leaked. He says it is very different from the hidden one.","title":"External data"},{"location":"05_Solution_Summary/","text":"Solution Summary This was my first hands-on experience with LLMs so I'm grateful to Kaggle and Google for giving me the incentive to do so. It was a fun competition although I was not able to find a way to consistently improve my LB score. Almost all the things I tried failed. It turned out that the black magic was an adversarial attack against the T5 embedding model. Solution in a nutshell The solution is an ensemble of fine-tuned version with LoRA of the following models: model Mistral 7B Llama 2 13B Mistral 22B Mistral 8x7B The models were fine-tuned on a small dataset of around 1000k samples created with GPT4. submission public LB private LB MMML v1 0.64 0.65 MMML v1 + 0.63 prompt 0.66 0.67 The predictions of the models were concatenated and the 0.63 public prompt was also appended. Links Web with all the work done Github repo Submission notebook Solution presentation slides Youtube video What worked? Making multiple predictions Contrary to some beliefs expressed in the forum that argued that making long predictions was hurtful for similarity score, I found that simply making n predictions and concatenating them improved the LB score. For example, I could improve the score from 0.62 to 0.64 simply by doing 4 inferences with v5 model. Fine-tuning on my own data Fine-tuning LLMs on my own data returned a LB score of 0.62 . By comparison the best score obtained fine-tuning on public data was 0.60 . Few-shot prompt This worked surprisingly well. Depending on the given samples, it was possible to score up to 0.62 on THE leaderboard. However, the inference was slow due to having such a big input prompt. Fine-tuning a model resulted on a much faster inference that allowed for multiple inferences. Mean prompts Making a submission with Please improve this text using the writing style with maintaining the original meaning but altering the tone. returned a score of 0.63 . This is probably a combination of using a poor chosen metric with a dumb model and the power of optimization of Kaggle, where 2k people make 5 submissions a day. But it is possible that the mean prompt will work as well on the hidden test set, so I believe I should make an inference with and without the mean prompt. What didn't work? Validation dataset, what is the task to learn? The lack of any validation data make very difficult to design and improve a solution for the challenge. I could only rely on a small public dataset with rounded precision. At the start of the challenge I tried using some datasets for validation, but did not find a clear correlation between validation results and leaderboard. Bigger models At the start of the challenge I thought that Mixtral was going to make a difference in the challenge. In reality, I have not seen any significative difference between Mistral, Mixtral or Llama. Public datasets The best LB score obtained when fine-tuning on public data was 0.60 , but most of the datasets scored around 0.53 Trying to improve my data I tried different strategies to improve my data: Multi-instruction prompts Imitate leaked prompts Prompts on more topics as suggested by GPT4 Added hints to datasets None of them gave improvements on the leaderboard score. Training with similar prompts I tried augmenting the train data using similar prompts: prompts that had the same meaning and scored high on T5 embeddings It didn't improve the LB score. Scaling the data I did not get any improvement after generating 2k new samples using GPT4. Using GPT4 to find a better mean prompt Maybe I did not do enough attempts (around 8) but GPT4 was unable to find a better mean prompt given the public available mean prompts and its scores. Criticism There is ambiguity in the task of recovering a prompt. Multiple prompts can lead to the same output. Prompts could be generic or specific and lead to the same outcome. F.e. Rewrite the sentence using a different word choice. and Rewrite the sentence replacing beautiful by cute . Thus, choosing similarity as the challenge metric was likely a poor choice. Instead, perplexity could have been used, although at the cost of higher computational cost. Learnings I have learned to use and fine-tune LLMs I have learned that it is possible to quantize LLMs to float4 and they still work very well I have learned that it is possible to do an adversarial attack against a text embedding model","title":"Solution Summary"},{"location":"05_Solution_Summary/#solution-summary","text":"This was my first hands-on experience with LLMs so I'm grateful to Kaggle and Google for giving me the incentive to do so. It was a fun competition although I was not able to find a way to consistently improve my LB score. Almost all the things I tried failed. It turned out that the black magic was an adversarial attack against the T5 embedding model.","title":"Solution Summary"},{"location":"05_Solution_Summary/#solution-in-a-nutshell","text":"The solution is an ensemble of fine-tuned version with LoRA of the following models: model Mistral 7B Llama 2 13B Mistral 22B Mistral 8x7B The models were fine-tuned on a small dataset of around 1000k samples created with GPT4. submission public LB private LB MMML v1 0.64 0.65 MMML v1 + 0.63 prompt 0.66 0.67 The predictions of the models were concatenated and the 0.63 public prompt was also appended.","title":"Solution in a nutshell"},{"location":"05_Solution_Summary/#links","text":"Web with all the work done Github repo Submission notebook Solution presentation slides Youtube video","title":"Links"},{"location":"05_Solution_Summary/#what-worked","text":"","title":"What worked?"},{"location":"05_Solution_Summary/#making-multiple-predictions","text":"Contrary to some beliefs expressed in the forum that argued that making long predictions was hurtful for similarity score, I found that simply making n predictions and concatenating them improved the LB score. For example, I could improve the score from 0.62 to 0.64 simply by doing 4 inferences with v5 model.","title":"Making multiple predictions"},{"location":"05_Solution_Summary/#fine-tuning-on-my-own-data","text":"Fine-tuning LLMs on my own data returned a LB score of 0.62 . By comparison the best score obtained fine-tuning on public data was 0.60 .","title":"Fine-tuning on my own data"},{"location":"05_Solution_Summary/#few-shot-prompt","text":"This worked surprisingly well. Depending on the given samples, it was possible to score up to 0.62 on THE leaderboard. However, the inference was slow due to having such a big input prompt. Fine-tuning a model resulted on a much faster inference that allowed for multiple inferences.","title":"Few-shot prompt"},{"location":"05_Solution_Summary/#mean-prompts","text":"Making a submission with Please improve this text using the writing style with maintaining the original meaning but altering the tone. returned a score of 0.63 . This is probably a combination of using a poor chosen metric with a dumb model and the power of optimization of Kaggle, where 2k people make 5 submissions a day. But it is possible that the mean prompt will work as well on the hidden test set, so I believe I should make an inference with and without the mean prompt.","title":"Mean prompts"},{"location":"05_Solution_Summary/#what-didnt-work","text":"","title":"What didn't work?"},{"location":"05_Solution_Summary/#validation-dataset-what-is-the-task-to-learn","text":"The lack of any validation data make very difficult to design and improve a solution for the challenge. I could only rely on a small public dataset with rounded precision. At the start of the challenge I tried using some datasets for validation, but did not find a clear correlation between validation results and leaderboard.","title":"Validation dataset, what is the task to learn?"},{"location":"05_Solution_Summary/#bigger-models","text":"At the start of the challenge I thought that Mixtral was going to make a difference in the challenge. In reality, I have not seen any significative difference between Mistral, Mixtral or Llama.","title":"Bigger models"},{"location":"05_Solution_Summary/#public-datasets","text":"The best LB score obtained when fine-tuning on public data was 0.60 , but most of the datasets scored around 0.53","title":"Public datasets"},{"location":"05_Solution_Summary/#trying-to-improve-my-data","text":"I tried different strategies to improve my data: Multi-instruction prompts Imitate leaked prompts Prompts on more topics as suggested by GPT4 Added hints to datasets None of them gave improvements on the leaderboard score.","title":"Trying to improve my data"},{"location":"05_Solution_Summary/#training-with-similar-prompts","text":"I tried augmenting the train data using similar prompts: prompts that had the same meaning and scored high on T5 embeddings It didn't improve the LB score.","title":"Training with similar prompts"},{"location":"05_Solution_Summary/#scaling-the-data","text":"I did not get any improvement after generating 2k new samples using GPT4.","title":"Scaling the data"},{"location":"05_Solution_Summary/#using-gpt4-to-find-a-better-mean-prompt","text":"Maybe I did not do enough attempts (around 8) but GPT4 was unable to find a better mean prompt given the public available mean prompts and its scores.","title":"Using GPT4 to find a better mean prompt"},{"location":"05_Solution_Summary/#criticism","text":"There is ambiguity in the task of recovering a prompt. Multiple prompts can lead to the same output. Prompts could be generic or specific and lead to the same outcome. F.e. Rewrite the sentence using a different word choice. and Rewrite the sentence replacing beautiful by cute . Thus, choosing similarity as the challenge metric was likely a poor choice. Instead, perplexity could have been used, although at the cost of higher computational cost.","title":"Criticism"},{"location":"05_Solution_Summary/#learnings","text":"I have learned to use and fine-tune LLMs I have learned that it is possible to quantize LLMs to float4 and they still work very well I have learned that it is possible to do an adversarial attack against a text embedding model","title":"Learnings"},{"location":"06_Winning_Model_Documentation/","text":"Winning model documentation Winning Model Documentation Guidelines A. MODEL SUMMARY A1. Background on you/your team Competition Name: Team Name: Private Leaderboard Score: Private Leaderboard Place: Name: Guillermo Barbadillo Location: Pamplona, SPAIN Email: guilllermobarbadillo@gmail.com A2. Background on you/your team What is your academic/professional background? Did you have any prior experience that helped you succeed in this competition? What made you decide to enter this competition? How much time did you spend on the competition? If part of a team, how did you decide to team up? If you competed as part of a team, who did what? A3. Summary A4. Features Selection / Engineering What were the most important features? How did you select features? Did you make any important feature transformations? Did you find any interesting interactions between features? Did you use external data? (if permitted) A5. Training Method(s) What training methods did you use? Did you ensemble the models? If you did ensemble, how did you weight the different models? A6. Interesting findings What was the most important trick you used? What do you think set you apart from others in the competition? Did you find any interesting relationships in the data that don't fit in the sections above? A7. Simple Features and Methods A8. Model Execution Time How long does it take to train your model? How long does it take to generate predictions using your model? How long does it take to train the simplified model (referenced in section A6)? How long does it take to generate predictions from the simplified model? A9. References B. SUBMISSION MODEL B1. All code, data, and your trained model goes in a single archive B2. README.md B3. Configuration files B4. requirements.txt B5. directory_structure.txt B6. SETTINGS.json B7. Serialized copy of the trained model B8. entry_points.md","title":"Winning model documentation"},{"location":"06_Winning_Model_Documentation/#winning-model-documentation","text":"Winning Model Documentation Guidelines","title":"Winning model documentation"},{"location":"06_Winning_Model_Documentation/#a-model-summary","text":"","title":"A. MODEL SUMMARY"},{"location":"06_Winning_Model_Documentation/#a1-background-on-youyour-team","text":"Competition Name: Team Name: Private Leaderboard Score: Private Leaderboard Place: Name: Guillermo Barbadillo Location: Pamplona, SPAIN Email: guilllermobarbadillo@gmail.com","title":"A1. Background on you/your team"},{"location":"06_Winning_Model_Documentation/#a2-background-on-youyour-team","text":"","title":"A2. Background on you/your team"},{"location":"06_Winning_Model_Documentation/#what-is-your-academicprofessional-background","text":"","title":"What is your academic/professional background?"},{"location":"06_Winning_Model_Documentation/#did-you-have-any-prior-experience-that-helped-you-succeed-in-this-competition","text":"","title":"Did you have any prior experience that helped you succeed in this competition?"},{"location":"06_Winning_Model_Documentation/#what-made-you-decide-to-enter-this-competition","text":"","title":"What made you decide to enter this competition?"},{"location":"06_Winning_Model_Documentation/#how-much-time-did-you-spend-on-the-competition","text":"","title":"How much time did you spend on the competition?"},{"location":"06_Winning_Model_Documentation/#if-part-of-a-team-how-did-you-decide-to-team-up","text":"","title":"If part of a team, how did you decide to team up?"},{"location":"06_Winning_Model_Documentation/#if-you-competed-as-part-of-a-team-who-did-what","text":"","title":"If you competed as part of a team, who did what?"},{"location":"06_Winning_Model_Documentation/#a3-summary","text":"","title":"A3. Summary"},{"location":"06_Winning_Model_Documentation/#a4-features-selection-engineering","text":"","title":"A4. Features Selection / Engineering"},{"location":"06_Winning_Model_Documentation/#what-were-the-most-important-features","text":"","title":"What were the most important features?"},{"location":"06_Winning_Model_Documentation/#how-did-you-select-features","text":"","title":"How did you select features?"},{"location":"06_Winning_Model_Documentation/#did-you-make-any-important-feature-transformations","text":"","title":"Did you make any important feature transformations?"},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-interactions-between-features","text":"","title":"Did you find any interesting interactions between features?"},{"location":"06_Winning_Model_Documentation/#did-you-use-external-data-if-permitted","text":"","title":"Did you use external data? (if permitted)"},{"location":"06_Winning_Model_Documentation/#a5-training-methods","text":"","title":"A5. Training Method(s)"},{"location":"06_Winning_Model_Documentation/#what-training-methods-did-you-use","text":"","title":"What training methods did you use?"},{"location":"06_Winning_Model_Documentation/#did-you-ensemble-the-models","text":"","title":"Did you ensemble the models?"},{"location":"06_Winning_Model_Documentation/#if-you-did-ensemble-how-did-you-weight-the-different-models","text":"","title":"If you did ensemble, how did you weight the different models?"},{"location":"06_Winning_Model_Documentation/#a6-interesting-findings","text":"","title":"A6. Interesting findings"},{"location":"06_Winning_Model_Documentation/#what-was-the-most-important-trick-you-used","text":"","title":"What was the most important trick you used?"},{"location":"06_Winning_Model_Documentation/#what-do-you-think-set-you-apart-from-others-in-the-competition","text":"","title":"What do you think set you apart from others in the competition?"},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-relationships-in-the-data-that-dont-fit-in-the-sections-above","text":"","title":"Did you find any interesting relationships in the data that don't fit in the sections above?"},{"location":"06_Winning_Model_Documentation/#a7-simple-features-and-methods","text":"","title":"A7. Simple Features and Methods"},{"location":"06_Winning_Model_Documentation/#a8-model-execution-time","text":"","title":"A8. Model Execution Time"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-your-model","text":"","title":"How long does it take to train your model?"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-using-your-model","text":"","title":"How long does it take to generate predictions using your model?"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-the-simplified-model-referenced-in-section-a6","text":"","title":"How long does it take to train the simplified model (referenced in section A6)?"},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-from-the-simplified-model","text":"","title":"How long does it take to generate predictions from the simplified model?"},{"location":"06_Winning_Model_Documentation/#a9-references","text":"","title":"A9. References"},{"location":"06_Winning_Model_Documentation/#b-submission-model","text":"","title":"B. SUBMISSION MODEL"},{"location":"06_Winning_Model_Documentation/#b1-all-code-data-and-your-trained-model-goes-in-a-single-archive","text":"","title":"B1. All code, data, and your trained model goes in a single archive"},{"location":"06_Winning_Model_Documentation/#b2-readmemd","text":"","title":"B2. README.md"},{"location":"06_Winning_Model_Documentation/#b3-configuration-files","text":"","title":"B3. Configuration files"},{"location":"06_Winning_Model_Documentation/#b4-requirementstxt","text":"","title":"B4. requirements.txt"},{"location":"06_Winning_Model_Documentation/#b5-directory_structuretxt","text":"","title":"B5. directory_structure.txt"},{"location":"06_Winning_Model_Documentation/#b6-settingsjson","text":"","title":"B6. SETTINGS.json"},{"location":"06_Winning_Model_Documentation/#b7-serialized-copy-of-the-trained-model","text":"","title":"B7. Serialized copy of the trained model"},{"location":"06_Winning_Model_Documentation/#b8-entry_pointsmd","text":"","title":"B8. entry_points.md"},{"location":"modeling/","text":"Modeling Select modeling technique Prompt engineering On a first step I should try using prompt engineering. We should get a good baseline just by asking an LLM to look at the original text, the rewritten text and ask what the prompt could have been. Also giving some examples could boost the scores even further. Model fine-tuning Being able to fine-tune the model using new tokens could teach the model to learn a new task. This has the potential to achieve even better results. Model inversion We already know which model was used to generate the text, can't we reverse the process to fill in the middle ? Generate experimentation design The lack of data makes evaluation very difficult. We don't know if our generated data follows a similar distribution to the test dataset. The public leaderboard uses only 15% of the data, so it's just 195 samples.","title":"Modeling"},{"location":"modeling/#modeling","text":"","title":"Modeling"},{"location":"modeling/#select-modeling-technique","text":"","title":"Select modeling technique"},{"location":"modeling/#prompt-engineering","text":"On a first step I should try using prompt engineering. We should get a good baseline just by asking an LLM to look at the original text, the rewritten text and ask what the prompt could have been. Also giving some examples could boost the scores even further.","title":"Prompt engineering"},{"location":"modeling/#model-fine-tuning","text":"Being able to fine-tune the model using new tokens could teach the model to learn a new task. This has the potential to achieve even better results.","title":"Model fine-tuning"},{"location":"modeling/#model-inversion","text":"We already know which model was used to generate the text, can't we reverse the process to fill in the middle ?","title":"Model inversion"},{"location":"modeling/#generate-experimentation-design","text":"The lack of data makes evaluation very difficult. We don't know if our generated data follows a similar distribution to the test dataset. The public leaderboard uses only 15% of the data, so it's just 195 samples.","title":"Generate experimentation design"},{"location":"modeling/Iteration_01_biggest_model/","text":"Iteration 1. Biggest model 18-03-2024 Goal What is the biggest model that can be used to make a submission? Motivation Scaling laws say that bigger models give better results. To be competitive we have to use the biggest model available for the challenge. Development Candidates study The most popular open-source models are: Mistral, Llama, Phi and Gemma. Since we want to use the biggest model possible that excludes Phi. Phi-2 model has just 2.7B parameters and the biggest Gemma model is 7B parameters. Mistral releases claim that the Mistral 7B model is better than Llama 2 13B model (and of course better than Llama 2 7B). If we trust those claims it won't have sense to use Llama 2 models. Mixtral is the best model but it has 56B parameters, that will fit very tightly on 32GB of VRAM memory. I have to test if I can make reliable predictions using Mixtral and wether I can fine-tune it. Google claims that Gemma is better than Mistral and Llama 2. The differences between Mistral 7B and Gemma 7B seem to be context dependent. In some contexts like math and code Gemma is better, on reasoning and real life scenarios Mistral seems to be better. If possible I should use Mixtral because it's the most powerful model available. If I'm unable to use Mixtral then I should go with Mistral 7B or Gemma 7B. Links: https://mistral.ai/news/announcing-mistral-7b/ https://mistral.ai/news/mixtral-of-experts/ https://blog.google/technology/developers/gemma-open-models/ https://www.ankursnewsletter.com/p/comparative-analysis-gemma-7b-vs First steps with Mixtral conda create -n prometeo pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 -y conda activate prometeo pip install autotransformers Downloading the model from Kaggle took around 2 hours, it is a 151 GB .tar.gz file. However inside it has two different formats, so the model ends up weighting around 93 GB, it seems that it is saved in float16 format. One trick was to copy the model to the SSD, I was able to read it in less than 1 minute, compared to 12 minutes in Kaggle and 24 minutes when reading from HDD. Just by creating the environment with the instructions above, and downloading the model from Kaggle I was able to run without trouble the model on my PC at a speed of 10 tokens/s. Prompt engineering with Gemma 2b I have been playing with Gemma 2b-it because it is fast enough to be able to make predictions with it. The problem is that the model is pretty dumb . Very frequently ignores the given instructions, so doing prompt engineering with the model is challenging. One option could be to divide the task in two: Create a list with the differences between the two texts Given the list of the differences summarize the differences into a prompt This is probably the chain of thought that a person will likely do to solve the problem. Results Mixtral can be used for inference I have made a few submissions with tiny changes in input formatting and generation parameters that scored 0.51 and 0.52. It's a pity that an LLM scores below a simple sentence like Improve this text , but the good thing is that now I know that it is possible to use Mixtral for inference. Thus Mixtral should be my preferred workhorse for this challenge. Unless I'm unable to finetune it I should use Mixtral until the end of the challenge. GPU Memory Input tokens Memory and inference time grow linearly with the input tokens. Output tokens In the scale of the tokens that we are going generate, output memory is constant and inference time scales linearly. Batch size The memory grows linearly with the batch size, as expected. Unless we use a very small input size on inference batching the inference won't be beneficial. Maximum submission input tokens When loading the model there is a device_map parameter that it is set to auto in the code samples. This results on an unbalanced memory usage between the GPUs as shown on the table below. device_map GPU 0 memory (GB) GPU 1 memory (GB) auto 10.3 13.2 create_shared_device_map(16) 11.8 11.8 create_intertwined_device_map() 11.8 11.8 It seems I can do inference reliably with up to 7200 input tokens . I needed to carefully balanced the layers of the model between the 2 gpus. With previous auto configuration only 3500 input tokens were allowed. Since I have been able to make a submission with the previous configuration that implies that none of the samples of the hidden test set had a higher input size of 3500 tokens. It is not clear if create_shared_device_map is faster than create_intertwined_device_map . The first one splits the model in two halfs so the GPU 0 does the first stage of the model and GPU 1 the second stage. The intertwined strategy assigns the layers alternatively to each GPU, thus it needs more communication between GPUs but it is likely that heat dissipation would be better. Mixtral has a maximum context window of 32k tokens, so we are very far from there. Which LLMs are fast enough to be used for inference? On a first step I tried different models, the table below shows the speed in tokens per second. LLM LMstudio Ubuntu LMstudio Windows Victor Windows P4 gpu GGUF 4 bits 2xP4 4 bits 1xP4 phi 2 3B q8 131 60 50 Gemma 3B it - 31 38 Llama 2 7B q8 76 37 25 7.9 10.4 mistral 7B q8 75.5 40 60 23 10 11.3 Gemma 7B it q8 - 17 30 15 Llama 2 13B 15 Mixtral 4.1 Mixtral 8x7B is about twice as slow as Mistral 7B despite having 8 times more parameters. That is the magic of sparse mixture of experts. Conclusion It is possible to make submissions using Mixtral. It is the biggest and most capable model that can be used for this challenge. I could use an input size up to 7200 tokens, which is around 400 lines or 4700 words. That seems a lot of room to play with prompt engineering a few shot prompting. Next steps TODO Which LLMs are fast enough can be used for inference? LLama 2 Mistral 7B Phi-2 Gemma Which speed can I get on my computer using lmstudio? Mixtral Can I make reliable inference with it? Can I fine-tune it? https://www.kaggle.com/code/ashishkumarak/mixtral-moe-8x7b-instruct-inference-t4-2-gpu Fine-tune Mixtral-8x7B on Your Computer (QLoRA) https://www.kaggle.com/models/mistral-ai/mixtral/frameworks/PyTorch/variations/8x7b-instruct-v0.1-hf/versions/1 Which dataset was used to fine-tune Guanaco? On Qlora paper it is said that it was fine-tuned in less than one day How can I make a submission with a HuggingFace model? How much could I improve the evaluation speed if using a more powerful GPU? Which LLMs I can finetune and use for inference? Fine-tuning de grandes modelos de lenguaje con Manuel Romero | Hackathon Somos NLP 2023 Est\u00e1 un poco anticuada porque es de hace un a\u00f1o pero la teor\u00eda est\u00e1 muy bien explicada. https://github.com/somosnlp/recursos/blob/main/hackathon_2024/entrenamiento_llm_instrucciones.ipynb","title":"Iteration 1. Biggest model"},{"location":"modeling/Iteration_01_biggest_model/#iteration-1-biggest-model","text":"18-03-2024","title":"Iteration 1. Biggest model"},{"location":"modeling/Iteration_01_biggest_model/#goal","text":"What is the biggest model that can be used to make a submission?","title":"Goal"},{"location":"modeling/Iteration_01_biggest_model/#motivation","text":"Scaling laws say that bigger models give better results. To be competitive we have to use the biggest model available for the challenge.","title":"Motivation"},{"location":"modeling/Iteration_01_biggest_model/#development","text":"","title":"Development"},{"location":"modeling/Iteration_01_biggest_model/#candidates-study","text":"The most popular open-source models are: Mistral, Llama, Phi and Gemma. Since we want to use the biggest model possible that excludes Phi. Phi-2 model has just 2.7B parameters and the biggest Gemma model is 7B parameters. Mistral releases claim that the Mistral 7B model is better than Llama 2 13B model (and of course better than Llama 2 7B). If we trust those claims it won't have sense to use Llama 2 models. Mixtral is the best model but it has 56B parameters, that will fit very tightly on 32GB of VRAM memory. I have to test if I can make reliable predictions using Mixtral and wether I can fine-tune it. Google claims that Gemma is better than Mistral and Llama 2. The differences between Mistral 7B and Gemma 7B seem to be context dependent. In some contexts like math and code Gemma is better, on reasoning and real life scenarios Mistral seems to be better. If possible I should use Mixtral because it's the most powerful model available. If I'm unable to use Mixtral then I should go with Mistral 7B or Gemma 7B. Links: https://mistral.ai/news/announcing-mistral-7b/ https://mistral.ai/news/mixtral-of-experts/ https://blog.google/technology/developers/gemma-open-models/ https://www.ankursnewsletter.com/p/comparative-analysis-gemma-7b-vs","title":"Candidates study"},{"location":"modeling/Iteration_01_biggest_model/#first-steps-with-mixtral","text":"conda create -n prometeo pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 -y conda activate prometeo pip install autotransformers Downloading the model from Kaggle took around 2 hours, it is a 151 GB .tar.gz file. However inside it has two different formats, so the model ends up weighting around 93 GB, it seems that it is saved in float16 format. One trick was to copy the model to the SSD, I was able to read it in less than 1 minute, compared to 12 minutes in Kaggle and 24 minutes when reading from HDD. Just by creating the environment with the instructions above, and downloading the model from Kaggle I was able to run without trouble the model on my PC at a speed of 10 tokens/s.","title":"First steps with Mixtral"},{"location":"modeling/Iteration_01_biggest_model/#prompt-engineering-with-gemma-2b","text":"I have been playing with Gemma 2b-it because it is fast enough to be able to make predictions with it. The problem is that the model is pretty dumb . Very frequently ignores the given instructions, so doing prompt engineering with the model is challenging. One option could be to divide the task in two: Create a list with the differences between the two texts Given the list of the differences summarize the differences into a prompt This is probably the chain of thought that a person will likely do to solve the problem.","title":"Prompt engineering with Gemma 2b"},{"location":"modeling/Iteration_01_biggest_model/#results","text":"","title":"Results"},{"location":"modeling/Iteration_01_biggest_model/#mixtral-can-be-used-for-inference","text":"I have made a few submissions with tiny changes in input formatting and generation parameters that scored 0.51 and 0.52. It's a pity that an LLM scores below a simple sentence like Improve this text , but the good thing is that now I know that it is possible to use Mixtral for inference. Thus Mixtral should be my preferred workhorse for this challenge. Unless I'm unable to finetune it I should use Mixtral until the end of the challenge.","title":"Mixtral can be used for inference"},{"location":"modeling/Iteration_01_biggest_model/#gpu-memory","text":"","title":"GPU Memory"},{"location":"modeling/Iteration_01_biggest_model/#input-tokens","text":"Memory and inference time grow linearly with the input tokens.","title":"Input tokens"},{"location":"modeling/Iteration_01_biggest_model/#output-tokens","text":"In the scale of the tokens that we are going generate, output memory is constant and inference time scales linearly.","title":"Output tokens"},{"location":"modeling/Iteration_01_biggest_model/#batch-size","text":"The memory grows linearly with the batch size, as expected. Unless we use a very small input size on inference batching the inference won't be beneficial.","title":"Batch size"},{"location":"modeling/Iteration_01_biggest_model/#maximum-submission-input-tokens","text":"When loading the model there is a device_map parameter that it is set to auto in the code samples. This results on an unbalanced memory usage between the GPUs as shown on the table below. device_map GPU 0 memory (GB) GPU 1 memory (GB) auto 10.3 13.2 create_shared_device_map(16) 11.8 11.8 create_intertwined_device_map() 11.8 11.8 It seems I can do inference reliably with up to 7200 input tokens . I needed to carefully balanced the layers of the model between the 2 gpus. With previous auto configuration only 3500 input tokens were allowed. Since I have been able to make a submission with the previous configuration that implies that none of the samples of the hidden test set had a higher input size of 3500 tokens. It is not clear if create_shared_device_map is faster than create_intertwined_device_map . The first one splits the model in two halfs so the GPU 0 does the first stage of the model and GPU 1 the second stage. The intertwined strategy assigns the layers alternatively to each GPU, thus it needs more communication between GPUs but it is likely that heat dissipation would be better. Mixtral has a maximum context window of 32k tokens, so we are very far from there.","title":"Maximum submission input tokens"},{"location":"modeling/Iteration_01_biggest_model/#which-llms-are-fast-enough-to-be-used-for-inference","text":"On a first step I tried different models, the table below shows the speed in tokens per second. LLM LMstudio Ubuntu LMstudio Windows Victor Windows P4 gpu GGUF 4 bits 2xP4 4 bits 1xP4 phi 2 3B q8 131 60 50 Gemma 3B it - 31 38 Llama 2 7B q8 76 37 25 7.9 10.4 mistral 7B q8 75.5 40 60 23 10 11.3 Gemma 7B it q8 - 17 30 15 Llama 2 13B 15 Mixtral 4.1 Mixtral 8x7B is about twice as slow as Mistral 7B despite having 8 times more parameters. That is the magic of sparse mixture of experts.","title":"Which LLMs are fast enough to be used for inference?"},{"location":"modeling/Iteration_01_biggest_model/#conclusion","text":"It is possible to make submissions using Mixtral. It is the biggest and most capable model that can be used for this challenge. I could use an input size up to 7200 tokens, which is around 400 lines or 4700 words. That seems a lot of room to play with prompt engineering a few shot prompting.","title":"Conclusion"},{"location":"modeling/Iteration_01_biggest_model/#next-steps","text":"","title":"Next steps"},{"location":"modeling/Iteration_01_biggest_model/#todo","text":"Which LLMs are fast enough can be used for inference? LLama 2 Mistral 7B Phi-2 Gemma Which speed can I get on my computer using lmstudio? Mixtral Can I make reliable inference with it? Can I fine-tune it? https://www.kaggle.com/code/ashishkumarak/mixtral-moe-8x7b-instruct-inference-t4-2-gpu Fine-tune Mixtral-8x7B on Your Computer (QLoRA) https://www.kaggle.com/models/mistral-ai/mixtral/frameworks/PyTorch/variations/8x7b-instruct-v0.1-hf/versions/1 Which dataset was used to fine-tune Guanaco? On Qlora paper it is said that it was fine-tuned in less than one day How can I make a submission with a HuggingFace model? How much could I improve the evaluation speed if using a more powerful GPU? Which LLMs I can finetune and use for inference? Fine-tuning de grandes modelos de lenguaje con Manuel Romero | Hackathon Somos NLP 2023 Est\u00e1 un poco anticuada porque es de hace un a\u00f1o pero la teor\u00eda est\u00e1 muy bien explicada. https://github.com/somosnlp/recursos/blob/main/hackathon_2024/entrenamiento_llm_instrucciones.ipynb","title":"TODO"},{"location":"modeling/Iteration_02_prompt_engineering/","text":"Iteration 2. Prompt engineering 21-03-2024 Goal Can we improve the baseline leaderboard score of 0.52 of Mixtral using prompt engineering? We will have to implement some validation and test evaluations to be able to do prompt engineering. Motivation We already know that Mixtral is going to be our workhorse for the challenge and that the input to the model should be below 7200 tokens. Now is the time to see how much we can improve simply with prompt engineering and few shot prompting. Development Cleaning the datasets I have downloaded 6 datasets and iteratively created code to clean the datasets. Most of the cleaning was done to remove Gemma starts that give hints of the prompt used. For example very frequently answers start by Sure, here is your rewritten text: . I also removed some of the samples where Gemma refused to answer. The process of finding the patterns to clean the data was done iteratively. I evaluated the datasets using a model and then looked at the worse and best predictions. Slow evaluation Evaluating the 6 datasets can take around 90 minutes. Thus prompt engineering is slow because if we want to measure small improvements we need that evaluation time. I could use Mistral API to accelerate evaluation. Assuming that each request would use around 2500 input tokens, each dataset evaluation (around 300 samples) would cost 0.7$, so the total evaluation in 6 datasets would be likely less than 4$. If I make 5 requests per second that would be around 6 minutes. Results First prompt engineering optimizations These results are previous to data cleaning. prompt gemma_suppl_rewrite leaderboard baseline 0.628 0.52 few shot markdown 0.691 0.58 few shot conversation 0.715 0.61 conversation + Improve the text to this - 0.63 Improve the text to this 0.617 0.6 Adding 9 examples and changing the prompt slightly was enough to boost the score ~0.09 Merging the prediction with the simple text of Improve the text to this improves the LB even further. With a template prompt of around 2k tokens the submission takes less than 6 hours. Number of output tokens number of tokens mean score 10 0.588 25 0.637 50 0.639 75 0.639 A small number of tokens is hurtful, but once a point is reached there is no point on allowing more output tokens. Second steps After data cleaning I have carried more thorough optimization: prompt variations gemma_suppl_rewrite nbroad_v2 newtonbaba dipamc77 alexxxsem galileo mean few shot conversation 0.698 0.609 0.672 0.634 0.574 0.647 0.639 add user and assistant names 0.694 0.606 0.656 0.61 0.572 0.632 0.628 more detailed intro 0.705 0.62 0.672 0.632 0.569 0.664 0.644 prompt instead of instruction 0.721 0.621 0.672 0.633 0.572 0.662 0.647 order instead of instruction 0.642 0.551 0.615 0.635 0.555 0.597 0.599 order instead of instruction v2 0.627 0.544 0.59 0.612 0.545 0.584 0.584 prompt and more detailed intro 0.717 0.623 0.679 0.65 0.581 0.674 0.654 Improve response start 0.724 0.623 0.68 0.647 0.577 0.665 0.653 Original and rewritten text in a single message 0.711 0.622 0.675 0.615 0.572 0.651 0.641 chatgpt intro 0.713 0.627 0.671 0.635 0.579 0.67 0.649 reorder the introduction 0.719 0.627 0.68 0.65 0.587 0.676 0.657 By adding a more detailed intro (better definition of the task) and replacing the word \"instruction\" by \"text prompt\" I was able to improve the mean scs score from 0.639 to 0.657 . prompt variations mean validation public lb public lb + 'Improve the text to this' few shot conversation 0.639 0.61 0.63 prompt and more detailed intro 0.654 0.6 0.62 reorder the introduction 0.657 0.6 0.62 However when making submissions the validation result did not correlate with leaderboard. Conclusion By using prompt engineering we were able to improve the leaderboard score from 0.52 to 0.63. We had to use the trick of adding Improve the text to this to the submission, which highlights the importance of imitating the style of the host's prompts. The correlation between current local validation score and public leaderboard is not perfect. However it might be the case that it is simply the uncertainty of the evaluation, that I believe is close to 0.02 for the public leaderboard. Next steps Is fine-tuning necessary for this problem? Which data should I use to finetune? Analyze the supplementary material Make a list of real rewrite scenarios, not made up ones Prepare more examples for few shot prompting, some of them must exploit the hints leave by Gemma. TODO Update the submission notebook to avoid loading the model when the test set is small and save GPU credits Which dataset I could use for validation? Set up a validation pipeline ~~Batch size speedup on inference~~ Does the order of the prompt has an effect on inference time? YES","title":"Iteration 2. Prompt engineering"},{"location":"modeling/Iteration_02_prompt_engineering/#iteration-2-prompt-engineering","text":"21-03-2024","title":"Iteration 2. Prompt engineering"},{"location":"modeling/Iteration_02_prompt_engineering/#goal","text":"Can we improve the baseline leaderboard score of 0.52 of Mixtral using prompt engineering? We will have to implement some validation and test evaluations to be able to do prompt engineering.","title":"Goal"},{"location":"modeling/Iteration_02_prompt_engineering/#motivation","text":"We already know that Mixtral is going to be our workhorse for the challenge and that the input to the model should be below 7200 tokens. Now is the time to see how much we can improve simply with prompt engineering and few shot prompting.","title":"Motivation"},{"location":"modeling/Iteration_02_prompt_engineering/#development","text":"","title":"Development"},{"location":"modeling/Iteration_02_prompt_engineering/#cleaning-the-datasets","text":"I have downloaded 6 datasets and iteratively created code to clean the datasets. Most of the cleaning was done to remove Gemma starts that give hints of the prompt used. For example very frequently answers start by Sure, here is your rewritten text: . I also removed some of the samples where Gemma refused to answer. The process of finding the patterns to clean the data was done iteratively. I evaluated the datasets using a model and then looked at the worse and best predictions.","title":"Cleaning the datasets"},{"location":"modeling/Iteration_02_prompt_engineering/#slow-evaluation","text":"Evaluating the 6 datasets can take around 90 minutes. Thus prompt engineering is slow because if we want to measure small improvements we need that evaluation time. I could use Mistral API to accelerate evaluation. Assuming that each request would use around 2500 input tokens, each dataset evaluation (around 300 samples) would cost 0.7$, so the total evaluation in 6 datasets would be likely less than 4$. If I make 5 requests per second that would be around 6 minutes.","title":"Slow evaluation"},{"location":"modeling/Iteration_02_prompt_engineering/#results","text":"","title":"Results"},{"location":"modeling/Iteration_02_prompt_engineering/#first-prompt-engineering-optimizations","text":"These results are previous to data cleaning. prompt gemma_suppl_rewrite leaderboard baseline 0.628 0.52 few shot markdown 0.691 0.58 few shot conversation 0.715 0.61 conversation + Improve the text to this - 0.63 Improve the text to this 0.617 0.6 Adding 9 examples and changing the prompt slightly was enough to boost the score ~0.09 Merging the prediction with the simple text of Improve the text to this improves the LB even further. With a template prompt of around 2k tokens the submission takes less than 6 hours.","title":"First prompt engineering optimizations"},{"location":"modeling/Iteration_02_prompt_engineering/#number-of-output-tokens","text":"number of tokens mean score 10 0.588 25 0.637 50 0.639 75 0.639 A small number of tokens is hurtful, but once a point is reached there is no point on allowing more output tokens.","title":"Number of output tokens"},{"location":"modeling/Iteration_02_prompt_engineering/#second-steps","text":"After data cleaning I have carried more thorough optimization: prompt variations gemma_suppl_rewrite nbroad_v2 newtonbaba dipamc77 alexxxsem galileo mean few shot conversation 0.698 0.609 0.672 0.634 0.574 0.647 0.639 add user and assistant names 0.694 0.606 0.656 0.61 0.572 0.632 0.628 more detailed intro 0.705 0.62 0.672 0.632 0.569 0.664 0.644 prompt instead of instruction 0.721 0.621 0.672 0.633 0.572 0.662 0.647 order instead of instruction 0.642 0.551 0.615 0.635 0.555 0.597 0.599 order instead of instruction v2 0.627 0.544 0.59 0.612 0.545 0.584 0.584 prompt and more detailed intro 0.717 0.623 0.679 0.65 0.581 0.674 0.654 Improve response start 0.724 0.623 0.68 0.647 0.577 0.665 0.653 Original and rewritten text in a single message 0.711 0.622 0.675 0.615 0.572 0.651 0.641 chatgpt intro 0.713 0.627 0.671 0.635 0.579 0.67 0.649 reorder the introduction 0.719 0.627 0.68 0.65 0.587 0.676 0.657 By adding a more detailed intro (better definition of the task) and replacing the word \"instruction\" by \"text prompt\" I was able to improve the mean scs score from 0.639 to 0.657 . prompt variations mean validation public lb public lb + 'Improve the text to this' few shot conversation 0.639 0.61 0.63 prompt and more detailed intro 0.654 0.6 0.62 reorder the introduction 0.657 0.6 0.62 However when making submissions the validation result did not correlate with leaderboard.","title":"Second steps"},{"location":"modeling/Iteration_02_prompt_engineering/#conclusion","text":"By using prompt engineering we were able to improve the leaderboard score from 0.52 to 0.63. We had to use the trick of adding Improve the text to this to the submission, which highlights the importance of imitating the style of the host's prompts. The correlation between current local validation score and public leaderboard is not perfect. However it might be the case that it is simply the uncertainty of the evaluation, that I believe is close to 0.02 for the public leaderboard.","title":"Conclusion"},{"location":"modeling/Iteration_02_prompt_engineering/#next-steps","text":"Is fine-tuning necessary for this problem? Which data should I use to finetune? Analyze the supplementary material Make a list of real rewrite scenarios, not made up ones Prepare more examples for few shot prompting, some of them must exploit the hints leave by Gemma.","title":"Next steps"},{"location":"modeling/Iteration_02_prompt_engineering/#todo","text":"Update the submission notebook to avoid loading the model when the test set is small and save GPU credits Which dataset I could use for validation? Set up a validation pipeline ~~Batch size speedup on inference~~ Does the order of the prompt has an effect on inference time? YES","title":"TODO"},{"location":"modeling/Iteration_03_study_leaked_data/","text":"Iteration 3. Study Leaked data 24-03-2024 Goal Extract all the possible information from leaked data. Motivation It is very surprising that a basic submission with Improve the text to this scores 0.60 on leaderboard while a naive submission with Mixtral model scores 0.52. This is likely caused by the ambiguity of the task: many different prompts can lead to the same output. The space of input texts and prompts is infinite. Thus the more information we can get about the data that the host team have created we will increase the chance of winning. Development Results Public information On a first step let's gather all the data available in the challenge. Overview On the overview of the challenge we can see some sample prompts: id,rewrite_prompt 000aaa,\"Rewrite this essay but do it using the writing style of Dr. Seuss\" 111bbb,\"Rewrite this essay but do it using the writing style of William Shakespeare\" 222ccc,\"Rewrite this essay but do it using the writing style of Tupac Shakur\" Is this a hint that in the test set some of the rewritten texts follow the style of some famous english authors? Sample submission id,rewrite_prompt 9559194,Improve that text. Very similar to the overview data. Is it a coincidence that the prompt is similar to Improve the text to this ? Train id,original_text,rewrite_prompt,rewritten_text -1,\"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text. Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\",\"Convert this into a sea shanty: \"\"\"\"\"\"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text. Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\"\"\"\"\"\"\",\"Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\" The train data is contradictory. The prompt has the original text inside. I have made a submission adding the original text and the score decreases from 0.63 to 0.51. It's a mistery why they decided to show a train example like that... Starter notebook rewrite_prompts = [ 'Explain this to me like I\\'m five.', 'Convert this into a sea shanty.', 'Make this rhyme.', ] All prompts rewrite_prompts = [ 'Explain this to me like I\\'m five.', 'Convert this into a sea shanty.', 'Make this rhyme.', 'Improve that text.', \"Rewrite this essay but do it using the writing style of Dr. Seuss\", \"Rewrite this essay but do it using the writing style of William Shakespeare\", \"Rewrite this essay but do it using the writing style of Tupac Shakur\", ] Leaked dataset https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/481811 Where do the original texts come from? I cannot find them using google. It is likely that they have been generated with an LLM. How are the original texts? Many memos Ladies and gentleman, announcements Many letters/emails and messages, they start by Dear, Hey, Subject: (Gmail) News that start by Title: Rewritten texts No text starts by Sure, here is...: . I believe this is a clear indication of postprocessing. There is some postprocessing but still some texts have hints about the received prompt, for example: Adjust, Rephrase, Reword, Rework, Rewrite, Summary, Update... There are even some failed generations that have the whole prompt: Update this email to make it more typical and incorporate the topic of \"incentive\". Begin the prompt with \"Update this email to...\". The new version should be shorter than 53 words long. Rework this text message to infuse it with a playful and humorous complaint, adding a touch of whimsy and lightness to the conversation: Rephrase this memo to evoke a sense of concern and incorporate the topic \"acre\", ensuring that the tone reflects worry and apprehension. If possible, start the prompt with \"Rephrase\".. The new version should be shorter than 53 words long. It is possible that they are using the same model to create the original text, the prompt and the rewritten text and the model was confused on the previous examples. Otherwise I don't understand orders such as start the prompt with \"Rephrase\" or Begin the prompt with \"Update this email to...\" My hyphotesis is that: The original text was generated with Gemma The prompt was also generated with Gemma, following some high level instructions from the host Obviosly the rewritten text is also generated with Gemma, as the challenge description says. Which kind of transformations are done? I cannot know the prompt, but I can study the kind of transformations. I believe I need the help of GPT4, my english level is not so good for many of the transformations. Where did the leak happened? Paul Mooney said: This dataset was originally created for testing purposes without the intention of making it public -- but I accidentally made it public for ~15 minutes in mid-January This is interesting because Gemma was announced on Feb 21. Info about Paul: https://www.kaggle.com/paultimothymooney https://www.linkedin.com/in/paultimothymooney/ No activity on LinkedIn https://github.com/paultimothymooney There are some private contributions in mid-January, which could be related the the message. https://twitter.com/ptimothymooney He's not very active in twitter. Searching in google for paultimothymooney in the range jan5-jan25 2024 I don't get anything useful. He has a lot of Kaggle code, these are some relevant notebooks: https://www.kaggle.com/code/paultimothymooney/generate-text-w-gemini-pro-in-a-kaggle-notebook https://www.kaggle.com/code/paultimothymooney/few-shot-prompting-w-gemini-pro-on-kaggle The few shot prompt is interesting because it shows a way to do it on Gemini, it is likely that Gemma will be similar. It has a dataset of instruction-following records. I could not find any additional information about the leak Conclusion The task of guessing the prompt is not easy. For a non-native speaker like me, many times is difficult because some unknown words are used and I'm not able to describe the difference. I need to use GPT4 to guess the prompt on a following iteration. Next steps Dr Seuss might be related with the word whimsical, I could make an example of a rule that whenever the word whimsical is seen the prompt should be Rewrite this essay but do it using the writing style of Dr. Seuss . TODO [ ]","title":"Iteration 3. Study Leaked data"},{"location":"modeling/Iteration_03_study_leaked_data/#iteration-3-study-leaked-data","text":"24-03-2024","title":"Iteration 3. Study Leaked data"},{"location":"modeling/Iteration_03_study_leaked_data/#goal","text":"Extract all the possible information from leaked data.","title":"Goal"},{"location":"modeling/Iteration_03_study_leaked_data/#motivation","text":"It is very surprising that a basic submission with Improve the text to this scores 0.60 on leaderboard while a naive submission with Mixtral model scores 0.52. This is likely caused by the ambiguity of the task: many different prompts can lead to the same output. The space of input texts and prompts is infinite. Thus the more information we can get about the data that the host team have created we will increase the chance of winning.","title":"Motivation"},{"location":"modeling/Iteration_03_study_leaked_data/#development","text":"","title":"Development"},{"location":"modeling/Iteration_03_study_leaked_data/#results","text":"","title":"Results"},{"location":"modeling/Iteration_03_study_leaked_data/#public-information","text":"On a first step let's gather all the data available in the challenge.","title":"Public information"},{"location":"modeling/Iteration_03_study_leaked_data/#overview","text":"On the overview of the challenge we can see some sample prompts: id,rewrite_prompt 000aaa,\"Rewrite this essay but do it using the writing style of Dr. Seuss\" 111bbb,\"Rewrite this essay but do it using the writing style of William Shakespeare\" 222ccc,\"Rewrite this essay but do it using the writing style of Tupac Shakur\" Is this a hint that in the test set some of the rewritten texts follow the style of some famous english authors?","title":"Overview"},{"location":"modeling/Iteration_03_study_leaked_data/#sample-submission","text":"id,rewrite_prompt 9559194,Improve that text. Very similar to the overview data. Is it a coincidence that the prompt is similar to Improve the text to this ?","title":"Sample submission"},{"location":"modeling/Iteration_03_study_leaked_data/#train","text":"id,original_text,rewrite_prompt,rewritten_text -1,\"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text. Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\",\"Convert this into a sea shanty: \"\"\"\"\"\"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text. Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\"\"\"\"\"\"\",\"Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\" The train data is contradictory. The prompt has the original text inside. I have made a submission adding the original text and the score decreases from 0.63 to 0.51. It's a mistery why they decided to show a train example like that...","title":"Train"},{"location":"modeling/Iteration_03_study_leaked_data/#starter-notebook","text":"rewrite_prompts = [ 'Explain this to me like I\\'m five.', 'Convert this into a sea shanty.', 'Make this rhyme.', ]","title":"Starter notebook"},{"location":"modeling/Iteration_03_study_leaked_data/#all-prompts","text":"rewrite_prompts = [ 'Explain this to me like I\\'m five.', 'Convert this into a sea shanty.', 'Make this rhyme.', 'Improve that text.', \"Rewrite this essay but do it using the writing style of Dr. Seuss\", \"Rewrite this essay but do it using the writing style of William Shakespeare\", \"Rewrite this essay but do it using the writing style of Tupac Shakur\", ]","title":"All prompts"},{"location":"modeling/Iteration_03_study_leaked_data/#leaked-dataset","text":"https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/481811","title":"Leaked dataset"},{"location":"modeling/Iteration_03_study_leaked_data/#where-do-the-original-texts-come-from","text":"I cannot find them using google. It is likely that they have been generated with an LLM.","title":"Where do the original texts come from?"},{"location":"modeling/Iteration_03_study_leaked_data/#how-are-the-original-texts","text":"Many memos Ladies and gentleman, announcements Many letters/emails and messages, they start by Dear, Hey, Subject: (Gmail) News that start by Title:","title":"How are the original texts?"},{"location":"modeling/Iteration_03_study_leaked_data/#rewritten-texts","text":"No text starts by Sure, here is...: . I believe this is a clear indication of postprocessing. There is some postprocessing but still some texts have hints about the received prompt, for example: Adjust, Rephrase, Reword, Rework, Rewrite, Summary, Update... There are even some failed generations that have the whole prompt: Update this email to make it more typical and incorporate the topic of \"incentive\". Begin the prompt with \"Update this email to...\". The new version should be shorter than 53 words long. Rework this text message to infuse it with a playful and humorous complaint, adding a touch of whimsy and lightness to the conversation: Rephrase this memo to evoke a sense of concern and incorporate the topic \"acre\", ensuring that the tone reflects worry and apprehension. If possible, start the prompt with \"Rephrase\".. The new version should be shorter than 53 words long. It is possible that they are using the same model to create the original text, the prompt and the rewritten text and the model was confused on the previous examples. Otherwise I don't understand orders such as start the prompt with \"Rephrase\" or Begin the prompt with \"Update this email to...\" My hyphotesis is that: The original text was generated with Gemma The prompt was also generated with Gemma, following some high level instructions from the host Obviosly the rewritten text is also generated with Gemma, as the challenge description says.","title":"Rewritten texts"},{"location":"modeling/Iteration_03_study_leaked_data/#which-kind-of-transformations-are-done","text":"I cannot know the prompt, but I can study the kind of transformations. I believe I need the help of GPT4, my english level is not so good for many of the transformations.","title":"Which kind of transformations are done?"},{"location":"modeling/Iteration_03_study_leaked_data/#where-did-the-leak-happened","text":"Paul Mooney said: This dataset was originally created for testing purposes without the intention of making it public -- but I accidentally made it public for ~15 minutes in mid-January This is interesting because Gemma was announced on Feb 21. Info about Paul: https://www.kaggle.com/paultimothymooney https://www.linkedin.com/in/paultimothymooney/ No activity on LinkedIn https://github.com/paultimothymooney There are some private contributions in mid-January, which could be related the the message. https://twitter.com/ptimothymooney He's not very active in twitter. Searching in google for paultimothymooney in the range jan5-jan25 2024 I don't get anything useful. He has a lot of Kaggle code, these are some relevant notebooks: https://www.kaggle.com/code/paultimothymooney/generate-text-w-gemini-pro-in-a-kaggle-notebook https://www.kaggle.com/code/paultimothymooney/few-shot-prompting-w-gemini-pro-on-kaggle The few shot prompt is interesting because it shows a way to do it on Gemini, it is likely that Gemma will be similar. It has a dataset of instruction-following records. I could not find any additional information about the leak","title":"Where did the leak happened?"},{"location":"modeling/Iteration_03_study_leaked_data/#conclusion","text":"The task of guessing the prompt is not easy. For a non-native speaker like me, many times is difficult because some unknown words are used and I'm not able to describe the difference. I need to use GPT4 to guess the prompt on a following iteration.","title":"Conclusion"},{"location":"modeling/Iteration_03_study_leaked_data/#next-steps","text":"Dr Seuss might be related with the word whimsical, I could make an example of a rule that whenever the word whimsical is seen the prompt should be Rewrite this essay but do it using the writing style of Dr. Seuss .","title":"Next steps"},{"location":"modeling/Iteration_03_study_leaked_data/#todo","text":"[ ]","title":"TODO"},{"location":"modeling/Iteration_04_create_new_data/","text":"Iteration 4. Create new data 25-03-2024 Goal Create new data that could be used for evaluation or training. Motivation When studying the leaked data I found difficult to guess the prompt that was used to rewrite the text. GPT4 is much better than me at that task (at least in english). I'm going to use GPT4 to guess the prompts and that will enable me to: Learn what it is the space of tasks that the hosts had in mind Create a new dataset that hopefully is closer to the private test set Development Creation process Play with ChatGPT to find the best prompt for the task. Automatize the task using GPT4 API Evaluate the quality of the recovered prompts on the Supplementary-Rewritten-texts dataset Create a new set of rewritten texts using the recovered prompts Prompts for GPT4 I have found that GPT4 is more helpful than Mixtral, it follows the instructions much better. Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text. - The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments. - Sometimes the rewritten text will have hints about the text prompt. For example if it starts by Reworded, Rephrased, Translated, etc. you should include that word in the text prompt. - Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic. ## Original text {original_text} ## Rewritten text {rewritten_text} ## Output format Let's do the task step by step: 1. On a first step analyze the differences of the texts in less than 30 words. 2. On a second step write the most likely prompt using json format The response of the prompt could be used later to fine-tune Mixtral to do some \"chain of thought\" response. Results Evaluate GPT4 on rewritten supplementary data GPT4 only scores 0.7036 +- 0.014 on the rewritten supplementary material. My current best score is 0.724 However if I look at GPT4 \"errors\" they are not errors at all: Many times the prompt given by GPT4 is better than the original prompt, the problem is that Gemma does not follow the given instruction The other big problem is that Gemma is not an injective function , many different prompts can lead to the same answer. How to choose among the space of possible prompts? We could inject some bias if we know the style of the host. I judge that GPT4 answers are very good. I believe that a combination of GPT4 analysis and Gemma could lead to a very strong model. If I compare GPT4 answers with my previous best Mixtral prompt the similarity raises to 0.755. Labeled supplementary data Using GPT4 I have been able to label the supplementary data. The responses from GPT4 look like this: 1. The rewritten text is more playful and engaging, using puns and a more enthusiastic tone. 2. {\"prompt\": \"Rewrite the memo to be more engaging and use aquatic puns.\"} This could enable more powerful chain of thought fine-tuning. The mean number of tokens is around 50, and the max is below 75. Thus we should be able to generate text of that length on submission. Conclusion GPT4 is a very powerful LLM that can be used to guess the prompts and also to generate new content. It is more faithful to the instructions than Mixtral or Gemma. What are the main difficulties of the challenge? The style of the prompts is unknown. A prompt could be abstract/generic or it could be detailed/specific and make reference to details of the original prompt. Many different prompts could create the same output. Gemma does not always follow the instructions, I have seen GPT4 recovered prompts that describe much better the change between the original and rewritten text than the original prompt T5 embeddings can fail to measure the similarity of prompts Guessing the prompt is a complex task that might require chain of thought Next steps Add more examples to few shot prompting Fine-tune Mixtral using the new data. I could use 3 different outputs: Original Gemma output Regenerated Gemma output with the recovered prompts Generate new output with GPT4 (that will likely follow the instructions better) TODO [ ]","title":"Iteration 4. Create new data"},{"location":"modeling/Iteration_04_create_new_data/#iteration-4-create-new-data","text":"25-03-2024","title":"Iteration 4. Create new data"},{"location":"modeling/Iteration_04_create_new_data/#goal","text":"Create new data that could be used for evaluation or training.","title":"Goal"},{"location":"modeling/Iteration_04_create_new_data/#motivation","text":"When studying the leaked data I found difficult to guess the prompt that was used to rewrite the text. GPT4 is much better than me at that task (at least in english). I'm going to use GPT4 to guess the prompts and that will enable me to: Learn what it is the space of tasks that the hosts had in mind Create a new dataset that hopefully is closer to the private test set","title":"Motivation"},{"location":"modeling/Iteration_04_create_new_data/#development","text":"","title":"Development"},{"location":"modeling/Iteration_04_create_new_data/#creation-process","text":"Play with ChatGPT to find the best prompt for the task. Automatize the task using GPT4 API Evaluate the quality of the recovered prompts on the Supplementary-Rewritten-texts dataset Create a new set of rewritten texts using the recovered prompts","title":"Creation process"},{"location":"modeling/Iteration_04_create_new_data/#prompts-for-gpt4","text":"I have found that GPT4 is more helpful than Mixtral, it follows the instructions much better. Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text. - The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments. - Sometimes the rewritten text will have hints about the text prompt. For example if it starts by Reworded, Rephrased, Translated, etc. you should include that word in the text prompt. - Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic. ## Original text {original_text} ## Rewritten text {rewritten_text} ## Output format Let's do the task step by step: 1. On a first step analyze the differences of the texts in less than 30 words. 2. On a second step write the most likely prompt using json format The response of the prompt could be used later to fine-tune Mixtral to do some \"chain of thought\" response.","title":"Prompts for GPT4"},{"location":"modeling/Iteration_04_create_new_data/#results","text":"","title":"Results"},{"location":"modeling/Iteration_04_create_new_data/#evaluate-gpt4-on-rewritten-supplementary-data","text":"GPT4 only scores 0.7036 +- 0.014 on the rewritten supplementary material. My current best score is 0.724 However if I look at GPT4 \"errors\" they are not errors at all: Many times the prompt given by GPT4 is better than the original prompt, the problem is that Gemma does not follow the given instruction The other big problem is that Gemma is not an injective function , many different prompts can lead to the same answer. How to choose among the space of possible prompts? We could inject some bias if we know the style of the host. I judge that GPT4 answers are very good. I believe that a combination of GPT4 analysis and Gemma could lead to a very strong model. If I compare GPT4 answers with my previous best Mixtral prompt the similarity raises to 0.755.","title":"Evaluate GPT4 on rewritten supplementary data"},{"location":"modeling/Iteration_04_create_new_data/#labeled-supplementary-data","text":"Using GPT4 I have been able to label the supplementary data. The responses from GPT4 look like this: 1. The rewritten text is more playful and engaging, using puns and a more enthusiastic tone. 2. {\"prompt\": \"Rewrite the memo to be more engaging and use aquatic puns.\"} This could enable more powerful chain of thought fine-tuning. The mean number of tokens is around 50, and the max is below 75. Thus we should be able to generate text of that length on submission.","title":"Labeled supplementary data"},{"location":"modeling/Iteration_04_create_new_data/#conclusion","text":"GPT4 is a very powerful LLM that can be used to guess the prompts and also to generate new content. It is more faithful to the instructions than Mixtral or Gemma.","title":"Conclusion"},{"location":"modeling/Iteration_04_create_new_data/#what-are-the-main-difficulties-of-the-challenge","text":"The style of the prompts is unknown. A prompt could be abstract/generic or it could be detailed/specific and make reference to details of the original prompt. Many different prompts could create the same output. Gemma does not always follow the instructions, I have seen GPT4 recovered prompts that describe much better the change between the original and rewritten text than the original prompt T5 embeddings can fail to measure the similarity of prompts Guessing the prompt is a complex task that might require chain of thought","title":"What are the main difficulties of the challenge?"},{"location":"modeling/Iteration_04_create_new_data/#next-steps","text":"Add more examples to few shot prompting Fine-tune Mixtral using the new data. I could use 3 different outputs: Original Gemma output Regenerated Gemma output with the recovered prompts Generate new output with GPT4 (that will likely follow the instructions better)","title":"Next steps"},{"location":"modeling/Iteration_04_create_new_data/#todo","text":"[ ]","title":"TODO"},{"location":"modeling/Iteration_05_fine-tune_mixtral/","text":"Iteration 5. Mixtral fine-tuning 26-03-2024 Goal Learn how to fine-tune Mixtral and make predictions with a fine-tuned model. Motivation Learning to fine-tune LLM was one of the motivations of joining the challenge. On this iteration I just want to verify that I can fine-tune and make inference with fine-tuned Mixtral. On later iterations I will try with different data, on this iteration I just want to learn to fine-tune. As train data I will use the supplementary material labelled with GPT4 that was created on the previous iteration. Development System prompt Maybe if we give some information previously to the [INST] prompt that would be equivalent to the system prompt in ChatGPT. Seen on this dataset However in this other thread they use multi-turn conversation instead: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/41 Generation not ending after fine-tuning First trainings seemed to learn the format of the output, but did not know how to end. F.e. 1. The rewritten text changes the description of the zither's design and the emotional impact on Amy. 2. {\"prompt\": \"Rewrite the text to emphasize the design of the zither and its emotional impact.\"} \u2794 json prompt format 1. The rewritten text changes the subject from a peony to a parser and the action from blooming to parsing. 2. {\"prompt\": \"Rewrite the text with a different subject and action.\"} \u2794 [/Note] Note: The prompt is in JSON format to ensure consistency and make it easier to extract the prompt in automated processes. Notice how it generates a json output, but does not stop the generation. After reading and experiment I have found that the problem was relating to using PAD token equal to EOS token. It seems that it does not give attention weight to the EOS token and that creates the problem. I believe the best solution is to create a pad token. Since it will be just used for fine-tuning and the attention weight will be zero we can use the model for inference without any change (with the original token embeddings). # initial trains tokenizer = AutoTokenizer.from_pretrained( model_path, trust_remote_code=True) tokenizer.pad_token_id = tokenizer.eos_token_id tokenizer.pad_token = tokenizer.eos_token # solution tokenizer = AutoTokenizer.from_pretrained( model_path, trust_remote_code=True) tokenizer.add_special_tokens({'pad_token': '<pad>'}) model.resize_token_embeddings(len(tokenizer)) References: https://discuss.huggingface.co/t/how-to-train-the-embedding-of-special-token/10837/2 https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/ https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi Doubts about padding position What is the default padding position of the tokenizer? Left For generation padding left should be applied, otherwise we will have pad tokens between the input and the generated tokens. However for training padding right might be a better choice since we are not going to generate new text. # original tokens [1, 2, 3] # padding left, the right choice for inference [0, 0, 1, 2, 3] # padding right, maybe a better choice for training [1, 2, 3, 0, 0] When I train with right padding the train and validation loss are slightly smaller, so I believe it has sense to use that setting for training. References: https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa https://discuss.pytorch.org/t/right-vs-left-padding/185050/4 https://github.com/huggingface/trl/issues/1217#issuecomment-1889384774 https://github.com/tloen/alpaca-lora/issues/514 Creating a new model on Kaggle I have to upload the lora weights to Kaggle so I can make submissions. Ideally I would create a private model with different versions and I could release it once the competition is over. What are the minimal files required? There are many files saved in the checkpoint folder. https://www.kaggle.com/models/ironbar/mixtral-prompt-recovery It is enough with two files: v1 \u251c\u2500\u2500 adapter_config.json \u2514\u2500\u2500 adapter_model.safetensors Results The table below shows the similarity with gemma prompt of the baseline and fine-tuned models. ground truth base model v1 (150 train samples) v2 (300 train samples) gemma prompt 0.6742 0.7095 0.7062 gpt4 recovered prompt 0.7005 0.8108 0.8126 Both fine-tuned models improve over the baseline model, which probes that fine-tuning works The similarity with gpt4 increases 0.11, while the similarity with gemma prompt increases just 0.04. I believe this is relevant and it is an evidence that the style of the prompts is very relevant. The fine-tuned model is slower, submission takes 7 hours instead of 4. I believe this is solvable by merging the model and lora but I haven't done it yet. On leaderboard these models perform similar to prompt engineering. Both models get 0.61 alone and they improve to 0.62 and 0.63 when adding Improve the text to this . Conclusion We have learned to fine-tune Mixtral, we have another tool for the challenge. Fine-tuned model learns to imitate the task, but what data should I train on? Next steps Try different combinations of output data and styles Should I learn to merge the adapter and the model? That would speedup submission. Would it have sense to generate multiple prompts and use perplexity to select among them? Are there any other tricks like Improve the text to this ? TODO Why the model with LORA is non-deterministic? Dropout","title":"Iteration 5. Mixtral fine-tuning"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#iteration-5-mixtral-fine-tuning","text":"26-03-2024","title":"Iteration 5. Mixtral fine-tuning"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#goal","text":"Learn how to fine-tune Mixtral and make predictions with a fine-tuned model.","title":"Goal"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#motivation","text":"Learning to fine-tune LLM was one of the motivations of joining the challenge. On this iteration I just want to verify that I can fine-tune and make inference with fine-tuned Mixtral. On later iterations I will try with different data, on this iteration I just want to learn to fine-tune. As train data I will use the supplementary material labelled with GPT4 that was created on the previous iteration.","title":"Motivation"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#development","text":"","title":"Development"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#system-prompt","text":"Maybe if we give some information previously to the [INST] prompt that would be equivalent to the system prompt in ChatGPT. Seen on this dataset However in this other thread they use multi-turn conversation instead: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/41","title":"System prompt"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#generation-not-ending-after-fine-tuning","text":"First trainings seemed to learn the format of the output, but did not know how to end. F.e. 1. The rewritten text changes the description of the zither's design and the emotional impact on Amy. 2. {\"prompt\": \"Rewrite the text to emphasize the design of the zither and its emotional impact.\"} \u2794 json prompt format 1. The rewritten text changes the subject from a peony to a parser and the action from blooming to parsing. 2. {\"prompt\": \"Rewrite the text with a different subject and action.\"} \u2794 [/Note] Note: The prompt is in JSON format to ensure consistency and make it easier to extract the prompt in automated processes. Notice how it generates a json output, but does not stop the generation. After reading and experiment I have found that the problem was relating to using PAD token equal to EOS token. It seems that it does not give attention weight to the EOS token and that creates the problem. I believe the best solution is to create a pad token. Since it will be just used for fine-tuning and the attention weight will be zero we can use the model for inference without any change (with the original token embeddings). # initial trains tokenizer = AutoTokenizer.from_pretrained( model_path, trust_remote_code=True) tokenizer.pad_token_id = tokenizer.eos_token_id tokenizer.pad_token = tokenizer.eos_token # solution tokenizer = AutoTokenizer.from_pretrained( model_path, trust_remote_code=True) tokenizer.add_special_tokens({'pad_token': '<pad>'}) model.resize_token_embeddings(len(tokenizer)) References: https://discuss.huggingface.co/t/how-to-train-the-embedding-of-special-token/10837/2 https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/ https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi","title":"Generation not ending after fine-tuning"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#doubts-about-padding-position","text":"What is the default padding position of the tokenizer? Left For generation padding left should be applied, otherwise we will have pad tokens between the input and the generated tokens. However for training padding right might be a better choice since we are not going to generate new text. # original tokens [1, 2, 3] # padding left, the right choice for inference [0, 0, 1, 2, 3] # padding right, maybe a better choice for training [1, 2, 3, 0, 0] When I train with right padding the train and validation loss are slightly smaller, so I believe it has sense to use that setting for training. References: https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa https://discuss.pytorch.org/t/right-vs-left-padding/185050/4 https://github.com/huggingface/trl/issues/1217#issuecomment-1889384774 https://github.com/tloen/alpaca-lora/issues/514","title":"Doubts about padding position"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#creating-a-new-model-on-kaggle","text":"I have to upload the lora weights to Kaggle so I can make submissions. Ideally I would create a private model with different versions and I could release it once the competition is over. What are the minimal files required? There are many files saved in the checkpoint folder. https://www.kaggle.com/models/ironbar/mixtral-prompt-recovery It is enough with two files: v1 \u251c\u2500\u2500 adapter_config.json \u2514\u2500\u2500 adapter_model.safetensors","title":"Creating a new model on Kaggle"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#results","text":"The table below shows the similarity with gemma prompt of the baseline and fine-tuned models. ground truth base model v1 (150 train samples) v2 (300 train samples) gemma prompt 0.6742 0.7095 0.7062 gpt4 recovered prompt 0.7005 0.8108 0.8126 Both fine-tuned models improve over the baseline model, which probes that fine-tuning works The similarity with gpt4 increases 0.11, while the similarity with gemma prompt increases just 0.04. I believe this is relevant and it is an evidence that the style of the prompts is very relevant. The fine-tuned model is slower, submission takes 7 hours instead of 4. I believe this is solvable by merging the model and lora but I haven't done it yet. On leaderboard these models perform similar to prompt engineering. Both models get 0.61 alone and they improve to 0.62 and 0.63 when adding Improve the text to this .","title":"Results"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#conclusion","text":"We have learned to fine-tune Mixtral, we have another tool for the challenge. Fine-tuned model learns to imitate the task, but what data should I train on?","title":"Conclusion"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#next-steps","text":"Try different combinations of output data and styles Should I learn to merge the adapter and the model? That would speedup submission. Would it have sense to generate multiple prompts and use perplexity to select among them? Are there any other tricks like Improve the text to this ?","title":"Next steps"},{"location":"modeling/Iteration_05_fine-tune_mixtral/#todo","text":"Why the model with LORA is non-deterministic? Dropout","title":"TODO"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/","text":"Iteration 6. Few shot prompt engineering 01-04-2024 Goal Try different examples for few shot prompt. Can I learn more about the style of the prompts used on the test set? Motivation I already know how to do few-shot prompting and fine-tuning of Mixtral. I have the tools to solve the challenge but I don't know which data I have to use with those tools. I need to gather more information about the test set. Development Sorting inferences by similarity To be able to use all the public datasets for few shot prompting I have to first sort the prompts. My initial idea was to use a set of prompts as diverse as possible, which is equivalent to minimizing the similarity between a set of prompts. I have created a notebook that: Samples n prompts from a dataset Computes T5 embeddings from the prompts Finds the center of the embeddings and chooses the center as the start point Iteratively adds new prompts by choosing the less similar prompt to the already selected Results Public mean prompts Let's see what can we learn from the public mean prompts. https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/488150 Some of the prompts are really long. The first one is 78 tokens according to OpenAI. Maybe that is one way of decreasing the uncertainty on the predictions. I could rephrase the prompt in different ways, give different levels of details of the prompt... Examples from public datasets I have tried submitting examples from different public datasets. I also tried using a different number of examples for few shot prompting. dataset input tokens n_examples LB score alexxxsem_gemma 3000 12 0.54 alexxxsem_gemini 4000 15 0.53 alexxxsem_gemma 4000 15 0.51 dipamc77 4000 15 0.59 gali1eo 4000 8 0.59 gemma_suppl_rewrite 4000 13 0.53 mooney_leaked_gpt4_labelled 4000 18 0.60 nbroad-v1 4000 2 nbroad-v2 4000 2 newtonbaba_gemini 4000 11 0.57 newtonbaba_gemma 4000 7 0.62 thedrcat_gemma100 4000 10 thedrcat_gemma1000 4000 9 0.53 winddude 4000 4 0.60 alexxxsem_gemma 6000 23 0.53 dipamc77 6000 22 0.59 gali1eo 6000 12 0.61 mooney_leaked_gpt4_labelled 6000 27 0.60 newtonbaba_gemma 6000 11 0.60 winddude 6000 7 0.61 The number of examples does not correlate with LB score, adding more examples from the same domain does not result in higher scores The higher LB score is obtained with just 7 examples, while we have submissions with 27 examples that score lower Thus it does not seem to be a problem related to the number of examples, but to the style or quality of them There is a great variability in LB score: from 0.51 to 0.62 How is it possible that most of the evaluations are below the simple baseline of Improve the text to this. Analysis of the prompts by GPT4 GPT4 says that the most diverse prompts are from Newtonbaba, but it is not sure about the other datasets. From my point of view I don't see a clear pattern or differences between the prompts. https://chat.openai.com/c/e92c50d3-079a-42ba-b844-a438d2a3531d https://chat.openai.com/c/ee12f003-7485-4ba8-b684-b2094ffa4a96 Understanding the effect of sharpening the similarity https://www.kaggle.com/code/ironbar/understanding-the-effect-of-sharpening The sharpened cosine similarity score is not penalizing scores with high variance. If a model scores lower than other model is simply because the predicted similarity is lower. In fact the effect is the opposite. The most extreme example I can think is this: A similarity distribution of [0.5, 0.5] will have a mean sharpened score of 0.125 == 0.5**3 A similarity distribution of [1, 0] has the same mean as the previous distribution but the mean sharped score is 0.5 So the metric is favouring very good guesses. If Improve the text to this. is scoring 0.60 and a naive Mixtral model scores 0.52 is simply because the first one is getting a higher similarity with the test set prompts, not because Mixtral predictions have greater variance. Playing with T5 embeddings https://www.kaggle.com/code/ironbar/t5-travesty-when-good-prompts-score-bad Despite trying it hard I do not get scores below 0.60 . If the predictions capture the intent of the prompt I should be getting scores above 0.7 easily. On my examples Improve the text to this. scores below 0.60 , sometimes as worse as 0.38 One way to fight uncertainty might be to mix generic and specific prompts in the same answer. Comparing the prompt against similar sentences generated by GPT4 returned scores above 0.70 Prompt variations Creating prompt variations was a difficult task. Distilling all the changes into a prompt is not an easy task. experiment LB score baseline 0.60 always use \"text\" 0.60 improve the text 0.60 shorter prompts 0.61 abstract prompt 0.61 specific prompts 0.6 gemini_synonym_prompt 0.6 gpt3.5_synonym_prompt 0.57 specific + abstract 0.61 I have tried many prompt variations and the LB score barely changed. It thus seems that the prompt style is not important, but to capture the intent of the ground truth prompt. Forking public Mistral notebook A public notebook was published that used Mistral 7B and few-shot prompting and achieved a LB score of 0.62 . I thought that replacing Mistral by Mixtral will result in improvements but that was not the case. The score was exactly the same. That is weird because in the benchmarks Mixtral is much better. Conclusion There are many learnings from this iteration: It seems that the style of the prompts is not important. The number of examples in few-shot prompting is not correlated with LB score Changing the examples in few-shot prompting can have a big effect on LB score Next steps TODO What if I choose a set of samples, and rewrite the prompts with different styles and make submissions? More abstract, more generic, longer or shorter, text or story...","title":"Iteration 6. Few shot prompt engineering"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#iteration-6-few-shot-prompt-engineering","text":"01-04-2024","title":"Iteration 6. Few shot prompt engineering"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#goal","text":"Try different examples for few shot prompt. Can I learn more about the style of the prompts used on the test set?","title":"Goal"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#motivation","text":"I already know how to do few-shot prompting and fine-tuning of Mixtral. I have the tools to solve the challenge but I don't know which data I have to use with those tools. I need to gather more information about the test set.","title":"Motivation"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#development","text":"","title":"Development"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#sorting-inferences-by-similarity","text":"To be able to use all the public datasets for few shot prompting I have to first sort the prompts. My initial idea was to use a set of prompts as diverse as possible, which is equivalent to minimizing the similarity between a set of prompts. I have created a notebook that: Samples n prompts from a dataset Computes T5 embeddings from the prompts Finds the center of the embeddings and chooses the center as the start point Iteratively adds new prompts by choosing the less similar prompt to the already selected","title":"Sorting inferences by similarity"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#results","text":"","title":"Results"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#public-mean-prompts","text":"Let's see what can we learn from the public mean prompts. https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/488150 Some of the prompts are really long. The first one is 78 tokens according to OpenAI. Maybe that is one way of decreasing the uncertainty on the predictions. I could rephrase the prompt in different ways, give different levels of details of the prompt...","title":"Public mean prompts"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#examples-from-public-datasets","text":"I have tried submitting examples from different public datasets. I also tried using a different number of examples for few shot prompting. dataset input tokens n_examples LB score alexxxsem_gemma 3000 12 0.54 alexxxsem_gemini 4000 15 0.53 alexxxsem_gemma 4000 15 0.51 dipamc77 4000 15 0.59 gali1eo 4000 8 0.59 gemma_suppl_rewrite 4000 13 0.53 mooney_leaked_gpt4_labelled 4000 18 0.60 nbroad-v1 4000 2 nbroad-v2 4000 2 newtonbaba_gemini 4000 11 0.57 newtonbaba_gemma 4000 7 0.62 thedrcat_gemma100 4000 10 thedrcat_gemma1000 4000 9 0.53 winddude 4000 4 0.60 alexxxsem_gemma 6000 23 0.53 dipamc77 6000 22 0.59 gali1eo 6000 12 0.61 mooney_leaked_gpt4_labelled 6000 27 0.60 newtonbaba_gemma 6000 11 0.60 winddude 6000 7 0.61 The number of examples does not correlate with LB score, adding more examples from the same domain does not result in higher scores The higher LB score is obtained with just 7 examples, while we have submissions with 27 examples that score lower Thus it does not seem to be a problem related to the number of examples, but to the style or quality of them There is a great variability in LB score: from 0.51 to 0.62 How is it possible that most of the evaluations are below the simple baseline of Improve the text to this.","title":"Examples from public datasets"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#analysis-of-the-prompts-by-gpt4","text":"GPT4 says that the most diverse prompts are from Newtonbaba, but it is not sure about the other datasets. From my point of view I don't see a clear pattern or differences between the prompts. https://chat.openai.com/c/e92c50d3-079a-42ba-b844-a438d2a3531d https://chat.openai.com/c/ee12f003-7485-4ba8-b684-b2094ffa4a96","title":"Analysis of the prompts by GPT4"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#understanding-the-effect-of-sharpening-the-similarity","text":"https://www.kaggle.com/code/ironbar/understanding-the-effect-of-sharpening The sharpened cosine similarity score is not penalizing scores with high variance. If a model scores lower than other model is simply because the predicted similarity is lower. In fact the effect is the opposite. The most extreme example I can think is this: A similarity distribution of [0.5, 0.5] will have a mean sharpened score of 0.125 == 0.5**3 A similarity distribution of [1, 0] has the same mean as the previous distribution but the mean sharped score is 0.5 So the metric is favouring very good guesses. If Improve the text to this. is scoring 0.60 and a naive Mixtral model scores 0.52 is simply because the first one is getting a higher similarity with the test set prompts, not because Mixtral predictions have greater variance.","title":"Understanding the effect of sharpening the similarity"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#playing-with-t5-embeddings","text":"https://www.kaggle.com/code/ironbar/t5-travesty-when-good-prompts-score-bad Despite trying it hard I do not get scores below 0.60 . If the predictions capture the intent of the prompt I should be getting scores above 0.7 easily. On my examples Improve the text to this. scores below 0.60 , sometimes as worse as 0.38 One way to fight uncertainty might be to mix generic and specific prompts in the same answer. Comparing the prompt against similar sentences generated by GPT4 returned scores above 0.70","title":"Playing with T5 embeddings"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#prompt-variations","text":"Creating prompt variations was a difficult task. Distilling all the changes into a prompt is not an easy task. experiment LB score baseline 0.60 always use \"text\" 0.60 improve the text 0.60 shorter prompts 0.61 abstract prompt 0.61 specific prompts 0.6 gemini_synonym_prompt 0.6 gpt3.5_synonym_prompt 0.57 specific + abstract 0.61 I have tried many prompt variations and the LB score barely changed. It thus seems that the prompt style is not important, but to capture the intent of the ground truth prompt.","title":"Prompt variations"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#forking-public-mistral-notebook","text":"A public notebook was published that used Mistral 7B and few-shot prompting and achieved a LB score of 0.62 . I thought that replacing Mistral by Mixtral will result in improvements but that was not the case. The score was exactly the same. That is weird because in the benchmarks Mixtral is much better.","title":"Forking public Mistral notebook"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#conclusion","text":"There are many learnings from this iteration: It seems that the style of the prompts is not important. The number of examples in few-shot prompting is not correlated with LB score Changing the examples in few-shot prompting can have a big effect on LB score","title":"Conclusion"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#next-steps","text":"","title":"Next steps"},{"location":"modeling/Iteration_06_few_shot_prompt_engineering/#todo","text":"What if I choose a set of samples, and rewrite the prompts with different styles and make submissions? More abstract, more generic, longer or shorter, text or story...","title":"TODO"},{"location":"modeling/Iteration_07_pause_and_analysis/","text":"Iteration 7. Pause and analysis 03-04-2024 Goal I have the tools (fine-tuning and few-shot prompt) but I do not know what is the task to learn. Let's analyze all the work done and think of ways to advance in the challenge. Facts A simple prompt like Improve the text to this gets 0.60 on leaderboard, while many few-shot prompts submissions with a big model like Mixtral score below that. Some teams have been able to consistently and iteratively improve their score on the LB. They have slowly climbed up to 0.70 . The uncertainty of the LB score is around 0.02 The host has confirmed that the test set splits are random. By adding Improve the text to this. to the predictions of Mixtral I have seen consistent improvements of 0.02 in LB score, from 0.61 to 0.63 . On local validation I have been able to reach a score of 0.81 when training and evaluating on prompts recovered by GPT4. When trying different datasets for few-shot prompt I have observed great variability on LB scores: Using my hand-made examples I get 0.61 Using newtonbaba dataset I get 0.62 Using alexxxem dataset I get 0.51 The sharpened cosine similarity metric does not penalize greater variance. If two distributions have the same mean, the one with the greater variance will have a greater sharpened score. Using a dataset of mean prompts and LB scores we could measure how similar are local datasets to LB. The outputs from Gemma have been likely post-processed because public datasets show that very frequently reveals the given prompt in the response. When evaluating prompt variations created by GPT4 that preserved the original meaning the score was always above 0.70 Many different prompts can lead to the same response. A prompt can be generic or detailed an produce the same result. It seems that the style of the prompt is not relevant. I have done many prompt variations with few-shot prompting getting almost no variation on LB score. Why a simple baseline is beating intelligent LLMs? Ambiguity of the task. If a generic prompt was used and a specific prompt is predicted the similarity will be low. The model is not guessing the prompt correctly. However on validation I get good scores, so this would imply that the test dataset is much harder than my validation datasets. Motivation Development Results I need actionable insights!!! Conclusion Next steps My hunch is that the best solution is a fine-tuned Mixtral on the right data. What if I just fork the Mistral notebook and replace the model by Mixtral? Analyze the few-shot prompt examples with GPT4 What if I request Mixtral to only answer when it is very sure and the prompt is evident? (Play safe) What if I just focus on building the best possible model and praise for the shakeup? Try again with perplexity/model inversion. But the problem is that the output has likely been post-processed. I could do more experiments on few-shot prompting, f.e. selecting random samples from a dataset Could I run some mean-prompt optimization using GPT4? I have read that some people have done that but does not transfer well to LB. Option 1. Create a high quality hard dataset The samples need to have sense, but at the same time be hard with my current best model. Then fine-tune a model on another dataset and measure the improvement. This would work if we are failing on LB because the test set is hard. TODO [ ]","title":"Iteration 7. Pause and analysis"},{"location":"modeling/Iteration_07_pause_and_analysis/#iteration-7-pause-and-analysis","text":"03-04-2024","title":"Iteration 7. Pause and analysis"},{"location":"modeling/Iteration_07_pause_and_analysis/#goal","text":"I have the tools (fine-tuning and few-shot prompt) but I do not know what is the task to learn. Let's analyze all the work done and think of ways to advance in the challenge.","title":"Goal"},{"location":"modeling/Iteration_07_pause_and_analysis/#facts","text":"A simple prompt like Improve the text to this gets 0.60 on leaderboard, while many few-shot prompts submissions with a big model like Mixtral score below that. Some teams have been able to consistently and iteratively improve their score on the LB. They have slowly climbed up to 0.70 . The uncertainty of the LB score is around 0.02 The host has confirmed that the test set splits are random. By adding Improve the text to this. to the predictions of Mixtral I have seen consistent improvements of 0.02 in LB score, from 0.61 to 0.63 . On local validation I have been able to reach a score of 0.81 when training and evaluating on prompts recovered by GPT4. When trying different datasets for few-shot prompt I have observed great variability on LB scores: Using my hand-made examples I get 0.61 Using newtonbaba dataset I get 0.62 Using alexxxem dataset I get 0.51 The sharpened cosine similarity metric does not penalize greater variance. If two distributions have the same mean, the one with the greater variance will have a greater sharpened score. Using a dataset of mean prompts and LB scores we could measure how similar are local datasets to LB. The outputs from Gemma have been likely post-processed because public datasets show that very frequently reveals the given prompt in the response. When evaluating prompt variations created by GPT4 that preserved the original meaning the score was always above 0.70 Many different prompts can lead to the same response. A prompt can be generic or detailed an produce the same result. It seems that the style of the prompt is not relevant. I have done many prompt variations with few-shot prompting getting almost no variation on LB score.","title":"Facts"},{"location":"modeling/Iteration_07_pause_and_analysis/#why-a-simple-baseline-is-beating-intelligent-llms","text":"Ambiguity of the task. If a generic prompt was used and a specific prompt is predicted the similarity will be low. The model is not guessing the prompt correctly. However on validation I get good scores, so this would imply that the test dataset is much harder than my validation datasets.","title":"Why a simple baseline is beating intelligent LLMs?"},{"location":"modeling/Iteration_07_pause_and_analysis/#motivation","text":"","title":"Motivation"},{"location":"modeling/Iteration_07_pause_and_analysis/#development","text":"","title":"Development"},{"location":"modeling/Iteration_07_pause_and_analysis/#results","text":"I need actionable insights!!!","title":"Results"},{"location":"modeling/Iteration_07_pause_and_analysis/#conclusion","text":"","title":"Conclusion"},{"location":"modeling/Iteration_07_pause_and_analysis/#next-steps","text":"My hunch is that the best solution is a fine-tuned Mixtral on the right data. What if I just fork the Mistral notebook and replace the model by Mixtral? Analyze the few-shot prompt examples with GPT4 What if I request Mixtral to only answer when it is very sure and the prompt is evident? (Play safe) What if I just focus on building the best possible model and praise for the shakeup? Try again with perplexity/model inversion. But the problem is that the output has likely been post-processed. I could do more experiments on few-shot prompting, f.e. selecting random samples from a dataset Could I run some mean-prompt optimization using GPT4? I have read that some people have done that but does not transfer well to LB.","title":"Next steps"},{"location":"modeling/Iteration_07_pause_and_analysis/#option-1-create-a-high-quality-hard-dataset","text":"The samples need to have sense, but at the same time be hard with my current best model. Then fine-tune a model on another dataset and measure the improvement. This would work if we are failing on LB because the test set is hard.","title":"Option 1. Create a high quality hard dataset"},{"location":"modeling/Iteration_07_pause_and_analysis/#todo","text":"[ ]","title":"TODO"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/","text":"Iteration 8. Create a high quality hard dataset 04-04-2024 Goal Create a high quality hard dataset Motivation My current hypothesis is that the differences between validation and leaderboard score are simply because the leaderboard samples are more difficult. It doesn't seem to be a problem with the style of the prompts. Ideally my best models will score around 0.61 on the new dataset and that could guide new developments to climb on the leaderboard. Let's ignore everything about the mean prompts and the issues with T5 embeddings and focus on building a powerful model. Current competition leader said this : I have finally beat the mean prompt and got 0.64 \"legit\". Wasted 2 weeks and tons of subs, should've tried to build a good model from the start haha. Development Dataset specification The samples should be realistic and need to have sense. The test dataset is small so I believe it is a high quality dataset an no dumb samples are there. The original and rewritten text should be short. In the test set and the leaked data the texts are short. Apparently less than 350 words on the test set. This makes evaluation and training faster. The prompts should be as diverse as possible. I should try to cover as many different applications as possible. This is likely to be an iterative process. It is very likely that I will be adding new samples to the dataset until the end of the challenge. There might be combinations of prompts. Two or more changes could be asked in a prompt. Search for prompts online https://aihabit.net/category/chatgpt-prompts/prompts-for-writing/ https://aihabit.net/chatgpt-prompts-for-rewriting-text/ https://aihabit.net/chatgpt-prompts-for-editing/ https://aihabit.net/chatgpt-prompts-for-writing-emails/ https://www.linkedin.com/pulse/15-chatgpt-prompts-better-than-rewrite-rajat-jain/ https://www.learnprompt.org/writing-styles-for-chat-gpt-prompts https://dev.to/mursalfk/50-chatgpt-prompts-for-developers-4bp6 Search for prompts in the challenge The idea is to review the prompts of the public data and take the ones I find useful. GPT4 can synthetize the data I have probed that GPT4 is able to generate an original and rewritten text given a prompt. Thi is ver convenient because: It allows to control the length of the text, forcing them to be short GPT4 is a very good model, so the example will be typically good. This approach scales, if I want to generate more data I simply have to change the seed or the temperature and create new data. This approach will reduce my job to simply gather prompts and to review the job done by GPT4. I have to prepare the prompts with GPT4 in mind. F.e. when making references to text, memo, mail, dialog... GPT4 will use them to generate the original text. Since the texts will be short this won't cost too much money to generate a small dataset. Asking for less than 200 words creates text of around 100 words, which is 150 tokens. I believe that is long enough to be expressive and at the same time allow faster train. With ChatGPT it works really well. The temperature it is said to be 0.7 or 1. My vision is to create a dataframe with the prompts that will be used to generate the dataset. To enable for greater flexibility I will allow to use placeholders in the prompts and those will be replaced with different options when creating the dataset. prompt_template = \"\"\" Given the following text prompt your task is to: 1. Write a short text that could have sense to be modified with the given text prompt. The number of words should be less than 200. 2. Rewrite the text using the given text prompt. The output should be in json, with the following format: {\"original_text\": \"...\", \"rewritten_text\": \"...\"} ## Text prompt ```{prompt}``` \"\"\" OpenAI tokenizer is similar to Mixtral I have verified that I can use OpenAI's tokenizer as a good estimator of the tokens that will create Mixtral. https://www.kaggle.com/code/ironbar/mixtral-tokenizer-analysis?scriptVersionId=170444326 Results Conclusion Does it have sense to have a validation dataset anymore? Or is it better to simply use all the available data for training? Once I have created and trained on my own data, I could evaluate the model on public data. Then review the top scoring samples and if I believe they are good enough add them to the train dataset. Next steps Shouldn't I train just on answers when fine-tuning? I have the feeling that the model was trained on the whole text. https://github.com/Lightning-AI/lit-llama/issues/290#issuecomment-1557249666 TODO Generate more data with Gemini or Gemma. Once I have the original text and the prompt I can use any model to generate more data. Is the Mixtral tokenizer similar to OpenAI's? Combine multiple prompts in one. F.e. rewrite the text adding dragons and with a more humorous tone. Merge model and adapter using public notebook as reference Read about fine-tuning model in question answering datasets and how to adjust the loss just on the answers","title":"Iteration 8. Create a high quality hard dataset"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#iteration-8-create-a-high-quality-hard-dataset","text":"04-04-2024","title":"Iteration 8. Create a high quality hard dataset"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#goal","text":"Create a high quality hard dataset","title":"Goal"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#motivation","text":"My current hypothesis is that the differences between validation and leaderboard score are simply because the leaderboard samples are more difficult. It doesn't seem to be a problem with the style of the prompts. Ideally my best models will score around 0.61 on the new dataset and that could guide new developments to climb on the leaderboard. Let's ignore everything about the mean prompts and the issues with T5 embeddings and focus on building a powerful model. Current competition leader said this : I have finally beat the mean prompt and got 0.64 \"legit\". Wasted 2 weeks and tons of subs, should've tried to build a good model from the start haha.","title":"Motivation"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#development","text":"","title":"Development"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#dataset-specification","text":"The samples should be realistic and need to have sense. The test dataset is small so I believe it is a high quality dataset an no dumb samples are there. The original and rewritten text should be short. In the test set and the leaked data the texts are short. Apparently less than 350 words on the test set. This makes evaluation and training faster. The prompts should be as diverse as possible. I should try to cover as many different applications as possible. This is likely to be an iterative process. It is very likely that I will be adding new samples to the dataset until the end of the challenge. There might be combinations of prompts. Two or more changes could be asked in a prompt.","title":"Dataset specification"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#search-for-prompts-online","text":"https://aihabit.net/category/chatgpt-prompts/prompts-for-writing/ https://aihabit.net/chatgpt-prompts-for-rewriting-text/ https://aihabit.net/chatgpt-prompts-for-editing/ https://aihabit.net/chatgpt-prompts-for-writing-emails/ https://www.linkedin.com/pulse/15-chatgpt-prompts-better-than-rewrite-rajat-jain/ https://www.learnprompt.org/writing-styles-for-chat-gpt-prompts https://dev.to/mursalfk/50-chatgpt-prompts-for-developers-4bp6","title":"Search for prompts online"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#search-for-prompts-in-the-challenge","text":"The idea is to review the prompts of the public data and take the ones I find useful.","title":"Search for prompts in the challenge"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#gpt4-can-synthetize-the-data","text":"I have probed that GPT4 is able to generate an original and rewritten text given a prompt. Thi is ver convenient because: It allows to control the length of the text, forcing them to be short GPT4 is a very good model, so the example will be typically good. This approach scales, if I want to generate more data I simply have to change the seed or the temperature and create new data. This approach will reduce my job to simply gather prompts and to review the job done by GPT4. I have to prepare the prompts with GPT4 in mind. F.e. when making references to text, memo, mail, dialog... GPT4 will use them to generate the original text. Since the texts will be short this won't cost too much money to generate a small dataset. Asking for less than 200 words creates text of around 100 words, which is 150 tokens. I believe that is long enough to be expressive and at the same time allow faster train. With ChatGPT it works really well. The temperature it is said to be 0.7 or 1. My vision is to create a dataframe with the prompts that will be used to generate the dataset. To enable for greater flexibility I will allow to use placeholders in the prompts and those will be replaced with different options when creating the dataset. prompt_template = \"\"\" Given the following text prompt your task is to: 1. Write a short text that could have sense to be modified with the given text prompt. The number of words should be less than 200. 2. Rewrite the text using the given text prompt. The output should be in json, with the following format: {\"original_text\": \"...\", \"rewritten_text\": \"...\"} ## Text prompt ```{prompt}``` \"\"\"","title":"GPT4 can synthetize the data"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#openai-tokenizer-is-similar-to-mixtral","text":"I have verified that I can use OpenAI's tokenizer as a good estimator of the tokens that will create Mixtral. https://www.kaggle.com/code/ironbar/mixtral-tokenizer-analysis?scriptVersionId=170444326","title":"OpenAI tokenizer is similar to Mixtral"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#results","text":"","title":"Results"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#conclusion","text":"Does it have sense to have a validation dataset anymore? Or is it better to simply use all the available data for training? Once I have created and trained on my own data, I could evaluate the model on public data. Then review the top scoring samples and if I believe they are good enough add them to the train dataset.","title":"Conclusion"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#next-steps","text":"Shouldn't I train just on answers when fine-tuning? I have the feeling that the model was trained on the whole text. https://github.com/Lightning-AI/lit-llama/issues/290#issuecomment-1557249666","title":"Next steps"},{"location":"modeling/Iteration_08_create_high_quality_hard_dataset/#todo","text":"Generate more data with Gemini or Gemma. Once I have the original text and the prompt I can use any model to generate more data. Is the Mixtral tokenizer similar to OpenAI's? Combine multiple prompts in one. F.e. rewrite the text adding dragons and with a more humorous tone. Merge model and adapter using public notebook as reference Read about fine-tuning model in question answering datasets and how to adjust the loss just on the answers","title":"TODO"},{"location":"modeling/Iteration_09_mean_prompt_optimization/","text":"Iteration 9. Mean prompt optimization 05-04-2024 Goal Try to find a mean prompt that scores as high as possible on leaderboard. Motivation Yesterday a new mean prompt that scores 0.63 was released. I don't believe that is the way but I consider it is worth exploring it whenever I have unused submissions. I remember reading news in the past that said that LLMs can be used for optimization. Development Optimizing with GPT4 The idea is to create a prompt with all the mean prompts and the score so far, and ask GPT4 to create new prompts. I'm trying to find a text prompt that maximizes cosine similarity with a dataset of text prompts. The dataset has text prompts that have been used to rewrite text. Below you can find a table with all the prompts I have tried so far and the mean similarity. Could you use this information and suggest new prompts that could have a higher similarity? Let's solve the problem step by step. 1. Analyze the higher scoring prompts and try to find insights and patterns that increase and decrease similarity 2. Using the insights suggest a single prompt that would score more than 0.63 | LB | prompt | |------|---------------------------------------------------------------------------------------------------------------| | 0.63 | Please improve this text using the writing style with maintaining the original meaning but altering the tone. | It could be a conversation where I submit the suggestions and give the scores as feedback. https://chat.openai.com/c/83219e8b-db76-4763-9298-1bbee6dc1329 https://www.kaggle.com/code/ironbar/mean-prompt-submission Results https://docs.google.com/spreadsheets/d/10MUcBnQporulcX5FZGhsQ7uWtT0WmlekX8NTT3fJNSc/edit#gid=1042558503&range=A1 I was unable to improve the public prompt with around 8 guess from GPT4. Conclusion Next steps TODO [ ]","title":"Iteration 9. Mean prompt optimization"},{"location":"modeling/Iteration_09_mean_prompt_optimization/#iteration-9-mean-prompt-optimization","text":"05-04-2024","title":"Iteration 9. Mean prompt optimization"},{"location":"modeling/Iteration_09_mean_prompt_optimization/#goal","text":"Try to find a mean prompt that scores as high as possible on leaderboard.","title":"Goal"},{"location":"modeling/Iteration_09_mean_prompt_optimization/#motivation","text":"Yesterday a new mean prompt that scores 0.63 was released. I don't believe that is the way but I consider it is worth exploring it whenever I have unused submissions. I remember reading news in the past that said that LLMs can be used for optimization.","title":"Motivation"},{"location":"modeling/Iteration_09_mean_prompt_optimization/#development","text":"","title":"Development"},{"location":"modeling/Iteration_09_mean_prompt_optimization/#optimizing-with-gpt4","text":"The idea is to create a prompt with all the mean prompts and the score so far, and ask GPT4 to create new prompts. I'm trying to find a text prompt that maximizes cosine similarity with a dataset of text prompts. The dataset has text prompts that have been used to rewrite text. Below you can find a table with all the prompts I have tried so far and the mean similarity. Could you use this information and suggest new prompts that could have a higher similarity? Let's solve the problem step by step. 1. Analyze the higher scoring prompts and try to find insights and patterns that increase and decrease similarity 2. Using the insights suggest a single prompt that would score more than 0.63 | LB | prompt | |------|---------------------------------------------------------------------------------------------------------------| | 0.63 | Please improve this text using the writing style with maintaining the original meaning but altering the tone. | It could be a conversation where I submit the suggestions and give the scores as feedback. https://chat.openai.com/c/83219e8b-db76-4763-9298-1bbee6dc1329 https://www.kaggle.com/code/ironbar/mean-prompt-submission","title":"Optimizing with GPT4"},{"location":"modeling/Iteration_09_mean_prompt_optimization/#results","text":"https://docs.google.com/spreadsheets/d/10MUcBnQporulcX5FZGhsQ7uWtT0WmlekX8NTT3fJNSc/edit#gid=1042558503&range=A1 I was unable to improve the public prompt with around 8 guess from GPT4.","title":"Results"},{"location":"modeling/Iteration_09_mean_prompt_optimization/#conclusion","text":"","title":"Conclusion"},{"location":"modeling/Iteration_09_mean_prompt_optimization/#next-steps","text":"","title":"Next steps"},{"location":"modeling/Iteration_09_mean_prompt_optimization/#todo","text":"[ ]","title":"TODO"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/","text":"Iteration 10. Fine-tune on high quality data 08-04-2024 Goal Can I improve my leaderboard score using the recently create high quality data? Motivation I believe I have to forget about mean-prompts and bad T5 embeddings and simply focus on building the best model possible for prompt recovery. It's time to see if the newly created dataset gives better results than previous fine-tunings. Development Results Train just on completions On a first step I have done a first experiment using previous data were the model was trained just on completions . This means that the model will only be trained on the recovery prompts, not on generating the original and rewritten text. ground truth base model v1 v2 v3, just on completions gemma prompt 0.6742 0.7095 0.7062 0.716 gpt4 recovered prompt 0.7005 0.8108 0.8126 0.8245 LB - 0.61 0.61 0.61 We can see an improvement in validation, but do not see improvements on leaderboard. The training dynamics were changed and the model overfitted earlier. This is very likely due to the training output being shorter. On previous fine-tunings the best validation epoch was around 15, while on this training it was around 6. Thus we see an improvement on validation score and also the trainings are faster because the best epoch is achieved faster. First trainings on new data version loss output type data LB score 1 Full CoT 1/2 mooney_test_with_gpt4 0.61 2 Full CoT mooney_test_with_gpt4 0.61 3 Completions CoT mooney_test_with_gpt4 0.61 4 Completions Prompt high_quality_dataset_v1 0.59 5 Completions Prompt high_quality_dataset_v1, mooney_test_with_gpt4 0.62 6 Completions Prompt high_quality_dataset_v1, mooney_test_with_gpt4, gemma_suppl_rewrite_curated_with_gpt4 0.61 v5 model reaches the best LB score so far for any individual model (without any mean prompt combination) This probes that chain of thought (CoT) prompts were not necessary. It seemed like a good idea, but in hindsight we were just saying the same in the thoughts and in the prompt. It might be possible that we need more that if we train on completions and just prompts, because the number of tokens that are used for training is much smaller. Submission with multiprompt Since the model is just predicting a short prompt the submission is much faster. It runs in less than 3 hours. Thus it is possible to make more than one prediction for each sample and concatenate all together. model version single prompt LB score multiprompt LB score 4 0.59 0.61 5 0.62 0.64 6 0.61 0.62 All the experiments improve when making multiple predictions. 0.64 is the best result so far with a single model. This could be a way, but I'm far from 0.70. Do not quantize Mixtral gates I have read that quantizing Mixtral gates could be problematic. Thus I have created a notebook to see if I can avoid that quantization. If we don't quantize the gates and the lm_head of Mixtral the memory usage is almost the same since they are small layers. Just add llm_int8_skip_modules=['gate', 'lm_head'], to the configuration. References: https://huggingface.co/docs/transformers/main_classes/quantization#transformers.QuantoConfig.modules_to_not_convert https://github.com/mobiusml/hqq/issues/2 To see the effect of this change I have launched two experiments: Fine-tune a model without quantizing those layers Resubmit the fork from the forum where I replaced Mistral by Mixtral. The results on leaderboard do not change. The fine-tuned model gets 0.61 and the fork gets 0.62, exactly as before. What if I fine-tune Mistral? Validation loss during training is slightly higher: 0.7679 vs 0.7456 https://www.kaggle.com/datasets/ahmadsaladin/mistral-7b-it-v02 However in LB score I get exactly the same score as Mixtral: 0.61 This is the second evidence against the use of Mixtral, the first one was the forum notebook where I simply replaced Mistral by Mixtral and got the same score. Reducing LoRA size on Mistral lora_r best_val_loss model_slug LB 16 0.6143 mistral_v2 0.62 8 0.6234 4 0.6128 2 0.6128 1 0.5916 mistral_v3 0.62 The size of the weights decreases from 54.6 to 3.4 MB Maybe the task is easy, so we are just learning some kind of \"good prompt\" Maybe with bigger datasets the r becomes relevant Combination of transformers I tried making a submission with different models, but it scored worse than using the same model multiple times. Conclusions By fine-tuning on my own high quality data I was able to reach a LB score of 0.62 , better than the previous 0.61 . Moreover by making multiple inferences with the same model I was able to improve that score to 0.64 . Training with lora r=1 gave same or better results than r=16 , suggesting that this task does not need a big model change. There is no evidence that Mixtral gives better results than Mistral. If I concatenate the model predictions with the mean prompt that scores 0.63 I'm able to reach a LB score of 0.66 Next steps Try using LLama 2 13B, https://www.kaggle.com/models/metaresearch/llama-2/pyTorch/13b-chat-hf Why Mixtral is not getting better results than Mistral? TODO What if MoE does not deal correctly with quantization and should I leave some layers as they are? What if I make multiple predictions with the same model and concatenate them? Or with different adapters? Upload new models to: https://www.kaggle.com/models/ironbar/mixtral-prompt-recovery Create new data with Newtonbaba? The prompts didn't look right Evaluate new dataset New data with multiple prompt instructions. What if I fine-tune Mistral instead of Mixtral?","title":"Iteration 10. Fine-tune on high quality data"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#iteration-10-fine-tune-on-high-quality-data","text":"08-04-2024","title":"Iteration 10. Fine-tune on high quality data"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#goal","text":"Can I improve my leaderboard score using the recently create high quality data?","title":"Goal"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#motivation","text":"I believe I have to forget about mean-prompts and bad T5 embeddings and simply focus on building the best model possible for prompt recovery. It's time to see if the newly created dataset gives better results than previous fine-tunings.","title":"Motivation"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#development","text":"","title":"Development"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#results","text":"","title":"Results"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#train-just-on-completions","text":"On a first step I have done a first experiment using previous data were the model was trained just on completions . This means that the model will only be trained on the recovery prompts, not on generating the original and rewritten text. ground truth base model v1 v2 v3, just on completions gemma prompt 0.6742 0.7095 0.7062 0.716 gpt4 recovered prompt 0.7005 0.8108 0.8126 0.8245 LB - 0.61 0.61 0.61 We can see an improvement in validation, but do not see improvements on leaderboard. The training dynamics were changed and the model overfitted earlier. This is very likely due to the training output being shorter. On previous fine-tunings the best validation epoch was around 15, while on this training it was around 6. Thus we see an improvement on validation score and also the trainings are faster because the best epoch is achieved faster.","title":"Train just on completions"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#first-trainings-on-new-data","text":"version loss output type data LB score 1 Full CoT 1/2 mooney_test_with_gpt4 0.61 2 Full CoT mooney_test_with_gpt4 0.61 3 Completions CoT mooney_test_with_gpt4 0.61 4 Completions Prompt high_quality_dataset_v1 0.59 5 Completions Prompt high_quality_dataset_v1, mooney_test_with_gpt4 0.62 6 Completions Prompt high_quality_dataset_v1, mooney_test_with_gpt4, gemma_suppl_rewrite_curated_with_gpt4 0.61 v5 model reaches the best LB score so far for any individual model (without any mean prompt combination) This probes that chain of thought (CoT) prompts were not necessary. It seemed like a good idea, but in hindsight we were just saying the same in the thoughts and in the prompt. It might be possible that we need more that if we train on completions and just prompts, because the number of tokens that are used for training is much smaller.","title":"First trainings on new data"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#submission-with-multiprompt","text":"Since the model is just predicting a short prompt the submission is much faster. It runs in less than 3 hours. Thus it is possible to make more than one prediction for each sample and concatenate all together. model version single prompt LB score multiprompt LB score 4 0.59 0.61 5 0.62 0.64 6 0.61 0.62 All the experiments improve when making multiple predictions. 0.64 is the best result so far with a single model. This could be a way, but I'm far from 0.70.","title":"Submission with multiprompt"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#do-not-quantize-mixtral-gates","text":"I have read that quantizing Mixtral gates could be problematic. Thus I have created a notebook to see if I can avoid that quantization. If we don't quantize the gates and the lm_head of Mixtral the memory usage is almost the same since they are small layers. Just add llm_int8_skip_modules=['gate', 'lm_head'], to the configuration. References: https://huggingface.co/docs/transformers/main_classes/quantization#transformers.QuantoConfig.modules_to_not_convert https://github.com/mobiusml/hqq/issues/2 To see the effect of this change I have launched two experiments: Fine-tune a model without quantizing those layers Resubmit the fork from the forum where I replaced Mistral by Mixtral. The results on leaderboard do not change. The fine-tuned model gets 0.61 and the fork gets 0.62, exactly as before.","title":"Do not quantize Mixtral gates"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#what-if-i-fine-tune-mistral","text":"Validation loss during training is slightly higher: 0.7679 vs 0.7456 https://www.kaggle.com/datasets/ahmadsaladin/mistral-7b-it-v02 However in LB score I get exactly the same score as Mixtral: 0.61 This is the second evidence against the use of Mixtral, the first one was the forum notebook where I simply replaced Mistral by Mixtral and got the same score.","title":"What if I fine-tune Mistral?"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#reducing-lora-size-on-mistral","text":"lora_r best_val_loss model_slug LB 16 0.6143 mistral_v2 0.62 8 0.6234 4 0.6128 2 0.6128 1 0.5916 mistral_v3 0.62 The size of the weights decreases from 54.6 to 3.4 MB Maybe the task is easy, so we are just learning some kind of \"good prompt\" Maybe with bigger datasets the r becomes relevant","title":"Reducing LoRA size on Mistral"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#combination-of-transformers","text":"I tried making a submission with different models, but it scored worse than using the same model multiple times.","title":"Combination of transformers"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#conclusions","text":"By fine-tuning on my own high quality data I was able to reach a LB score of 0.62 , better than the previous 0.61 . Moreover by making multiple inferences with the same model I was able to improve that score to 0.64 . Training with lora r=1 gave same or better results than r=16 , suggesting that this task does not need a big model change. There is no evidence that Mixtral gives better results than Mistral. If I concatenate the model predictions with the mean prompt that scores 0.63 I'm able to reach a LB score of 0.66","title":"Conclusions"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#next-steps","text":"Try using LLama 2 13B, https://www.kaggle.com/models/metaresearch/llama-2/pyTorch/13b-chat-hf Why Mixtral is not getting better results than Mistral?","title":"Next steps"},{"location":"modeling/Iteration_10_fine-tune_on_high_quality_data/#todo","text":"What if MoE does not deal correctly with quantization and should I leave some layers as they are? What if I make multiple predictions with the same model and concatenate them? Or with different adapters? Upload new models to: https://www.kaggle.com/models/ironbar/mixtral-prompt-recovery Create new data with Newtonbaba? The prompts didn't look right Evaluate new dataset New data with multiple prompt instructions. What if I fine-tune Mistral instead of Mixtral?","title":"TODO"},{"location":"modeling/Iteration_11_llama_2/","text":"Iteration 11. Llama 2 10-04-2024 Goal Can we get better results if we switch from Mistral to Llama 2? Motivation My initial model choice for the challenge was to use Mixtral because it was the biggest and most capable model that could be used to make a submission for the challenge. However I haven't found evidences that Mixtral is better than Mistral for this challenge. Two different experiments gave the same results to Mistral 7B and Mixtral 8x7B. Maybe the MoE is difficult to fine-tune. Thus I have decided to try with Llama 2 13b chat because it might give better results. Development Problem when running quantized Llama After quantization I get RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x5120 and 1x6912) If I change \"pretraining_tp\": 2 to \"pretraining_tp\": 1 in the config.json it works! A simpler solution is to do model.config.pretraining_tp = 1 after loading the model. Reference: https://discuss.huggingface.co/t/llama2-finetuning-giving-error-mat1-and-mat2-shapes-cannot-be-multiplied-4096x5120-and-1x2560/47466/7 Llama 2 references The prompt format is very similar to Mistral, with the addition of the system prompt. For one way conversations: <s>[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_message }} [/INST] For multi-turn conversations: <s>[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST] https://www.kaggle.com/models/metaresearch/llama-2/PyTorch/13b-chat-hf https://replicate.com/blog/how-to-prompt-llama https://huggingface.co/docs/transformers/en/model_doc/llama2 Results Forum notebook for with Llama Notebook I get the same 0.62 score as with Mistral and Mixtral, maybe the model is not relevant... Fine-tune Llama Let's fine-tune Llama and compare to Mistral and Mixtral. I would like to do 2 trainings, one with sys prompt and another without. My guess is that the results will be very similar. The LB score is 0.61, same as Mixtral but lower than Mistral (0.62) althought the differences are not likely to be significative. I have started training with system prompt, but after 200 steps the loss was almost identical to using Mistral prompt so I have decided to stop the training. Fine-tuning parameters https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing This example uses a learning rate of 2e-4, which is 10 times bigger than 2e-5 used for Mixtral. https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing This other example uses a learning rate of 1e-4. The validation result is the same with a learning rate of 2e-5 and 4e-5. Conclusion We do not get better results with Llama. So far it seems that the chosen LLM is irrelevant. I have to try with more models like Phi-2 and Gemma. Next steps Other models https://www.databricks.com/blog/mpt-30b Falcon 40b Gemma 7b https://www.kaggle.com/models/google/gemma/transformers/1.1-7b-it Phi-2. https://www.kaggle.com/models/Microsoft/phi/Transformers/2 I might the tune the lora hyperparameters, but maybe on Mistral 7B to be faster. Create a python training script so I can run multiple trainings at night","title":"Iteration 11. Llama 2"},{"location":"modeling/Iteration_11_llama_2/#iteration-11-llama-2","text":"10-04-2024","title":"Iteration 11. Llama 2"},{"location":"modeling/Iteration_11_llama_2/#goal","text":"Can we get better results if we switch from Mistral to Llama 2?","title":"Goal"},{"location":"modeling/Iteration_11_llama_2/#motivation","text":"My initial model choice for the challenge was to use Mixtral because it was the biggest and most capable model that could be used to make a submission for the challenge. However I haven't found evidences that Mixtral is better than Mistral for this challenge. Two different experiments gave the same results to Mistral 7B and Mixtral 8x7B. Maybe the MoE is difficult to fine-tune. Thus I have decided to try with Llama 2 13b chat because it might give better results.","title":"Motivation"},{"location":"modeling/Iteration_11_llama_2/#development","text":"","title":"Development"},{"location":"modeling/Iteration_11_llama_2/#problem-when-running-quantized-llama","text":"After quantization I get RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x5120 and 1x6912) If I change \"pretraining_tp\": 2 to \"pretraining_tp\": 1 in the config.json it works! A simpler solution is to do model.config.pretraining_tp = 1 after loading the model. Reference: https://discuss.huggingface.co/t/llama2-finetuning-giving-error-mat1-and-mat2-shapes-cannot-be-multiplied-4096x5120-and-1x2560/47466/7","title":"Problem when running quantized Llama"},{"location":"modeling/Iteration_11_llama_2/#llama-2-references","text":"The prompt format is very similar to Mistral, with the addition of the system prompt. For one way conversations: <s>[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_message }} [/INST] For multi-turn conversations: <s>[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST] https://www.kaggle.com/models/metaresearch/llama-2/PyTorch/13b-chat-hf https://replicate.com/blog/how-to-prompt-llama https://huggingface.co/docs/transformers/en/model_doc/llama2","title":"Llama 2 references"},{"location":"modeling/Iteration_11_llama_2/#results","text":"","title":"Results"},{"location":"modeling/Iteration_11_llama_2/#forum-notebook-for-with-llama","text":"Notebook I get the same 0.62 score as with Mistral and Mixtral, maybe the model is not relevant...","title":"Forum notebook for with Llama"},{"location":"modeling/Iteration_11_llama_2/#fine-tune-llama","text":"Let's fine-tune Llama and compare to Mistral and Mixtral. I would like to do 2 trainings, one with sys prompt and another without. My guess is that the results will be very similar. The LB score is 0.61, same as Mixtral but lower than Mistral (0.62) althought the differences are not likely to be significative. I have started training with system prompt, but after 200 steps the loss was almost identical to using Mistral prompt so I have decided to stop the training.","title":"Fine-tune Llama"},{"location":"modeling/Iteration_11_llama_2/#fine-tuning-parameters","text":"https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing This example uses a learning rate of 2e-4, which is 10 times bigger than 2e-5 used for Mixtral. https://colab.research.google.com/drive/1SYpgFpcmtIUzdE7pxqknrM4ArCASfkFQ?usp=sharing This other example uses a learning rate of 1e-4. The validation result is the same with a learning rate of 2e-5 and 4e-5.","title":"Fine-tuning parameters"},{"location":"modeling/Iteration_11_llama_2/#conclusion","text":"We do not get better results with Llama. So far it seems that the chosen LLM is irrelevant. I have to try with more models like Phi-2 and Gemma.","title":"Conclusion"},{"location":"modeling/Iteration_11_llama_2/#next-steps","text":"Other models https://www.databricks.com/blog/mpt-30b Falcon 40b Gemma 7b https://www.kaggle.com/models/google/gemma/transformers/1.1-7b-it Phi-2. https://www.kaggle.com/models/Microsoft/phi/Transformers/2 I might the tune the lora hyperparameters, but maybe on Mistral 7B to be faster. Create a python training script so I can run multiple trainings at night","title":"Next steps"},{"location":"modeling/Iteration_12_other_models/","text":"Iteration 12. Fine-tuning Phi-2 and Gemma 11-04-2024 Goal Try fine-tuning and making predictions with Phi-2 and Gemma-7b-it . Motivation My experiments with Mistral, Mixtral and LLama 13b show tiny differences between the models. Maybe I can get the same results using Phi-2 and Gemma-7b-it. If that is the case it is likely that an ensemble using all the models would score better than making multiple submissions with the same model. Also a new Mistral-22B model is out that it is worth trying. Development The idea is to use the models in transformer format so I can reuse the code for fine-tuning previous models. I would have to look at the different prompt format of the new models. Prompt format Phi-2 https://www.kaggle.com/models/Microsoft/phi/Transformers/2 Instruct: Write a detailed analogy between mathematics and a lighthouse. Output: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us. <|endoftext|> Gemma https://www.promptingguide.ai/models/gemma <start_of_turn>user knock knock<end_of_turn> <start_of_turn>model who is there<end_of_turn> <start_of_turn>user Gemma<end_of_turn> <start_of_turn>model Gemma who?<end_of_turn><eos> Mistral-22B https://huggingface.co/Vezora/Mistral-22B-v0.2 <s>### System: You are a helpful assistant. ### Human: Give me the best chili recipe you can ### Assistant: Here is the best chili recipe...</s> Results These are the results of using fine-tuned models on the same data. Despite the number of parameters being different they all score around the same. model LB score Mistral 7B 0.62 Llama 2 13B 0.61 Mistral 22B 0.62 Mixtral 8x7B 0.61 I haven't made a submission with Gemma or Phi because considering that all other models score almost the same it is not promising and I have few submissions left. Conclusion Next steps TODO [ ]","title":"Iteration 12. Fine-tuning Phi-2 and Gemma"},{"location":"modeling/Iteration_12_other_models/#iteration-12-fine-tuning-phi-2-and-gemma","text":"11-04-2024","title":"Iteration 12. Fine-tuning Phi-2 and Gemma"},{"location":"modeling/Iteration_12_other_models/#goal","text":"Try fine-tuning and making predictions with Phi-2 and Gemma-7b-it .","title":"Goal"},{"location":"modeling/Iteration_12_other_models/#motivation","text":"My experiments with Mistral, Mixtral and LLama 13b show tiny differences between the models. Maybe I can get the same results using Phi-2 and Gemma-7b-it. If that is the case it is likely that an ensemble using all the models would score better than making multiple submissions with the same model. Also a new Mistral-22B model is out that it is worth trying.","title":"Motivation"},{"location":"modeling/Iteration_12_other_models/#development","text":"The idea is to use the models in transformer format so I can reuse the code for fine-tuning previous models. I would have to look at the different prompt format of the new models.","title":"Development"},{"location":"modeling/Iteration_12_other_models/#prompt-format","text":"","title":"Prompt format"},{"location":"modeling/Iteration_12_other_models/#phi-2","text":"https://www.kaggle.com/models/Microsoft/phi/Transformers/2 Instruct: Write a detailed analogy between mathematics and a lighthouse. Output: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us. <|endoftext|>","title":"Phi-2"},{"location":"modeling/Iteration_12_other_models/#gemma","text":"https://www.promptingguide.ai/models/gemma <start_of_turn>user knock knock<end_of_turn> <start_of_turn>model who is there<end_of_turn> <start_of_turn>user Gemma<end_of_turn> <start_of_turn>model Gemma who?<end_of_turn><eos>","title":"Gemma"},{"location":"modeling/Iteration_12_other_models/#mistral-22b","text":"https://huggingface.co/Vezora/Mistral-22B-v0.2 <s>### System: You are a helpful assistant. ### Human: Give me the best chili recipe you can ### Assistant: Here is the best chili recipe...</s>","title":"Mistral-22B"},{"location":"modeling/Iteration_12_other_models/#results","text":"These are the results of using fine-tuned models on the same data. Despite the number of parameters being different they all score around the same. model LB score Mistral 7B 0.62 Llama 2 13B 0.61 Mistral 22B 0.62 Mixtral 8x7B 0.61 I haven't made a submission with Gemma or Phi because considering that all other models score almost the same it is not promising and I have few submissions left.","title":"Results"},{"location":"modeling/Iteration_12_other_models/#conclusion","text":"","title":"Conclusion"},{"location":"modeling/Iteration_12_other_models/#next-steps","text":"","title":"Next steps"},{"location":"modeling/Iteration_12_other_models/#todo","text":"[ ]","title":"TODO"},{"location":"modeling/Iteration_13_datacentric_mistral/","text":"Iteration 13. Data centric approach around Mistral 7B 11-04-2024 Goal What is the highest LB score I can get with Mistral 7B? Motivation Training with Mistral is much faster than training with Mixtral. And the leaderboard results are the same. On this iteration I'm going to change the training data and try to get the best possible LB score. This is a continuation of iterations 8 and 10, but focusing on Mistral and the data. Development Explore public datasets dataset n n_prompts ratio median_tokens dipamc77_prompts_0_500_wiki_first_para_3000_curated 2872 494 0.17 215 gemma_suppl_rewrite_curated 298 189 0.63 171 nbroad-v1_curated 2162 109 0.05 1120 nbroad-v2_curated 2400 2400 1.00 1135 winddude_70k_gemma_template_built_curated 69487 61947 0.89 666 aishaalmahmoud/llm_dataset_1_curated 9842 839 0.09 197 aishaalmahmoud/llm_dataset_20k_curated 16620 860 0.05 198 alexxxsem/data_gemma_0_1000_curated 994 41 0.04 174 galileo/gemma_v1_7b-it_curated 26160 1469 0.06 427 newtonbaba/gemini_data_set_prompt_recover_3_curated 1802 1778 0.99 198 newtonbaba/gemma_data_set_prompt_recover_1_curated 994 766 0.77 418 newtonbaba/gemma_data_set_prompt_recover_2_curated 1530 1530 1.00 604 Except for nbroad the rest of the datasets have a reasonable number of tokens I'm going to train in all the datasets and see what is the leaderboard and validation score. If I group the datasets by creator that would be 8 experiments. Wandb With the code below it is possible to log different runs in the same notebook to Weights and Bias . import wandb w = wandb.init(reinit=True, project='datacentric_mistral', name=experiment_name) ... w.finish() Results Train on public datasets dataset rows promtps val loss LB score newtonbaba 2796 2544 2.32 0.55 nbroad 4562 2509 2.4 0.53 galileo 26160 1469 3 0.52 alexxxsem 994 41 4.74 0.52 aishallmahmoud 26000 1700 2.57 0.52 winddude 69487 61947 2.87 0.53 dipacmc77 2872 494 3.22 0.53 gemma_suppl_rewrite 298 189 3.33 0.60 mooney_test_with_gpt4 359 - - 0.61 high_quality_dataset_v1 280 - - 0.59 high_quality_datases + mooney_test_with_gpt4 + gemma_suppl_rewrite_curated_with_gpt4 1200 - - 0.62 Size of the dataset seems to be irrelevant All the public datasets except gemma_suppl_rewrite seem to be useless. Training on my data probed to be the best solution so far The training data is very important, so maybe be creating better training data I can improve my LB score Validation loss The validation loss diverged after step 50, that is 1600 train samples (batch size 32) I might have to add a custom metric There is no relation between validation loss and LB score, this is bad Conclusion All the public datasets except gemma_suppl_rewrite seem to be useless. I was able to get a LB score of 0.60 using that dataset. This is a worse score to 0.62 when using my own data, so I should double down on creating more high quality data. Next steps Create more high quality data and fine-tune Mistral. TODO Is there any useful public dataset that I can use directly? Measure text length and prompt diversity. Collect useful prompts from other datasets Generate samples with multi-instruction prompts (similar to leaked data) New notebook for sequential training Gain more control over wandb for easier inspection Increase batch size and maybe context length Make wandb work with sequential runs on the same notebook","title":"Iteration 13. Data centric approach around Mistral 7B"},{"location":"modeling/Iteration_13_datacentric_mistral/#iteration-13-data-centric-approach-around-mistral-7b","text":"11-04-2024","title":"Iteration 13. Data centric approach around Mistral 7B"},{"location":"modeling/Iteration_13_datacentric_mistral/#goal","text":"What is the highest LB score I can get with Mistral 7B?","title":"Goal"},{"location":"modeling/Iteration_13_datacentric_mistral/#motivation","text":"Training with Mistral is much faster than training with Mixtral. And the leaderboard results are the same. On this iteration I'm going to change the training data and try to get the best possible LB score. This is a continuation of iterations 8 and 10, but focusing on Mistral and the data.","title":"Motivation"},{"location":"modeling/Iteration_13_datacentric_mistral/#development","text":"","title":"Development"},{"location":"modeling/Iteration_13_datacentric_mistral/#explore-public-datasets","text":"dataset n n_prompts ratio median_tokens dipamc77_prompts_0_500_wiki_first_para_3000_curated 2872 494 0.17 215 gemma_suppl_rewrite_curated 298 189 0.63 171 nbroad-v1_curated 2162 109 0.05 1120 nbroad-v2_curated 2400 2400 1.00 1135 winddude_70k_gemma_template_built_curated 69487 61947 0.89 666 aishaalmahmoud/llm_dataset_1_curated 9842 839 0.09 197 aishaalmahmoud/llm_dataset_20k_curated 16620 860 0.05 198 alexxxsem/data_gemma_0_1000_curated 994 41 0.04 174 galileo/gemma_v1_7b-it_curated 26160 1469 0.06 427 newtonbaba/gemini_data_set_prompt_recover_3_curated 1802 1778 0.99 198 newtonbaba/gemma_data_set_prompt_recover_1_curated 994 766 0.77 418 newtonbaba/gemma_data_set_prompt_recover_2_curated 1530 1530 1.00 604 Except for nbroad the rest of the datasets have a reasonable number of tokens I'm going to train in all the datasets and see what is the leaderboard and validation score. If I group the datasets by creator that would be 8 experiments.","title":"Explore public datasets"},{"location":"modeling/Iteration_13_datacentric_mistral/#wandb","text":"With the code below it is possible to log different runs in the same notebook to Weights and Bias . import wandb w = wandb.init(reinit=True, project='datacentric_mistral', name=experiment_name) ... w.finish()","title":"Wandb"},{"location":"modeling/Iteration_13_datacentric_mistral/#results","text":"","title":"Results"},{"location":"modeling/Iteration_13_datacentric_mistral/#train-on-public-datasets","text":"dataset rows promtps val loss LB score newtonbaba 2796 2544 2.32 0.55 nbroad 4562 2509 2.4 0.53 galileo 26160 1469 3 0.52 alexxxsem 994 41 4.74 0.52 aishallmahmoud 26000 1700 2.57 0.52 winddude 69487 61947 2.87 0.53 dipacmc77 2872 494 3.22 0.53 gemma_suppl_rewrite 298 189 3.33 0.60 mooney_test_with_gpt4 359 - - 0.61 high_quality_dataset_v1 280 - - 0.59 high_quality_datases + mooney_test_with_gpt4 + gemma_suppl_rewrite_curated_with_gpt4 1200 - - 0.62 Size of the dataset seems to be irrelevant All the public datasets except gemma_suppl_rewrite seem to be useless. Training on my data probed to be the best solution so far The training data is very important, so maybe be creating better training data I can improve my LB score","title":"Train on public datasets"},{"location":"modeling/Iteration_13_datacentric_mistral/#validation-loss","text":"The validation loss diverged after step 50, that is 1600 train samples (batch size 32) I might have to add a custom metric There is no relation between validation loss and LB score, this is bad","title":"Validation loss"},{"location":"modeling/Iteration_13_datacentric_mistral/#conclusion","text":"All the public datasets except gemma_suppl_rewrite seem to be useless. I was able to get a LB score of 0.60 using that dataset. This is a worse score to 0.62 when using my own data, so I should double down on creating more high quality data.","title":"Conclusion"},{"location":"modeling/Iteration_13_datacentric_mistral/#next-steps","text":"Create more high quality data and fine-tune Mistral.","title":"Next steps"},{"location":"modeling/Iteration_13_datacentric_mistral/#todo","text":"Is there any useful public dataset that I can use directly? Measure text length and prompt diversity. Collect useful prompts from other datasets Generate samples with multi-instruction prompts (similar to leaked data) New notebook for sequential training Gain more control over wandb for easier inspection Increase batch size and maybe context length Make wandb work with sequential runs on the same notebook","title":"TODO"},{"location":"modeling/Iteration_14_more_high_quality_data/","text":"Iteration 14. More high quality data 12-04-2024 Goal Can I get a score higher than 0.62 by fine-tuning Mistral 7B on my own data? Motivation Fine-tuning Mistral 7B has shown that: My own data gives better results than public data Small datasets can give good results Thus I should focus on improving my datasets. Development Multi instruction theory Let's think which elements can be required to be changed on a rewritten text: length: extend, shorten, summarize, less than n words... tone: formal, informal, humorous, professional, persuasive, optimistic, sarcastic, inspirational... style: academic, fiction, in the style of some famous author, poem, rhyme, blog, journal, news remove: remove some parts of the text additions: add new elements to the text, f.e. questions, calls to action transformations: translate to other language, format as markdown, encode perspective: first person, third, change the main character audience: Adjusting the text to cater to a specific demographic, professional group, age range, cultural background, etc. Grammar and Syntax: Adjusting sentence structure for better readability or to fit stylistic preferences; correcting grammatical errors. Clarity and Precision: Enhancing the text to be clearer or more precise, which may involve rephrasing technical jargon, simplifying complex concepts, or providing additional explanations where needed. Legal and Ethical Compliance: Modifying text to comply with legal or ethical standards, particularly in marketing, health-related content, and public communications. Emphasis and Highlighting: Altering the text to emphasize key points or ideas, which might involve using bold or italic formatting, or restructuring sections to highlight critical information. Creating more data from the leaked examples with GPT4 I want to create a dataset of original and rewritten texts. The first step is to gather a diverse set of text prompts that would be used later to transform the original text into the rewritten text. Given the following examples, can you create 20 new prompts? ## Examples - Update this email to make it more typical and incorporate the topic of \"incentive\". The new version should be shorter than 53 words long - Rephrase this memo to evoke a sense of concern and incorporate the topic \"acre\", ensuring that the tone reflects worry and apprehension. The new version should be shorter than 53 words long. - Rework this text message to infuse it with a playful and humorous complaint, adding a touch of whimsy and lightness to the conversation ChatGPT is an amazing tool for this task. I'm able to generate realistic new data very easily. Creating multi-instruction prompts with GPT4 Let's try to do the same, but giving simple prompts and asking GPT4 to create new multi instruction prompts. I want to create a dataset of original and rewritten texts. The first step is to gather a diverse set of text prompts that would be used later to transform the original text into the rewritten text. Your task is to generate 20 realistic prompts. Below you can find a list with all the posible changes that can be required to be changed on a text. For each prompt pick randomly 2 or 3 types of changes from the list. I have listed also some sample prompts for reference Take in mind that the original texts will have less than 200 words. ## Theory This are the possible changes that can be done to a text. After each change some examples or explanations will follow. Do not limit yourself to this examples. - length: extend, shorten, summarize, less than n words... - tone: formal, informal, humorous, professional... - style: academic, fiction, in the style of some author, poem, rhyme - remove: remove some parts of the text - additions: add new elements to the text, f.e. questions, calls to action - transformations: translate to other language, format as markdown, encode - perspective: first person, third, change the main character - audience: Adjusting the text to cater to a specific demographic, professional group, age range, cultural background, etc. - Grammar and Syntax: Adjusting sentence structure for better readability or to fit stylistic preferences; correcting grammatical errors. - Clarity and Precision: Enhancing the text to be clearer or more precise, which may involve rephrasing technical jargon, simplifying complex concepts, or providing additional explanations where needed. - Legal and Ethical Compliance: Modifying text to comply with legal or ethical standards, particularly in marketing, health-related content, and public communications. - Emphasis and Highlighting: Altering the text to emphasize key points or ideas, which might involve using bold or italic formatting, or restructuring sections to highlight critical information. ## Sample prompts Create prompts for consistency, legal, DEI, SEO... I want to create a dataset of original and rewritten texts. The first step is to gather a diverse set of text prompts that would be used later to transform the original text into the rewritten text. Your task is to generate 20 realistic prompts. Can you create them with the following topic? Consistency: Ensuring that the text maintains consistent use of terminology, voice, and narrative points of view, which can be particularly important in longer texts or texts that have been edited by multiple people. Train parameters I believe I should use r=1 on LoRA because that gave very good LB results. I'm going to simplify the prompt because the previous prompt constrained the solution to be a single sentence and now that won't be always the case. The new prompt is minimalistic. I will be using high_quality_dataset_v1 for validation as in the previous data-centric iteration. On inference I should increase the max tokens to around 50. Create dataset with hints The leaked data showed many rewritten texts starting by words that hinted the prompt. I'm going to manually modify some of my data to include those hints. Results experiment LB score 60 prompts imitating leaked ones 0.61 dataset with prompt hints 0.61 multi instruction dataset 0.61 new prompts 0.61 all data together 0.6 None of the trainings using new data improved the LB score. All the trainings were done with Mistral, that has a best LB score of 0.62. Conclusion This iteration was very disappointing. I had the hope that by tweaking the data I would have been able to improve the LB score but all the variations I tried failed. Currently I'm out of ideas for improving my data. I believe it is very complete. I could scale it but I don't know how to improve it. Next steps I know that model does not seem to be relevant, but Mistral-22B is out! https://huggingface.co/Vezora/Mistral-22B-v0.2 I should try it. Prompt tuning. Maybe I don't need to fine-tune the model, but do prompt tuning. Prompt variations. What if I use GPT4 to rewrite the prompts and use those new variations for training. I should verify that T5 similarity is high. That way each training sample would have multiple outputs. The problem might be better defined. TODO Reread my prompts Read prompts from other relevant dataset: mooney leaked, supp_rewrite. I don't see anything special. Use ChatGPT to come up with more prompts -> autobrainstorm Multi-instruction prompts (inspired on leaked data)","title":"Iteration 14. More high quality data"},{"location":"modeling/Iteration_14_more_high_quality_data/#iteration-14-more-high-quality-data","text":"12-04-2024","title":"Iteration 14. More high quality data"},{"location":"modeling/Iteration_14_more_high_quality_data/#goal","text":"Can I get a score higher than 0.62 by fine-tuning Mistral 7B on my own data?","title":"Goal"},{"location":"modeling/Iteration_14_more_high_quality_data/#motivation","text":"Fine-tuning Mistral 7B has shown that: My own data gives better results than public data Small datasets can give good results Thus I should focus on improving my datasets.","title":"Motivation"},{"location":"modeling/Iteration_14_more_high_quality_data/#development","text":"","title":"Development"},{"location":"modeling/Iteration_14_more_high_quality_data/#multi-instruction-theory","text":"Let's think which elements can be required to be changed on a rewritten text: length: extend, shorten, summarize, less than n words... tone: formal, informal, humorous, professional, persuasive, optimistic, sarcastic, inspirational... style: academic, fiction, in the style of some famous author, poem, rhyme, blog, journal, news remove: remove some parts of the text additions: add new elements to the text, f.e. questions, calls to action transformations: translate to other language, format as markdown, encode perspective: first person, third, change the main character audience: Adjusting the text to cater to a specific demographic, professional group, age range, cultural background, etc. Grammar and Syntax: Adjusting sentence structure for better readability or to fit stylistic preferences; correcting grammatical errors. Clarity and Precision: Enhancing the text to be clearer or more precise, which may involve rephrasing technical jargon, simplifying complex concepts, or providing additional explanations where needed. Legal and Ethical Compliance: Modifying text to comply with legal or ethical standards, particularly in marketing, health-related content, and public communications. Emphasis and Highlighting: Altering the text to emphasize key points or ideas, which might involve using bold or italic formatting, or restructuring sections to highlight critical information.","title":"Multi instruction theory"},{"location":"modeling/Iteration_14_more_high_quality_data/#creating-more-data-from-the-leaked-examples-with-gpt4","text":"I want to create a dataset of original and rewritten texts. The first step is to gather a diverse set of text prompts that would be used later to transform the original text into the rewritten text. Given the following examples, can you create 20 new prompts? ## Examples - Update this email to make it more typical and incorporate the topic of \"incentive\". The new version should be shorter than 53 words long - Rephrase this memo to evoke a sense of concern and incorporate the topic \"acre\", ensuring that the tone reflects worry and apprehension. The new version should be shorter than 53 words long. - Rework this text message to infuse it with a playful and humorous complaint, adding a touch of whimsy and lightness to the conversation ChatGPT is an amazing tool for this task. I'm able to generate realistic new data very easily.","title":"Creating more data from the leaked examples with GPT4"},{"location":"modeling/Iteration_14_more_high_quality_data/#creating-multi-instruction-prompts-with-gpt4","text":"Let's try to do the same, but giving simple prompts and asking GPT4 to create new multi instruction prompts. I want to create a dataset of original and rewritten texts. The first step is to gather a diverse set of text prompts that would be used later to transform the original text into the rewritten text. Your task is to generate 20 realistic prompts. Below you can find a list with all the posible changes that can be required to be changed on a text. For each prompt pick randomly 2 or 3 types of changes from the list. I have listed also some sample prompts for reference Take in mind that the original texts will have less than 200 words. ## Theory This are the possible changes that can be done to a text. After each change some examples or explanations will follow. Do not limit yourself to this examples. - length: extend, shorten, summarize, less than n words... - tone: formal, informal, humorous, professional... - style: academic, fiction, in the style of some author, poem, rhyme - remove: remove some parts of the text - additions: add new elements to the text, f.e. questions, calls to action - transformations: translate to other language, format as markdown, encode - perspective: first person, third, change the main character - audience: Adjusting the text to cater to a specific demographic, professional group, age range, cultural background, etc. - Grammar and Syntax: Adjusting sentence structure for better readability or to fit stylistic preferences; correcting grammatical errors. - Clarity and Precision: Enhancing the text to be clearer or more precise, which may involve rephrasing technical jargon, simplifying complex concepts, or providing additional explanations where needed. - Legal and Ethical Compliance: Modifying text to comply with legal or ethical standards, particularly in marketing, health-related content, and public communications. - Emphasis and Highlighting: Altering the text to emphasize key points or ideas, which might involve using bold or italic formatting, or restructuring sections to highlight critical information. ## Sample prompts","title":"Creating multi-instruction prompts with GPT4"},{"location":"modeling/Iteration_14_more_high_quality_data/#create-prompts-for-consistency-legal-dei-seo","text":"I want to create a dataset of original and rewritten texts. The first step is to gather a diverse set of text prompts that would be used later to transform the original text into the rewritten text. Your task is to generate 20 realistic prompts. Can you create them with the following topic? Consistency: Ensuring that the text maintains consistent use of terminology, voice, and narrative points of view, which can be particularly important in longer texts or texts that have been edited by multiple people.","title":"Create prompts for consistency, legal, DEI, SEO..."},{"location":"modeling/Iteration_14_more_high_quality_data/#train-parameters","text":"I believe I should use r=1 on LoRA because that gave very good LB results. I'm going to simplify the prompt because the previous prompt constrained the solution to be a single sentence and now that won't be always the case. The new prompt is minimalistic. I will be using high_quality_dataset_v1 for validation as in the previous data-centric iteration. On inference I should increase the max tokens to around 50.","title":"Train parameters"},{"location":"modeling/Iteration_14_more_high_quality_data/#create-dataset-with-hints","text":"The leaked data showed many rewritten texts starting by words that hinted the prompt. I'm going to manually modify some of my data to include those hints.","title":"Create dataset with hints"},{"location":"modeling/Iteration_14_more_high_quality_data/#results","text":"experiment LB score 60 prompts imitating leaked ones 0.61 dataset with prompt hints 0.61 multi instruction dataset 0.61 new prompts 0.61 all data together 0.6 None of the trainings using new data improved the LB score. All the trainings were done with Mistral, that has a best LB score of 0.62.","title":"Results"},{"location":"modeling/Iteration_14_more_high_quality_data/#conclusion","text":"This iteration was very disappointing. I had the hope that by tweaking the data I would have been able to improve the LB score but all the variations I tried failed. Currently I'm out of ideas for improving my data. I believe it is very complete. I could scale it but I don't know how to improve it.","title":"Conclusion"},{"location":"modeling/Iteration_14_more_high_quality_data/#next-steps","text":"I know that model does not seem to be relevant, but Mistral-22B is out! https://huggingface.co/Vezora/Mistral-22B-v0.2 I should try it. Prompt tuning. Maybe I don't need to fine-tune the model, but do prompt tuning. Prompt variations. What if I use GPT4 to rewrite the prompts and use those new variations for training. I should verify that T5 similarity is high. That way each training sample would have multiple outputs. The problem might be better defined.","title":"Next steps"},{"location":"modeling/Iteration_14_more_high_quality_data/#todo","text":"Reread my prompts Read prompts from other relevant dataset: mooney leaked, supp_rewrite. I don't see anything special. Use ChatGPT to come up with more prompts -> autobrainstorm Multi-instruction prompts (inspired on leaked data)","title":"TODO"},{"location":"modeling/Iteration_15_prompt_tuning/","text":"Iteration 15. Prompt tuning 14-04-2024 Goal Can I improve the LB score by using prompt tuning? Motivation So far I have no seen leaderboard differences when using different models. I'm able to fine-tune them and learn the train data without any problem but it is hard to improve the LB score. One interesting thing is that I'm able to overfit on the train data using LoRA and just r=1 . Do I really need fine-tuning for this task? Maybe we can simply learn a good prompt and really on the power of the model to do the task. I believe it is unlikely that with the current scale of data (~1k) the model is going to learn a new task. Thus using the smallest amount possible of parameters could be better. Development How to do prompt fine-tuning Fine-Tuning Models using Prompt-Tuning with Hugging Face\u2019s PEFT Library . This seems very similar to the LoRA fine-tuning. Prompt tuning for causal language modeling, official Hugging Face Prompt tuning official documentation Experiment design The idea is to take Mistral-7B and do prompt fine-tuning as similar as possible as fine-tuning. I will compare the results to the previous iteration centered on data. Inference It seems that load adapter does not work. I have to use the PEFTModel and created a new notebook to make this submissions. Results Shuffle the train dataset When running multiple experiments with different learning rates and number of virtual tokens I have noticed that the train loss of the different runs had a very similar pattern. Thus I have realized that the training was not shuffling the data. So maybe I have to revisit previous fine-tuning adding data shuffling. The model trained with shuffled data consistently outperforms the other model. No improvement on leaderboard I have made a few submissions and the results did not improve over fine-tuning. Moreover one of the submissions seemed very brittle and a tiny change of adding one space to the submission resulted in very different predictions. base_model virtual tokens train steps train loss val loss LB score mistral 8 250 0.97 1 0.61 mistral 64 250 1.1 1.15 0.61 mistral 8 1000 0.61 0.78 0.56 mixtral 8 1000 0.96 1 0.6 Conclusion Prompt-tuning is not the solution we are looking for.","title":"Iteration 15. Prompt tuning"},{"location":"modeling/Iteration_15_prompt_tuning/#iteration-15-prompt-tuning","text":"14-04-2024","title":"Iteration 15. Prompt tuning"},{"location":"modeling/Iteration_15_prompt_tuning/#goal","text":"Can I improve the LB score by using prompt tuning?","title":"Goal"},{"location":"modeling/Iteration_15_prompt_tuning/#motivation","text":"So far I have no seen leaderboard differences when using different models. I'm able to fine-tune them and learn the train data without any problem but it is hard to improve the LB score. One interesting thing is that I'm able to overfit on the train data using LoRA and just r=1 . Do I really need fine-tuning for this task? Maybe we can simply learn a good prompt and really on the power of the model to do the task. I believe it is unlikely that with the current scale of data (~1k) the model is going to learn a new task. Thus using the smallest amount possible of parameters could be better.","title":"Motivation"},{"location":"modeling/Iteration_15_prompt_tuning/#development","text":"","title":"Development"},{"location":"modeling/Iteration_15_prompt_tuning/#how-to-do-prompt-fine-tuning","text":"Fine-Tuning Models using Prompt-Tuning with Hugging Face\u2019s PEFT Library . This seems very similar to the LoRA fine-tuning. Prompt tuning for causal language modeling, official Hugging Face Prompt tuning official documentation","title":"How to do prompt fine-tuning"},{"location":"modeling/Iteration_15_prompt_tuning/#experiment-design","text":"The idea is to take Mistral-7B and do prompt fine-tuning as similar as possible as fine-tuning. I will compare the results to the previous iteration centered on data.","title":"Experiment design"},{"location":"modeling/Iteration_15_prompt_tuning/#inference","text":"It seems that load adapter does not work. I have to use the PEFTModel and created a new notebook to make this submissions.","title":"Inference"},{"location":"modeling/Iteration_15_prompt_tuning/#results","text":"","title":"Results"},{"location":"modeling/Iteration_15_prompt_tuning/#shuffle-the-train-dataset","text":"When running multiple experiments with different learning rates and number of virtual tokens I have noticed that the train loss of the different runs had a very similar pattern. Thus I have realized that the training was not shuffling the data. So maybe I have to revisit previous fine-tuning adding data shuffling. The model trained with shuffled data consistently outperforms the other model.","title":"Shuffle the train dataset"},{"location":"modeling/Iteration_15_prompt_tuning/#no-improvement-on-leaderboard","text":"I have made a few submissions and the results did not improve over fine-tuning. Moreover one of the submissions seemed very brittle and a tiny change of adding one space to the submission resulted in very different predictions. base_model virtual tokens train steps train loss val loss LB score mistral 8 250 0.97 1 0.61 mistral 64 250 1.1 1.15 0.61 mistral 8 1000 0.61 0.78 0.56 mixtral 8 1000 0.96 1 0.6","title":"No improvement on leaderboard"},{"location":"modeling/Iteration_15_prompt_tuning/#conclusion","text":"Prompt-tuning is not the solution we are looking for.","title":"Conclusion"},{"location":"modeling/Iteration_16_similar_prompts/","text":"Iteration 16. Similar prompts 15-04-2024 Goal Can I improve the LB score by training on data of similar prompts? Motivation The idea is to have different prompts for the same sample that have the same meaning and high T5 similarity. If I train on this data I believe the task will be better defined and the overfit will be smaller. So the idea is to first use GPT4 to generate similar prompts, then measure similarity with T5 and finally fine-tune Mistral on the new data. Development I have parallelized the calls to GPT4 using 40 workers, that has decreased the generation time from 3.5 hours to just 5 minutes. The total cost of generating prompt variations was 11$. The distribution is pretty good, 85% of the prompt variations score above 0.8 . I have used that threshold to filter prompts with high similarity. This has resulted on a new dataset of 14k samples. The original one was around 1.6k samples. When combining both datasets I have 15,817 samples for training, with a batch size of 16 that would take around 1k steps to loop over all the data. Results I get a LB score of 0.61 , worse than the 0.62 which is the best score for a fine-tuned model. Conclusion No improvement when training on similar prompts.","title":"Iteration 16. Similar prompts"},{"location":"modeling/Iteration_16_similar_prompts/#iteration-16-similar-prompts","text":"15-04-2024","title":"Iteration 16. Similar prompts"},{"location":"modeling/Iteration_16_similar_prompts/#goal","text":"Can I improve the LB score by training on data of similar prompts?","title":"Goal"},{"location":"modeling/Iteration_16_similar_prompts/#motivation","text":"The idea is to have different prompts for the same sample that have the same meaning and high T5 similarity. If I train on this data I believe the task will be better defined and the overfit will be smaller. So the idea is to first use GPT4 to generate similar prompts, then measure similarity with T5 and finally fine-tune Mistral on the new data.","title":"Motivation"},{"location":"modeling/Iteration_16_similar_prompts/#development","text":"I have parallelized the calls to GPT4 using 40 workers, that has decreased the generation time from 3.5 hours to just 5 minutes. The total cost of generating prompt variations was 11$. The distribution is pretty good, 85% of the prompt variations score above 0.8 . I have used that threshold to filter prompts with high similarity. This has resulted on a new dataset of 14k samples. The original one was around 1.6k samples. When combining both datasets I have 15,817 samples for training, with a batch size of 16 that would take around 1k steps to loop over all the data.","title":"Development"},{"location":"modeling/Iteration_16_similar_prompts/#results","text":"I get a LB score of 0.61 , worse than the 0.62 which is the best score for a fine-tuned model.","title":"Results"},{"location":"modeling/Iteration_16_similar_prompts/#conclusion","text":"No improvement when training on similar prompts.","title":"Conclusion"},{"location":"modeling/Iteration_17_scale_the_data/","text":"Iteration 17. Scale the data 15-04-2024 Goal Can I improve the LB score simply by scaling the data? Motivation This is the last idea I have: to take GPT4 and use to to scale the training data. Development I'm going to divide the generation process into two steps: Given prompt samples create new prompts With those prompts create new training samples I have generated 2k new training samples, with a cost of around 20$. Results I get a LB score of 0.60 , worse than the best one which is 0.62 Conclusion No improvement when scaling the data.","title":"Iteration 17. Scale the data"},{"location":"modeling/Iteration_17_scale_the_data/#iteration-17-scale-the-data","text":"15-04-2024","title":"Iteration 17. Scale the data"},{"location":"modeling/Iteration_17_scale_the_data/#goal","text":"Can I improve the LB score simply by scaling the data?","title":"Goal"},{"location":"modeling/Iteration_17_scale_the_data/#motivation","text":"This is the last idea I have: to take GPT4 and use to to scale the training data.","title":"Motivation"},{"location":"modeling/Iteration_17_scale_the_data/#development","text":"I'm going to divide the generation process into two steps: Given prompt samples create new prompts With those prompts create new training samples I have generated 2k new training samples, with a cost of around 20$.","title":"Development"},{"location":"modeling/Iteration_17_scale_the_data/#results","text":"I get a LB score of 0.60 , worse than the best one which is 0.62","title":"Results"},{"location":"modeling/Iteration_17_scale_the_data/#conclusion","text":"No improvement when scaling the data.","title":"Conclusion"},{"location":"modeling/Iteration_18_generate_data_with_gemma/","text":"Iteration 18. Generate data with Gemma 15-04-2024 Goal So far I have generated all my data with GPT4, let's try to rewrite the texts using Gemma and train on that data. Motivation I had the belief that generating data with GPT4 would result on a better model, but maybe I was wrong. Conclusion I have cancelled the iteration because generation was painfully slow and the generated text did not look good. Next steps Focus on preparing the last submissions.","title":"Iteration 18. Generate data with Gemma"},{"location":"modeling/Iteration_18_generate_data_with_gemma/#iteration-18-generate-data-with-gemma","text":"15-04-2024","title":"Iteration 18. Generate data with Gemma"},{"location":"modeling/Iteration_18_generate_data_with_gemma/#goal","text":"So far I have generated all my data with GPT4, let's try to rewrite the texts using Gemma and train on that data.","title":"Goal"},{"location":"modeling/Iteration_18_generate_data_with_gemma/#motivation","text":"I had the belief that generating data with GPT4 would result on a better model, but maybe I was wrong.","title":"Motivation"},{"location":"modeling/Iteration_18_generate_data_with_gemma/#conclusion","text":"I have cancelled the iteration because generation was painfully slow and the generated text did not look good.","title":"Conclusion"},{"location":"modeling/Iteration_18_generate_data_with_gemma/#next-steps","text":"Focus on preparing the last submissions.","title":"Next steps"},{"location":"modeling/Iteration_19_plan_last_submission/","text":"Iteration 19. Plan the last submissions 16-04-2024 Goal Plan the last submissions to be as successful as possible. Motivation Only one day for the end of the challenge, that means I have 5 submissions left. I'm far from the top position but there might be a shakeup. It's 12:00 and I thought the competition was ending tomorrow and I had 10 submissions left. I should pay more attention to the deadline. Development Number of epochs Mistral_v3 -> 450 steps, batch size 16 -> 7200 train samples, max steps 940 https://wandb.ai/guillermobarbadillo/huggingface/runs/ulrk5o38 Mixtral_v5 -> 114 steps, batch size 16 -> 1824 train samples, max steps 1520 https://wandb.ai/guillermobarbadillo/huggingface/runs/b8rxqez9 Mixtral-22b -> 450 steps, batch size 16 -> 7200 train samples, max steps 1000 https://wandb.ai/guillermobarbadillo/other_models/runs/kh0hr94u?nw=nwuserguillermobarbadillo public datasets -> 50 steps, batch size 16 -> 800 train samples, max steps 250 https://wandb.ai/guillermobarbadillo/datacentric_mistral/runs/8sc27bj9?nw=nwuserguillermobarbadillo The data above shows the effective batch size, I have found that when using accumulated gradients each step is every time the gradient is updated. The number of steps is variable, from 50 to 450. I'm going to train for 500 steps. Later I will decide which is the step used for submission. After visualizing the training metrics I have decided to take the checkpoints from step 200. Training data I believe the best possible training data would be: mooney_test_with_gpt4, gemma_suppl_rewrite_curated_with_gpt4, high_quality_dataset_v2-4 but removing duplicated prompts This results in around 1k training samples, so the total train will be around 8 epochs. LoRA configuration I'm going to use r=1 because training and validation loss was almost the same as r=16 and leaderboard score was successful. Models These are my candidate models for the final ensemble: Mistral-7B Llama-13B Mistral-22B Mixtral-8x7B I will train them all and later try to find the best possible combination. Submission planning I can only make 5 submissions: A single model with the maximum predictions possible, mistral_v3 is a candidate for this, f.e. mistral_v3x10 The four models from previous train runs, using sampling. MMML v1 The above with mean prompt The four models trained on this iteration with one prediction each, using sampling. That would take 6 hours. MMML v2 The above with mean prompt The deadline is at 23:59 UTC, which is 2 hours more in my timezone. If I make the submission at 18:00 it would take 8 hours to run, that should be enough. But there might be GPU crunch. Results submission public LB private LB MMML v1 0.64 0.65 MMML v1 + 0.63 prompt 0.66 0.67 Conclusion The challenge has ended, I'm on a provisional 20th position. Next steps Prepare a summary of the solution. TODO How many predictions is the optimum to combine with the mean prompt? I believe infinite is the best if not using mean prompts. I know that 4 is better than 3 for Mixtral_v5 Submit a model with Is it better to make a lot of submissions with Mistral, or to combine different models? Intuition says is to use different models.","title":"Iteration 19. Plan the last submissions"},{"location":"modeling/Iteration_19_plan_last_submission/#iteration-19-plan-the-last-submissions","text":"16-04-2024","title":"Iteration 19. Plan the last submissions"},{"location":"modeling/Iteration_19_plan_last_submission/#goal","text":"Plan the last submissions to be as successful as possible.","title":"Goal"},{"location":"modeling/Iteration_19_plan_last_submission/#motivation","text":"Only one day for the end of the challenge, that means I have 5 submissions left. I'm far from the top position but there might be a shakeup. It's 12:00 and I thought the competition was ending tomorrow and I had 10 submissions left. I should pay more attention to the deadline.","title":"Motivation"},{"location":"modeling/Iteration_19_plan_last_submission/#development","text":"","title":"Development"},{"location":"modeling/Iteration_19_plan_last_submission/#number-of-epochs","text":"Mistral_v3 -> 450 steps, batch size 16 -> 7200 train samples, max steps 940 https://wandb.ai/guillermobarbadillo/huggingface/runs/ulrk5o38 Mixtral_v5 -> 114 steps, batch size 16 -> 1824 train samples, max steps 1520 https://wandb.ai/guillermobarbadillo/huggingface/runs/b8rxqez9 Mixtral-22b -> 450 steps, batch size 16 -> 7200 train samples, max steps 1000 https://wandb.ai/guillermobarbadillo/other_models/runs/kh0hr94u?nw=nwuserguillermobarbadillo public datasets -> 50 steps, batch size 16 -> 800 train samples, max steps 250 https://wandb.ai/guillermobarbadillo/datacentric_mistral/runs/8sc27bj9?nw=nwuserguillermobarbadillo The data above shows the effective batch size, I have found that when using accumulated gradients each step is every time the gradient is updated. The number of steps is variable, from 50 to 450. I'm going to train for 500 steps. Later I will decide which is the step used for submission. After visualizing the training metrics I have decided to take the checkpoints from step 200.","title":"Number of epochs"},{"location":"modeling/Iteration_19_plan_last_submission/#training-data","text":"I believe the best possible training data would be: mooney_test_with_gpt4, gemma_suppl_rewrite_curated_with_gpt4, high_quality_dataset_v2-4 but removing duplicated prompts This results in around 1k training samples, so the total train will be around 8 epochs.","title":"Training data"},{"location":"modeling/Iteration_19_plan_last_submission/#lora-configuration","text":"I'm going to use r=1 because training and validation loss was almost the same as r=16 and leaderboard score was successful.","title":"LoRA configuration"},{"location":"modeling/Iteration_19_plan_last_submission/#models","text":"These are my candidate models for the final ensemble: Mistral-7B Llama-13B Mistral-22B Mixtral-8x7B I will train them all and later try to find the best possible combination.","title":"Models"},{"location":"modeling/Iteration_19_plan_last_submission/#submission-planning","text":"I can only make 5 submissions: A single model with the maximum predictions possible, mistral_v3 is a candidate for this, f.e. mistral_v3x10 The four models from previous train runs, using sampling. MMML v1 The above with mean prompt The four models trained on this iteration with one prediction each, using sampling. That would take 6 hours. MMML v2 The above with mean prompt The deadline is at 23:59 UTC, which is 2 hours more in my timezone. If I make the submission at 18:00 it would take 8 hours to run, that should be enough. But there might be GPU crunch.","title":"Submission planning"},{"location":"modeling/Iteration_19_plan_last_submission/#results","text":"submission public LB private LB MMML v1 0.64 0.65 MMML v1 + 0.63 prompt 0.66 0.67","title":"Results"},{"location":"modeling/Iteration_19_plan_last_submission/#conclusion","text":"The challenge has ended, I'm on a provisional 20th position.","title":"Conclusion"},{"location":"modeling/Iteration_19_plan_last_submission/#next-steps","text":"Prepare a summary of the solution.","title":"Next steps"},{"location":"modeling/Iteration_19_plan_last_submission/#todo","text":"How many predictions is the optimum to combine with the mean prompt? I believe infinite is the best if not using mean prompts. I know that 4 is better than 3 for Mixtral_v5 Submit a model with Is it better to make a lot of submissions with Mistral, or to combine different models? Intuition says is to use different models.","title":"TODO"},{"location":"modeling/Iteration_n/","text":"Iteration n. Iteration_title start date Goal Motivation Development Results Conclusion Next steps TODO [ ]","title":"Iteration n. Iteration_title"},{"location":"modeling/Iteration_n/#iteration-n-iteration_title","text":"start date","title":"Iteration n. Iteration_title"},{"location":"modeling/Iteration_n/#goal","text":"","title":"Goal"},{"location":"modeling/Iteration_n/#motivation","text":"","title":"Motivation"},{"location":"modeling/Iteration_n/#development","text":"","title":"Development"},{"location":"modeling/Iteration_n/#results","text":"","title":"Results"},{"location":"modeling/Iteration_n/#conclusion","text":"","title":"Conclusion"},{"location":"modeling/Iteration_n/#next-steps","text":"","title":"Next steps"},{"location":"modeling/Iteration_n/#todo","text":"[ ]","title":"TODO"},{"location":"utils/00_Challenge_Workflow/","text":"Challenge workflow Start of the challenge Create a repository for the code using cookiecutter Add dates to the calendar Download rules of the challenge Bookmark challenge folder on file explorer Create a Google keep label for tasks and ideas of the challenge Download the challenge data Create a conda environment for the challenge and add it to jupyter conda create -n prometeo pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 -y conda activate prometeo python -m ipykernel install --user --name $CONDA_DEFAULT_ENV --display-name \"Python ($CONDA_DEFAULT_ENV)\" make env-export Create a github repo to have a backup of the data. Vscode allows to do it directly without having to go to the website, choose a private repo. At the end of the challenge it will be made public. Use TDD methodology whenever possible, this will save time because errors won't be propagated along the challenge. Have an apprentice attitude, collaborate on the forum, I have a lot to learn from Kaggle. Add a nice picture to README End of the challenge Prepare a report with a summary of the approach to the challenge Download the Google keep tasks to the repository in pdf format Delete the tasks on google keep and the label Delete unnecessary data Update the environment yml","title":"Challenge workflow"},{"location":"utils/00_Challenge_Workflow/#challenge-workflow","text":"","title":"Challenge workflow"},{"location":"utils/00_Challenge_Workflow/#start-of-the-challenge","text":"Create a repository for the code using cookiecutter Add dates to the calendar Download rules of the challenge Bookmark challenge folder on file explorer Create a Google keep label for tasks and ideas of the challenge Download the challenge data Create a conda environment for the challenge and add it to jupyter conda create -n prometeo pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 -y conda activate prometeo python -m ipykernel install --user --name $CONDA_DEFAULT_ENV --display-name \"Python ($CONDA_DEFAULT_ENV)\" make env-export Create a github repo to have a backup of the data. Vscode allows to do it directly without having to go to the website, choose a private repo. At the end of the challenge it will be made public. Use TDD methodology whenever possible, this will save time because errors won't be propagated along the challenge. Have an apprentice attitude, collaborate on the forum, I have a lot to learn from Kaggle. Add a nice picture to README","title":"Start of the challenge"},{"location":"utils/00_Challenge_Workflow/#end-of-the-challenge","text":"Prepare a report with a summary of the approach to the challenge Download the Google keep tasks to the repository in pdf format Delete the tasks on google keep and the label Delete unnecessary data Update the environment yml","title":"End of the challenge"},{"location":"utils/markdown_cheatsheet/","text":"Markdown cheatsheet Examples of attaching images First an image with markdown syntax Next an image with html syntax that allows to control the size Examples of equations Equation on a different line: \\[\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(log(\\frac{p_i}{a_i}))^2} \\] Examples of inline equations. Let's consider \\(e^{q_i} = p_i + 1\\) and \\(e^{b_i} = a_i + 1\\) . Easy way to create tables http://www.tablesgenerator.com/markdown_tables# representation_size fmeasure val_fmeasure 1024 0.893 0.573 512 0.819 0.476 256 0.676 0.365 128 0.45 0.33 64 0.31 0.29 32 0.26 0.26 16 0.214 0.216","title":"Markdown cheatsheet"},{"location":"utils/markdown_cheatsheet/#markdown-cheatsheet","text":"","title":"Markdown cheatsheet"},{"location":"utils/markdown_cheatsheet/#examples-of-attaching-images","text":"First an image with markdown syntax Next an image with html syntax that allows to control the size","title":"Examples of attaching images"},{"location":"utils/markdown_cheatsheet/#examples-of-equations","text":"Equation on a different line: \\[\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(log(\\frac{p_i}{a_i}))^2} \\] Examples of inline equations. Let's consider \\(e^{q_i} = p_i + 1\\) and \\(e^{b_i} = a_i + 1\\) .","title":"Examples of equations"},{"location":"utils/markdown_cheatsheet/#easy-way-to-create-tables","text":"http://www.tablesgenerator.com/markdown_tables# representation_size fmeasure val_fmeasure 1024 0.893 0.573 512 0.819 0.476 256 0.676 0.365 128 0.45 0.33 64 0.31 0.29 32 0.26 0.26 16 0.214 0.216","title":"Easy way to create tables"},{"location":"utils/methodology/","text":"Methodology I'm following CRISP-DM 1.0 methodology for the reports. I have skipped Evaluation and Deployment steps because they are not usually done on Kaggle. Business understanding Data understanding Data preparation Modeling Solution summary","title":"Methodology"},{"location":"utils/methodology/#methodology","text":"I'm following CRISP-DM 1.0 methodology for the reports. I have skipped Evaluation and Deployment steps because they are not usually done on Kaggle. Business understanding Data understanding Data preparation Modeling Solution summary","title":"Methodology"}]}