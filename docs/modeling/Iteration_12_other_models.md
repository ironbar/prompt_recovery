# Iteration 12. Fine-tuning Phi-2 and Gemma

_11-04-2024_

<!---
The work is done using short iterations. Each iteration needs to have a very
clear goal. This allows to gain greater knowledge of the problem on each iteration.
--->

## Goal

Try fine-tuning and making predictions with [Phi-2](https://www.kaggle.com/models/Microsoft/phi/Transformers/2) and [Gemma-7b-it](https://www.kaggle.com/models/google/gemma/transformers/1.1-7b-it).

## Motivation

My experiments with Mistral, Mixtral and LLama 13b show tiny differences between the models. Maybe
I can get the same results using Phi-2 and Gemma-7b-it. If that is the case it is likely that an
ensemble using all the models would score better than making multiple submissions with the same model.

## Development

The idea is to use the models in transformer format so I can reuse the code for fine-tuning previous models.
I would have to look at the different prompt format of the new models.

### Prompt format

#### Phi-2

TODO:

#### Gemma

TODO:

## Results

TODO: make a submission with each model trained on the same data and create a comparison table of LB score

## Conclusion

## Next steps

## TODO

- [ ]
