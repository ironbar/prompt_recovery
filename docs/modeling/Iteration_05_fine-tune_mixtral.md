# Iteration 5. Mixtral fine-tuning

_26-03-2024_

<!---
The work is done using short iterations. Each iteration needs to have a very
clear goal. This allows to gain greater knowledge of the problem on each iteration.
--->

## Goal

Learn how to fine-tune Mixtral and make predictions with a fine-tuned model.

## Motivation

Learning to fine-tune LLM was one of the motivations of joining the challenge. On this iteration I just
want to verify that I can fine-tune and make inference with fine-tuned Mixtral. On later iterations I
will try with different data, on this iteration I just want to learn to fine-tune.

As train data I will use the supplementary material labelled with GPT4 that was created on the previous iteration.

## Development

## Results

## Conclusion

## Next steps

- Try different combinations of output data and styles

## TODO

- [ ]
