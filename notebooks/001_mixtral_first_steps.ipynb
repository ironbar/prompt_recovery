{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps with Mixtral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that I can use the Mixtral model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)  \n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952ec1db333b4a5090d8d7a1d83c7779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = '/mnt/hdd0/Kaggle/llm_prompt_recovery/models/mixtral-8x7b-instruct-v0.1-hf'\n",
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 24 min when loading from HDD (reading at 62MB/s)\n",
    "- 1 min when loading from SDD (reading at 1.5GB/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id # this is needed to do batch inference\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting the prompt to Mixtral needs.\n",
      "Execution Time : 4.1 s, tokens per second: 6.4\n",
      " In the heart of Spain, where the passion runs deep,\n",
      "Lies a team called Real Madrid, a treasure to keep\n",
      "Formatting the prompt to Mixtral needs.\n",
      "Execution Time : 2.8 s, tokens per second: 9.3\n",
      " In the heart of Spain, where the passion runs deep,\n",
      "Lies a team called Real Madrid, a treasure to keep\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    print(chat_with_mixtral('write a poem about real madrid', max_new_tokens=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting the prompt to Mixtral needs.\n",
      "Execution Time : 21.4 s, tokens per second: 9.4\n",
      " Title: The Future of Digital Identity: A Realm of Endless Possibilities\n",
      "\n",
      "Introduction\n",
      "\n",
      "The digital landscape is evolving at an unprecedented pace, and the concept of digital identity is at the heart of this transformation. Digital identity is the collection of electronically stored data and information related to an individual, organization, or electronic device. As we become more interconnected, the future of digital identity is poised to undergo significant changes, promising endless possibilities. This essay explores the potential future developments in digital identity, focusing on areas such as self-sovereign identity, biometrics, artificial intelligence, and privacy.\n",
      "\n",
      "Self-Sovereign Identity (SSI)\n",
      "\n",
      "One of the most promising developments in digital identity is the emergence of self-sovereign identity (SSI). SSI is a model where individuals have sole ownership and control over their digital identities, enabling them to share and manage their data as they see fit.\n"
     ]
    }
   ],
   "source": [
    "print(chat_with_mixtral('Write an essay about the future of digital identity.', 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is generating at a speed of 10.4 tokens per second, when using `torch.float16`\n",
    "- When using `torch.bfloat16` it generated at 8.9 tokens per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if I'm using the correct input format:\n",
    "\n",
    "- https://www.kaggle.com/models/mistral-ai/mixtral/frameworks/PyTorch/variations/8x7b-instruct-v0.1-hf/versions/1\n",
    "- https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typical case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"say hi\"},\n",
    "]\n",
    "tokenizer.apply_chat_template(messages, return_tensors=\"pt\").numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'<s>[INST] say hi [/INST]'\n",
    "tokenizer.encode(prompt, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(prompt, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'<s>[INST] say hi'\n",
    "tokenizer.encode(prompt, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the encoding is exactly the same. Notice that I had to remove the space between `<s>` and `[INST]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Longer conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello\"},\n",
    "]\n",
    "tokenizer.apply_chat_template(messages, return_tensors=\"pt\").numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'<s>[INST] Hi [/INST]Hello'\n",
    "tokenizer.encode(prompt, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the difference is just that the chat template assumed the bot had ended the chat, but I didn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"user\", \"content\": \"Bye\"},\n",
    "]\n",
    "tokenizer.apply_chat_template(messages, return_tensors=\"pt\").numpy().tolist()[0][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'<s>[INST] Hi [/INST]Hello</s>[INST] Bye[/INST]'\n",
    "tokenizer.encode(prompt, add_special_tokens=False)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(f'<s>[INST] say hi [/INST]', do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(f'[INST] say hi [/INST]', do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows that by default the pipeline was not adding the special token, but if I use `add_special_tokens=True` I can get the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [f'<s>[INST] What is the capital of {country}? Do not give any additional information, just say the capital and shut up.[/INST]The capital of {country} is: ' for country in ['Spain', 'France', 'Germany', 'Italy']]\n",
    "pipe(prompts, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': ' The history of Spain is long, complex, and spans several cultural stages and eras. Here is a brief overview:\\n\\n1. Prehistory: The Iberian Peninsula, where Spain is located, has been inhabited since the'}],\n",
       " [{'generated_text': ' The history of France is extensive and complex, spanning thousands of years, from the prehistoric period to the present day. Here is a brief overview of the major periods and events in French history:\\n\\n1. Prehistory: The area'}],\n",
       " [{'generated_text': ' The history of Germany is complex and rich, spanning many centuries and involving numerous different tribes, states, and cultures. Here is a brief overview:\\n\\nPrehistory and Ancient Times:\\nThe area that is now Germany has been inhabited'}],\n",
       " [{'generated_text': ' The history of Italy is long and complex, spanning thousands of years and numerous civilizations. Here is a brief overview:\\n\\nPrehistory and Ancient Italy:\\nThe Italian Peninsula has been inhabited since the Paleolithic'}]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [f'<s>[INST] What is the history of {country}? [/INST]' for country in ['Spain', 'France', 'Germany', 'Italy']]\n",
    "pipe(prompts, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not seem to speedup the inference in any way using a pipe with multiple inputs. It works but not faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the tokenizer adds the BOS token. So it is likely that in the pipeline it is done as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bs4 = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': ' The history of Spain is long, complex, and spans several cultural stages and eras. Here is a brief overview:\\n\\n1. Prehistory: The Iberian Peninsula, where Spain is located, has been inhabited since the'}],\n",
       " [{'generated_text': ' The history of France is extensive and complex, spanning thousands of years, from the prehistoric period to the present day. Here is a brief overview of the major periods and events in French history:\\n\\n1. Prehistory: The area'}],\n",
       " [{'generated_text': ' The history of Germany is complex and rich, spanning many centuries and involving numerous different tribes, states, and cultures. Here is a brief overview:\\n\\nPrehistory and Ancient Times:\\nThe area that is now Germany has been inhabited'}],\n",
       " [{'generated_text': ' The history of Italy is long and complex, spanning thousands of years and numerous civilizations. Here is a brief overview:\\n\\nPrehistory and Ancient Italy:\\nThe Italian Peninsula has been inhabited since the Paleolithic'}]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [f'<s>[INST] What is the history of {country}? [/INST]' for country in ['Spain', 'France', 'Germany', 'Italy']]\n",
    "pipe_bs4(prompts, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': ' The history of Spain is long, complex, and spans several cultural stages and eras. Here is a brief overview:\\n\\n1. Prehistory: The Iberian Peninsula, where Spain is located, has been inhabited since the'}],\n",
       " [{'generated_text': ' The history of France is extensive and complex, spanning thousands of years, from the prehistoric period to the present day. Here is a brief overview of the major periods and events in French history:\\n\\n1. Prehistory: The area'}],\n",
       " [{'generated_text': ' The history of Germany is complex and rich, spanning many centuries and involving numerous different tribes, states, and cultures. Here is a brief overview:\\n\\nPrehistory and Ancient Times:\\nThe area that is now Germany has been inhabited'}],\n",
       " [{'generated_text': \" Italy, as a country, has a long and complex history that dates back to ancient times. Here is an overview of the major periods in Italy's history:\\n\\n1. Prehistory: The Italian peninsula has been inhabited since the\"}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [f'<s>[INST] What is the history of {country}? [/INST]' for country in ['Spain', 'France', 'Germany', 'Italy, one of the most important countries on Europe']]\n",
    "pipe_bs4(prompts, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU usage is higher, and generation is faster. Let's try to increase the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [f'<s>[INST] What is the history of {country}? [/INST]' for country in ['Spain']]\n",
    "max_new_tokens = 25\n",
    "for batch_size in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]: # OOM, 2048, 4096]:\n",
    "    new_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, batch_size=batch_size)\n",
    "    t0 = time.time()\n",
    "    output = new_pipe(prompts*batch_size, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=max_new_tokens)\n",
    "    t0 = time.time() - t0\n",
    "    print(f'Batch size: {batch_size}\\tExecution time: {t0:.1f} s, tokens per second: {max_new_tokens*batch_size/t0:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Batch size: 1\tExecution time: 3.1 s, tokens per second: 8.1\n",
    "Batch size: 2\tExecution time: 4.6 s, tokens per second: 10.8\n",
    "Batch size: 4\tExecution time: 4.6 s, tokens per second: 21.9\n",
    "Batch size: 8\tExecution time: 4.6 s, tokens per second: 43.3\n",
    "Batch size: 16\tExecution time: 4.7 s, tokens per second: 85.8\n",
    "Batch size: 32\tExecution time: 4.9 s, tokens per second: 164.0\n",
    "Batch size: 64\tExecution time: 5.1 s, tokens per second: 315.1\n",
    "Batch size: 128\tExecution time: 6.3 s, tokens per second: 511.9\n",
    "Batch size: 256\tExecution time: 8.4 s, tokens per second: 760.3\n",
    "Batch size: 512\tExecution time: 13.3 s, tokens per second: 963.9\n",
    "Batch size: 1024\tExecution time: 23.7 s, tokens per second: 1080.6\n",
    "```\n",
    "\n",
    "These are incredible speedups. If I batch the predictions I could do inference 100 times faster.\n",
    "When using a big batch size I see the GPUs alternating at 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/bible.txt', 'r') as f:\n",
    "    bible = f.read()\n",
    "print(bible[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    memory = torch.cuda.memory_allocated(0) + torch.cuda.memory_allocated(1)\n",
    "    memory = sum(torch.cuda.memory_allocated(device) for device in range(torch.cuda.device_count()))\n",
    "    max_memory = sum(torch.cuda.max_memory_allocated(device) for device in range(torch.cuda.device_count()))\n",
    "    print(f'GPU memory allocated: {memory/1024**3:.1f} GB, max memory allocated: {max_memory/1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The allocated memory remains constant, I have to log the max memory allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens, max_memory, inference_time = [], [], []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    text = bible[:int(i*4000)]\n",
    "    input_tokens.append(len(tokenizer.tokenize(text)))\n",
    "    print(f'Input tokens: {input_tokens[-1]}')\n",
    "    t0 = time.time()\n",
    "    chat_with_mixtral(text, max_new_tokens=25)\n",
    "    inference_time.append(time.time() - t0)\n",
    "    print_gpu_memory()\n",
    "    max_memory.append(sum(torch.cuda.max_memory_allocated(device) for device in range(torch.cuda.device_count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.plot(input_tokens, np.array(max_memory)/1e9, marker='o')\n",
    "plt.xlabel('Input tokens')\n",
    "plt.ylabel('Max GPU memory allocated (GB)')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(input_tokens, np.array(inference_time), marker='o')\n",
    "plt.xlabel('Input tokens')\n",
    "plt.ylabel('Inference time (s)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the memory and the inference time grow linearly with the input tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    memory = torch.cuda.memory_allocated(0) + torch.cuda.memory_allocated(1)\n",
    "    memory = sum(torch.cuda.memory_allocated(device) for device in range(torch.cuda.device_count()))\n",
    "    max_memory = sum(torch.cuda.max_memory_allocated(device) for device in range(torch.cuda.device_count()))\n",
    "    print(f'GPU memory allocated: {memory/1024**3:.1f} GB, max memory allocated: {max_memory/1024**3:.1f} GB')\n",
    "\n",
    "output_tokens, max_memory, inference_time = [], [], []\n",
    "\n",
    "text = bible[:int(4000)]\n",
    "\n",
    "for i in range(1, 10):\n",
    "    t0 = time.time()\n",
    "    response = chat_with_mixtral(text, max_new_tokens=int(25*i))\n",
    "    output_tokens.append(len(tokenizer.tokenize(response)))\n",
    "    print(f'Output tokens: {output_tokens[-1]}')\n",
    "    inference_time.append(time.time() - t0)\n",
    "    print_gpu_memory()\n",
    "    max_memory.append(sum(torch.cuda.max_memory_allocated(device) for device in range(torch.cuda.device_count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.plot(output_tokens, np.array(max_memory)/1e9, marker='o')\n",
    "plt.xlabel('Output tokens')\n",
    "plt.ylabel('Max GPU memory allocated (GB)')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(output_tokens, np.array(inference_time), marker='o')\n",
    "plt.xlabel('Output tokens')\n",
    "plt.ylabel('Inference time (s)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scale of the tokens that we are going generate, output memory is constant and inference time scales linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    memory = torch.cuda.memory_allocated(0) + torch.cuda.memory_allocated(1)\n",
    "    memory = sum(torch.cuda.memory_allocated(device) for device in range(torch.cuda.device_count()))\n",
    "    max_memory = sum(torch.cuda.max_memory_allocated(device) for device in range(torch.cuda.device_count()))\n",
    "    print(f'GPU memory allocated: {memory/1024**3:.1f} GB, max memory allocated: {max_memory/1024**3:.1f} GB')\n",
    "\n",
    "\n",
    "text = bible[:int(16000)]\n",
    "print(f'Input tokens: {len(tokenizer.tokenize(text))}')\n",
    "prompts = [f'<s>[INST] {text} [/INST]']\n",
    "max_new_tokens = 25\n",
    "\n",
    "batch_size_range = [1, 2, 4]\n",
    "max_memory, inference_time = [], []\n",
    "\n",
    "for batch_size in batch_size_range:\n",
    "    new_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, batch_size=batch_size)\n",
    "    t0 = time.time()\n",
    "    output = new_pipe(prompts*batch_size, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=max_new_tokens)\n",
    "    inference_time.append((time.time() - t0)/batch_size)\n",
    "    print_gpu_memory()\n",
    "    max_memory.append(sum(torch.cuda.max_memory_allocated(device) for device in range(torch.cuda.device_count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    dict(input_tokens=1129,\n",
    "         batch_size_range=[1, 2, 4, 8, 16],\n",
    "         max_memory=[25650272256, 26341480448, 27512605696, 29923305472, 34771817472],\n",
    "         inference_time=[3.436551809310913, 2.789551019668579, 1.6524068713188171, 1.117274284362793, 0.8535761386156082]),\n",
    "    dict(input_tokens=2320,\n",
    "         batch_size_range=[1, 2, 4, 8],\n",
    "         max_memory=[26281251840, 27565897216, 30037116416, 35002349056],\n",
    "         inference_time=[3.882540464401245, 3.33407199382782, 2.2599929571151733, 1.7502523958683014]),\n",
    "\n",
    "    dict(input_tokens=4830,\n",
    "         batch_size_range=[1, 2, 4],\n",
    "         max_memory=[27620456960, 30234573312, 35392415744],\n",
    "         inference_time=[5.286550998687744, 4.656355619430542, 3.7115885615348816]),\n",
    "]\n",
    "\n",
    "plt.subplot(121)\n",
    "for experiment in data:\n",
    "    plt.plot(experiment['batch_size_range'], np.array(experiment['max_memory'])/1e9, marker='o', label=f\"{experiment['input_tokens']} input tokens\")\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Max GPU memory allocated (GB)')\n",
    "xlim, ylim = plt.xlim(), plt.ylim()\n",
    "plt.fill_between(xlim, [31]*2, [ylim[1]]*2, color='red', alpha=0.1)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.grid()\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.subplot(122)\n",
    "for experiment in data:\n",
    "    plt.plot(experiment['batch_size_range'], experiment['inference_time'], marker='o', label=f\"{experiment['input_tokens']} input tokens\")\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Inference time (s)')\n",
    "plt.grid()\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory grows linearly with the batch size, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this [blogpost](https://huggingface.co/blog/mixtral) the context length is 32k tokens. Let's verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_characters = 20000\n",
    "prompt = f'<s>[INST] {bible[:n_characters]} [/INST]'\n",
    "print(f'Input tokens: {len(tokenizer.tokenize(prompt))}')\n",
    "print(chat_with_mixtral(prompt, max_new_tokens=25))\n",
    "prompt = f'<s>[INST] Summarize the following text: {bible[:n_characters]} [/INST]'\n",
    "print(chat_with_mixtral(prompt, max_new_tokens=25))\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_characters = 40000\n",
    "prompt = f'<s>[INST] {bible[:n_characters]} [/INST]'\n",
    "print(f'Input tokens: {len(tokenizer.tokenize(prompt))}')\n",
    "print(chat_with_mixtral(prompt, max_new_tokens=25))\n",
    "prompt = f'<s>[INST] Summarize the following text: {bible[:n_characters]} [/INST]'\n",
    "print(chat_with_mixtral(prompt, max_new_tokens=25))\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_characters = 80000\n",
    "prompt = f'<s>[INST] {bible[:n_characters]} [/INST]'\n",
    "print(f'Input tokens: {len(tokenizer.tokenize(prompt))}')\n",
    "print(chat_with_mixtral(prompt, max_new_tokens=25))\n",
    "prompt = f'<s>[INST] Summarize the following text: {bible[:n_characters]} [/INST]'\n",
    "print(chat_with_mixtral(prompt, max_new_tokens=25))\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_characters = 105000\n",
    "prompt = f'<s>[INST] {bible[:n_characters]} [/INST]'\n",
    "print(f'Input tokens: {len(tokenizer.tokenize(prompt))}')\n",
    "print(chat_with_mixtral(prompt, max_new_tokens=25))\n",
    "prompt = f'<s>[INST] Summarize the following text: {bible[:n_characters]} [/INST]'\n",
    "print(chat_with_mixtral(prompt, max_new_tokens=25))\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Input tokens: 32999\n",
    "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
    "Execution Time : 25.0 s, tokens per second: 1.0\n",
    "This is the book of the generations of Adam. In the day that God created man, in the likeness of God\n",
    "Execution Time : 24.8 s, tokens per second: 1.0\n",
    "The passage is the first book of the Bible, Genesis, and tells the story of the creation of the world and the\n",
    "GPU memory allocated: 23.3 GB, max memory allocated: 39.7 GB\n",
    "```\n",
    "\n",
    "We see a warning about surpassing the limit of 32768 tokens. We don't have to worry about this because on the submission we won't have that much VRAM memory available.\n",
    "\n",
    "We will have to deal with around 12k input token limits due to RAM and time constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch manages memory in a weird way, depending on the previous executions I would get OOM or not.\n",
    "F.e. I could not call the chat 2 times with the biggest input.\n",
    "\n",
    "¿Maybe I could if calling the pipe with a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] What is the message: `Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.`\n",
    "- [x] Try using `tokenizer.apply_chat_template()`, what is the correct input format?\n",
    "- [x] Can I use batches to speeedup generation? GPU use is around 13% when generating data\n",
    "- [ ] Maybe on another notebook: setup a pipeline to evaluate different prompts. This is the way of doing prompt engineering. Try some prompt, evaluate, iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
