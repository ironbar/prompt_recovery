{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps with Mixtral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that I can use the Mixtral model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/mnt/hdd0/Kaggle/llm_prompt_recovery/models/mixtral-8x7b-instruct-v0.1-hf'\n",
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 24 min when loading from HDD (reading at 62MB/s)\n",
    "- 1 min when loading from SDD (reading at 1.5GB/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s> [INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s> [INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    response = re.sub(r'[\\'\"]', '', response.replace(prompt, ''))\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    print(chat_with_mixtral('write a poem about real madrid', max_new_tokens=25, temperature=1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_with_mixtral('Write an essay about the future of digital identity.', 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is generating at a speed of 10.4 tokens per second, when using `torch.float16`\n",
    "- When using `torch.bfloat16` it generated at 8.9 tokens per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [ ] What is the message: `Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.`\n",
    "- [ ] Try using `tokenizer.apply_chat_template()`, what is the correct input format?\n",
    "- [ ] Can I use batches to speeedup generation? GPU use is around 13% when generating data\n",
    "- [ ] Maybe on another notebook: setup a pipeline to evaluate different prompts. This is the way of doing prompt engineering. Try some prompt, evaluate, iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
