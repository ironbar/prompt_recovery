{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which results do we get if we fine-tune Mistral?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    pipeline, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True,\n",
    "    llm_int8_skip_modules=['gate', 'lm_head'],\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/gbarbadillo/data/Mistral-7B-Instruct-v0.2/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "tokenizer.padding_side = 'right' # by default is left, for training right seems to be better\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    #response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I don't quantize the model the memory usage is good for my PC, but too much for the submission.\n",
    "\n",
    "```\n",
    "GPU 0 memory allocated: 14.5 GB, max memory allocated: 14.5 GB\n",
    "GPU 1 memory allocated: 14.5 GB, max memory allocated: 14.5 GB\n",
    "```\n",
    "\n",
    "With quantization memory usage is:\n",
    "\n",
    "```\n",
    "GPU 0 memory allocated: 3.3 GB, max memory allocated: 3.3 GB\n",
    "GPU 1 memory allocated: 4.1 GB, max memory allocated: 4.1 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST][prompt-recovery]\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, Update etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST] The most likely text prompt given to transform the original text into the rewritten text was: {response} </s>\"\"\"\n",
    "response_template = \"The most likely text prompt given to transform the original text into the rewritten text was:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_training(filepath, target_col='gpt4_normalized_response'):\n",
    "    df = pd.read_csv(filepath)\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        texts.append(prompt_template.format(original_text=row['original_text'],\n",
    "                                            rewritten_text=row['rewritten_text'],\n",
    "                                            response=row[target_col]))\n",
    "    df['text'] = texts\n",
    "    df['n_tokens'] = df['text'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = prepare_dataframe_for_training('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v1.csv',\n",
    "                                          target_col='rewrite_prompt')\n",
    "train_df_2 = prepare_dataframe_for_training('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv',\n",
    "                                            target_col='gpt4_prompt')\n",
    "bad_indices = [164, 181, 235]\n",
    "print(train_df_2.loc[bad_indices].rewritten_text.values)\n",
    "train_df_2.drop(bad_indices, inplace=True)\n",
    "train_df_3 = prepare_dataframe_for_training('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv',\n",
    "                                         target_col='gpt4_prompt')\n",
    "train_df = pd.concat([train_df_1, train_df_2, train_df_3], ignore_index=True).reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_indices = train_df.sample(frac=0.1, random_state=42).index\n",
    "eval_df = train_df.loc[eval_df_indices].copy()\n",
    "train_df.drop(eval_df_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_df['n_tokens'], bins=50, alpha=0.5, label='train', cumulative=True, density=True)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.title('Token distribution of the texts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There were {len(train_df)} samples for training and {len(eval_df)} samples for evaluation.')\n",
    "max_seq_length = 640\n",
    "train_df = train_df[train_df['n_tokens'] <= max_seq_length]\n",
    "eval_df = eval_df[eval_df['n_tokens'] <= max_seq_length]\n",
    "print(f'There are {len(train_df)} samples for training and {len(eval_df)} samples for evaluation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'One epoch is {len(train_df)//16} steps.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in train_df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text.split(response_template)[0] + response_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# without quantization\n",
    "Execution Time : 1.5 s, tokens per second: 11.1\n",
    " \"Rewrite the speech using the masculine pronoun for the speaker.\"\n",
    "Execution Time : 1.5 s, tokens per second: 17.7\n",
    " \"Rewrite the text about the most loyal friend a person can have, but this time focus on horses instead of dogs.\"\n",
    "Execution Time : 1.1 s, tokens per second: 15.1\n",
    " \"Rewrite the text in Mandarin Chinese for a Chinese audience.\"\n",
    "Execution Time : 1.1 s, tokens per second: 15.9\n",
    " \"Rewrite this text in Spanish for a Spanish-speaking audience.\"\n",
    "Execution Time : 0.9 s, tokens per second: 15.8\n",
    " \"Rewrite this announcement in Portuguese for an international audience.\"\n",
    "\n",
    "# with quantization\n",
    "Execution Time : 1.8 s, tokens per second: 14.2\n",
    " \"Rewrite the text using 'he' or 'his' instead of'she' or 'her' throughout.\"\n",
    "Execution Time : 1.0 s, tokens per second: 19.7\n",
    " \"Rewrite the text about a loyal friend using a different animal as an example.\"\n",
    "Execution Time : 1.2 s, tokens per second: 19.9\n",
    " \"Rewrite the text in simplified Chinese for a Chinese audience while maintaining the original meaning and style.\"\n",
    "Execution Time : 0.6 s, tokens per second: 17.5\n",
    " \"Rewrite this text in Spanish.\"\n",
    "Execution Time : 0.7 s, tokens per second: 19.1\n",
    " \"Rewrite this announcement in Portuguese for an international audience.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in eval_df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text.split(response_template)[0] + response_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    # lora_alpha: LoRA scaling factor.\n",
    "    lora_alpha=64, #64,\n",
    "    lora_dropout=0.1, # 0.1, althought Vaca suggested to use 0.05 for big models\n",
    "    # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    r=16, #16\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_steps = len(train_df)//16\n",
    "print(f'Logging steps: {logging_steps}')\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-04-08_new_trainings/05_mistral\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        per_device_train_batch_size=8, # 4-16 should be fine for lora.\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=8,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=logging_steps, #50,\n",
    "        logging_steps=logging_steps, #50,\n",
    "        learning_rate=2e-5, # maybe we can increase this\n",
    "        eval_steps=logging_steps, #50,\n",
    "        max_steps=logging_steps*20, #300,\n",
    "        warmup_steps=30,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, response_template=response_template)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=data_collator,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a few inferences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in train_df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text.split(response_template)[0] + response_template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in eval_df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text.split(response_template)[0] + response_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [ ] rslora? https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft\n",
    "- [ ] Reduce alpha, or reduce r as well\n",
    "- [ ] Try with Vaca's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
