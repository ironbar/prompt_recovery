{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset with GPT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPT4 API to create a diverse and high quality dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from prometeo.evaluation import get_sharpened_cosine_similarity, estimate_mean\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_types = \"\"\"\n",
    "text\n",
    "email\n",
    "letter\n",
    "memo\n",
    "dialogue\n",
    "news\n",
    "poem\n",
    "essay\n",
    "report\n",
    "academic paper\n",
    "story\n",
    "play\n",
    "blog\n",
    "advertisement\n",
    "brochure\n",
    "social media post\n",
    "manual\n",
    "guide\n",
    "legal document\n",
    "diary\n",
    "speech\n",
    "lecture\n",
    "review\n",
    "\"\"\"\n",
    "text_types = text_types.strip().split('\\n')\n",
    "print(text_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_types = ['haiku', 'sonnet', 'limerick']\n",
    "languages = ['chinese', 'spanish', 'hindi', 'portuguese', 'russian', 'french', 'german']\n",
    "programming_languages = ['javascript', 'python', 'java', 'C++',] # 'Ruby', 'PHP'] removed because the created samples were wrong\n",
    "codes = ['morse code', 'base64']\n",
    "encryptions = [f'Caesar cipher shift {shift}' for shift in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writing_styles = \"\"\"\n",
    "academic\n",
    "action-packed\n",
    "advertising copy\n",
    "blog writing\n",
    "business\n",
    "descriptive\n",
    "fantasy writing\n",
    "fiction\n",
    "non-fiction\n",
    "formal\n",
    "informal\n",
    "horror writing\n",
    "humorous\n",
    "imaginative\n",
    "journalistic\n",
    "legal writing\n",
    "medical writing\n",
    "mystery writing\n",
    "narrative style\n",
    "scientific report\n",
    "white paper\n",
    "\"\"\"\n",
    "writing_styles = writing_styles.strip().split('\\n')\n",
    "print(writing_styles)\n",
    "\n",
    "authors = \"\"\"\n",
    "Edgar Allan Poe\n",
    "Emily Dickinson\n",
    "Ernest Hemingway\n",
    "F. Scott Fitzgerald\n",
    "Jane Austen\n",
    "J.D. Salinger\n",
    "Langston Hughes\n",
    "Mark Twain\n",
    "Maya Angelou\n",
    "Toni Morrison\n",
    "Dr. Seuss\n",
    "Tupac Shakur\n",
    "William Shakespeare\n",
    "\"\"\"\n",
    "authors = authors.strip().split('\\n')\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tones = ['formal', 'informal', 'humorous', 'persuasive', 'objective', 'subjective',\n",
    "         'optimistic', 'pessimistic', 'sarcastic', 'sincere', 'inspirational', 'critical',\n",
    "         'descriptive', 'narrative', 'didactic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = ['first', 'third']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_formats = ['Markdown', 'HTML', 'RST (Restructured Text)']\n",
    "data_formats = ['CSV', 'JSON', 'YAML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = ['country', 'rock', 'pop', 'rap', 'reggae', 'metal', 'folk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'], organization=os.environ['OPENAI_API_ORG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt4(prompt, temperature=0.7):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Given the following text prompt your task is to:\n",
    "\n",
    "1. Write a short text that could have sense to be modified with the given text prompt. The number of words should be less than 200.\n",
    "2. Rewrite the text using the given text prompt.\n",
    "\n",
    "The output should be in json, with the following format:\n",
    "\n",
    "{\"original_text\": \"...\", \"rewritten_text\": \"...\"}\n",
    "\n",
    "## Text prompt\n",
    "\n",
    "```{prompt}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prompt(response):\n",
    "    if '{' not in response:\n",
    "        print(f'Response does not contain json: {response}')\n",
    "        return response\n",
    "\n",
    "    json_text = '{' + response.split('{')[1].split('}')[0] + '}'\n",
    "    data = eval(json_text)\n",
    "    if isinstance(data, dict):\n",
    "        return list(data.values())[0]\n",
    "    elif isinstance(data, set):\n",
    "        return data.pop()\n",
    "    raise ValueError(f'Could not parse prompt from {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_response(response, prompt):\n",
    "    normalized_response = response.split('{')[0]\n",
    "    normalized_response += '{\"prompt\": \"' + prompt + '\"}'\n",
    "    normalized_response = normalized_response.replace('\\n\\n', '\\n')\n",
    "    return normalized_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_variables = {\n",
    "    'text': text_types,\n",
    "    'poem': poem_types,\n",
    "    'language': languages,\n",
    "    'programming_language': programming_languages,\n",
    "    'code': codes,\n",
    "    'encryption': encryptions,\n",
    "    'writing_style': writing_styles,\n",
    "    'author': authors,\n",
    "    'tone': tones,\n",
    "    'person': persons,\n",
    "    'text_format': text_formats,\n",
    "    'data_format': data_formats,\n",
    "    'song': songs,\n",
    "}\n",
    "\n",
    "def format_prompts(prompts):\n",
    "    formatted_prompts = []\n",
    "    for prompt in prompts:\n",
    "        keys = re.findall(r'\\{.*?\\}', prompt)\n",
    "        if not keys:\n",
    "            formatted_prompts.append(prompt)\n",
    "            continue\n",
    "        prioritary_keys = [key for key in keys if key != '{text}']\n",
    "        if prioritary_keys:\n",
    "            formatted_prompts.extend(format_prompts(fill_variable_in_prompt(prompt, prioritary_keys[0])))\n",
    "        else:\n",
    "            formatted_prompts.extend(format_prompts(fill_variable_in_prompt(prompt, keys[0])))\n",
    "    return formatted_prompts\n",
    "\n",
    "def fill_variable_in_prompt(prompt, key):\n",
    "    naked_key = key[1:-1]\n",
    "    values = key_to_variables[naked_key]\n",
    "    if naked_key  == 'text':\n",
    "        while True:\n",
    "            sampled_variable = random.choice(values)\n",
    "            if sampled_variable not in prompt:\n",
    "                break\n",
    "        return [prompt.replace(key, sampled_variable, 1)]\n",
    "    else:\n",
    "        values = key_to_variables[naked_key]\n",
    "        return [prompt.replace(key, value) for value in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(random_seed):\n",
    "    df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/prompts/prompts_v1.csv')\n",
    "    random.seed(random_seed)\n",
    "    prompts = prepare_prompts(df.prompt)\n",
    "    rows = generate_samples(prompts)\n",
    "    dataset = pd.DataFrame(rows)\n",
    "    dataset.to_csv(f'/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v{random_seed}.csv', index=False)\n",
    "    dataset.head()\n",
    "\n",
    "def prepare_prompts(raw_prompts):\n",
    "    prompts = []\n",
    "    for raw_prompt in raw_prompts:\n",
    "        prompts.extend(format_prompts([raw_prompt]))\n",
    "    return prompts\n",
    "\n",
    "def generate_samples(prompts):\n",
    "    rows = []\n",
    "    for prompt in tqdm(prompts, desc='Creating data'):\n",
    "        try:\n",
    "            ret = chat_with_gpt4(prompt_template.replace('{prompt}', prompt))\n",
    "            output = json.loads(ret)\n",
    "            rows.append(dict(original_text=output['original_text'], rewritten_text=output['rewritten_text'], rewrite_prompt=prompt))\n",
    "        except Exception as e:\n",
    "            print(f'Error with prompt: {prompt}')\n",
    "            print(e)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset with high quality prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High quality prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a first step we have to take the prompts and format them because many of the prompts have placeholder for different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for random_seed in [3, 4]:\n",
    "    create_dataset(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each generation of around 275 prompts costs around 2$.\n",
    "\n",
    "- v1 did not use any random seed\n",
    "- v2, v3, v4 use the seed in their number (2, 3, 4...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset with Newtonbaba prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newtonbaba gemma produced the best results when using few-shot prompts, let's see how the prompts look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/newtonbaba/gemma_data_set_prompt_recover_1_curated.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_prompts = df.rewrite_prompt.unique()\n",
    "print(len(unique_prompts))\n",
    "unique_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't like the prompts. I prefer to generate more data using my own prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset with prompts that imitate the leaked ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/prompts/imitation_of_leaked_v1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = generate_samples(df.prompt)\n",
    "dataset = pd.DataFrame(rows)\n",
    "dataset.to_csv(f'/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_imitation_of_leaked_v1.csv', index=False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63 samples, would they be helpful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-instruction prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/prompts/multi-instruction-prompts-v1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = generate_samples(df.prompt)\n",
    "dataset = pd.DataFrame(rows)\n",
    "dataset.to_csv(f'/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_multi_instruction_v1.csv', index=False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/prompts/prompts_v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = generate_samples(df.prompt)\n",
    "dataset = pd.DataFrame(rows)\n",
    "dataset.to_csv(f'/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v5_gpt4.csv', index=False)\n",
    "dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
