{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First steps with prompt engineering:\n",
    "\n",
    "- Choose a validation dataset\n",
    "- Implement a validation pipeline\n",
    "- Try different prompt strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text  # Registers the ops.\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/mnt/hdd0/Kaggle/llm_prompt_recovery/models/mixtral-8x7b-instruct-v0.1-hf'\n",
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    #device_map=\"auto\",\n",
    "    device_map=create_intertwined_device_map(),\n",
    "    #device_map=create_shared_device_map(16),\n",
    "    trust_remote_code=True,\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id # this is needed to do batch inference\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash attention speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/en/model_doc/mixtral#speeding-up-mixtral-by-using-flash-attention\n",
    "\n",
    "```bash\n",
    "export FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE  \n",
    "pip install -U flash-attn --no-build-isolation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    print(chat_with_mixtral('write a poem about real madrid', max_new_tokens=50))\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/bible.txt', 'r') as f:\n",
    "    bible = f.read()\n",
    "print(chat_with_mixtral(bible[:24000], max_new_tokens=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# flash-attention\n",
    "Execution Time : 5.4 s, tokens per second: 9.4\n",
    "Execution Time : 9.1 s, tokens per second: 5.6\n",
    "\n",
    "# baseline\n",
    "Execution Time : 5.1 s, tokens per second: 10.0\n",
    "Execution Time : 9.2 s, tokens per second: 5.5\n",
    "```\n",
    "\n",
    "Not clear difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that batch prediction works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [f'<s>[INST] What is the history of {country}? [/INST]' for country in ['Spain', 'France', 'Germany', 'Italy']]\n",
    "pipe(prompts, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chat_with_mixtral(prompt, max_new_tokens=50) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "pipe_bs = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, batch_size=batch_size)\n",
    "pipe_bs(prompts, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bs(prompts[:3] + [bible[:24000]], do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the duration when batching inputs is determined by the longest input and output. Thus it might have sense to order the inputs by length.\n",
    "\n",
    "All the predictions are the same, despite the method used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study public datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/481811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I study the datasets? I have to read them\n",
    "\n",
    "I could read the prompts, this needs time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v1.csv').head(64)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROMPT = \"\"\"<s>[INST]\n",
    "# Guess the prompt\n",
    "\n",
    "Your task is to guess which prompt was given to transform the original text into the rewritten text.\n",
    "Read the two texts carefully and answer with the most likely prompt that was used to modify the original text.\n",
    "\n",
    "- The response should be as short and concise as possible. Whenever possible use a single short sentence prompt.\n",
    "- The prompt should be as generic as possible and content agnostic unless it is absolutely necessary to make reference to the content of the original text\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "## Prompt\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def truncate_txt(text, max_words=2000):\n",
    "    text_list = text.split()\n",
    "    return \" \".join(text_list[:max_words])\n",
    "\n",
    "prompts = []\n",
    "for _, row in test.iterrows():\n",
    "    prompts.append(MODEL_PROMPT.format(original_text=truncate_txt(row['original_text']),\n",
    "                                       rewritten_text=truncate_txt(row['rewritten_text'])))\n",
    "\n",
    "plt.hist([len(tokenizer.tokenize(prompt)) for prompt in prompts], bins=20);\n",
    "plt.title('Distribution of prompt token length')\n",
    "plt.xlabel('Prompt token length');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [chat_with_mixtral(prompt, max_new_tokens=25, verbose=False) for prompt in tqdm(prompts, smoothing=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the predictions one by one takes 3m2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "responses_bs = pipe(prompts, do_sample=False, return_full_text=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id, max_new_tokens=25,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lengths = [len(tokenizer.tokenize(prompt)) for prompt in prompts]\n",
    "sorted_prompts = [prompt for _, prompt in sorted(zip(prompt_lengths, prompts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "responses_bs = pipe(sorted_prompts, do_sample=False, return_full_text=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id, max_new_tokens=25,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class PromptsDataset(Dataset):\n",
    "    def __init__(self, prompts):\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "responses_bs = pipe(PromptsDataset(sorted_prompts), do_sample=False, return_full_text=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id, max_new_tokens=25,\n",
    "                    batch_size=batch_size)\n",
    "responses_bs = list(responses_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "responses_bs = pipe(sorted_prompts, do_sample=False, return_full_text=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id, max_new_tokens=25,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MODEL_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I cannot use a batch size higher than 8, which results in a very modest speedup of 2min vs 3min. And that is considering the current input tokens which may vary.\n",
    "- The batch affects to some of the responses, it might be due to numerical errors\n",
    "- Using a torch dataset does not speedup the inference\n",
    "\n",
    "Thus I believe the best option is to use a simple sequential inference. To be fast I should use a small evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some responses change when using batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "batch_size = 1\n",
    "print(pipe(prompts[idx: idx + batch_size], do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=25, batch_size=batch_size)[0][0]['generated_text'])\n",
    "\n",
    "batch_size = 4\n",
    "print(pipe(prompts[idx: idx + batch_size], do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=25, batch_size=batch_size)[0][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    padding='do_not_pad')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id # this is needed to do batch inference\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gc.collect()\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "pipe(prompts[:batch_size], do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=25, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt_template(prompt_template, max_new_tokens, test_filepath, n_samples, random_seed=7):\n",
    "    test = load_test_data(test_filepath, n_samples, random_seed)\n",
    "\n",
    "    prompts = []\n",
    "    for _, row in test.iterrows():\n",
    "        prompts.append(prompt_template.format(original_text=truncate_txt(row['original_text']),\n",
    "                                              rewritten_text=truncate_txt(row['rewritten_text'])))\n",
    "\n",
    "    plt.hist([len(tokenizer.tokenize(prompt)) for prompt in prompts], bins=20);\n",
    "    plt.title('Distribution of prompt token length')\n",
    "    plt.xlabel('Prompt token length');\n",
    "    plt.show()\n",
    "\n",
    "    responses = [chat_with_mixtral(prompt, max_new_tokens=max_new_tokens, verbose=False) for prompt in tqdm(prompts, smoothing=0, desc='Inference')]\n",
    "    test['predicted_prompt'] = responses\n",
    "\n",
    "    hub_url = \"https://www.kaggle.com/models/google/sentence-t5/frameworks/TensorFlow2/variations/st5-base/versions/1\"\n",
    "    encoder = hub.KerasLayer(hub_url)\n",
    "    responses_embeddings = encoder(tf.constant(responses))[0].numpy()\n",
    "    ground_truth_embeddings = encoder(tf.constant(test['rewrite_prompt']))[0].numpy()\n",
    "    similarity = np.sum(responses_embeddings * ground_truth_embeddings, axis=1)\n",
    "    test['cosine_similarity'] = similarity\n",
    "    test['sharpened_cosine_similarity'] = similarity**3\n",
    "    print(f'Mean sharpened cosine similarity: {test[\"sharpened_cosine_similarity\"].mean():.3f} +- {test[\"sharpened_cosine_similarity\"].std()/np.sqrt(len(test))*1.96:.3f}')\n",
    "    plt.hist(test['sharpened_cosine_similarity'], bins=20);\n",
    "    plt.title('Distribution of sharpened cosine similarity');\n",
    "    plt.show()\n",
    "    # TODO: save results\n",
    "\n",
    "def load_test_data(test_filepath, n_samples, random_seed=7):\n",
    "    test = pd.read_csv(test_filepath)\n",
    "    np.random.seed(random_seed)\n",
    "    keep_indices = np.random.choice(test.index, n_samples, replace=False)\n",
    "    test = test.loc[keep_indices]\n",
    "    return test\n",
    "\n",
    "def truncate_txt(text, max_words=2000):\n",
    "    text_list = text.split()\n",
    "    return \" \".join(text_list[:max_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prompt_template(\n",
    "    MODEL_PROMPT, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v1.csv', n_samples=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prompt_template(\n",
    "    MODEL_PROMPT, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v2.csv', n_samples=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_prompt = \"\"\"<s>[INST] Original Text: Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite![/INST]\n",
    "Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.</s>\n",
    "[INST] Re-written Text: Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.[/INST]\n",
    "The request was: Shift the narrative from offering an informal, comforting assurance to providing a more structured advisory note that emphasizes mindfulness and preparedness. This modification should mark a clear change in tone and purpose, aiming to responsibly inform and guide, while maintaining a positive and constructive approach.</s>\n",
    "<s>[INST] Original Text: A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.[/INST]\n",
    "Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.</s>\n",
    "[INST] Re-written Text: Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.[/INST]\n",
    "The request was: Transform your communication from an academic delivery to a dynamic, rhythm-infused presentation. Keep the essence of the information intact but weave in artistic elements, utilizing rhythm, rhyme, and a conversational style. This approach should make the content more relatable and enjoyable, aiming to both educate and entertain your audience.</s>\n",
    "<s>[INST] Original Text: {original_text} [/INST]\n",
    "Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.</s>\n",
    "[INST] Re-written Text: {rewritten_text} [/INST]\n",
    "The request was:\n",
    "\"\"\"\n",
    "evaluate_prompt_template(\n",
    "    forum_prompt, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v2.csv', n_samples=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prompt_template(\n",
    "    forum_prompt, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v1.csv', n_samples=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v1.csv')\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "keep_indices = np.random.choice(test.index, 256, replace=False)\n",
    "test = test.loc[keep_indices]\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROMPT = \"\"\"<s>[INST]\n",
    "# Guess the prompt\n",
    "\n",
    "Your task is to guess which prompt was given to transform the original text into the rewritten text.\n",
    "Read the two texts carefully and answer with the most likely prompt that was used to modify the original text.\n",
    "\n",
    "- The response should be as short and concise as possible. Whenever possible use a single short sentence prompt.\n",
    "- The prompt should be as generic as possible and content agnostic unless it is absolutely necessary to make reference to the content of the original text\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "## Prompt\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def truncate_txt(text, max_words=2000):\n",
    "    text_list = text.split()\n",
    "    return \" \".join(text_list[:max_words])\n",
    "\n",
    "prompts = []\n",
    "for _, row in test.iterrows():\n",
    "    prompts.append(MODEL_PROMPT.format(original_text=truncate_txt(row['original_text']),\n",
    "                                       rewritten_text=truncate_txt(row['rewritten_text'])))\n",
    "\n",
    "plt.hist([len(tokenizer.tokenize(prompt)) for prompt in prompts], bins=20);\n",
    "plt.title('Distribution of prompt token length')\n",
    "plt.xlabel('Prompt token length');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [chat_with_mixtral(prompt, max_new_tokens=25, verbose=False) for prompt in tqdm(prompts, smoothing=0)]\n",
    "test['predicted_prompt'] = responses\n",
    "\n",
    "hub_url = \"https://www.kaggle.com/models/google/sentence-t5/frameworks/TensorFlow2/variations/st5-base/versions/1\"\n",
    "encoder = hub.KerasLayer(hub_url)\n",
    "responses_embeddings = encoder(tf.constant(responses))[0].numpy()\n",
    "ground_truth_embeddings = encoder(tf.constant(test['rewrite_prompt']))[0].numpy()\n",
    "similarity = np.sum(responses_embeddings * ground_truth_embeddings, axis=1)\n",
    "test['cosine_similarity'] = similarity\n",
    "test['sharpened_cosine_similarity'] = similarity**3\n",
    "print(f'Mean sharpened cosine similarity: {test[\"sharpened_cosine_similarity\"].mean():.3f} +- {test[\"sharpened_cosine_similarity\"].std()/np.sqrt(len(test))*1.96:.3f}')\n",
    "plt.hist(test['sharpened_cosine_similarity'], bins=20);\n",
    "plt.title('Distribution of sharpened cosine similarity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [chat_with_mixtral(prompt, max_new_tokens=50, verbose=False) for prompt in tqdm(prompts, smoothing=0)]\n",
    "test['predicted_prompt'] = responses\n",
    "\n",
    "hub_url = \"https://www.kaggle.com/models/google/sentence-t5/frameworks/TensorFlow2/variations/st5-base/versions/1\"\n",
    "encoder = hub.KerasLayer(hub_url)\n",
    "responses_embeddings = encoder(tf.constant(responses))[0].numpy()\n",
    "ground_truth_embeddings = encoder(tf.constant(test['rewrite_prompt']))[0].numpy()\n",
    "similarity = np.sum(responses_embeddings * ground_truth_embeddings, axis=1)\n",
    "test['cosine_similarity'] = similarity\n",
    "test['sharpened_cosine_similarity'] = similarity**3\n",
    "print(f'Mean sharpened cosine similarity: {test[\"sharpened_cosine_similarity\"].mean():.3f} +- {test[\"sharpened_cosine_similarity\"].std()/np.sqrt(len(test))*1.96:.3f}')\n",
    "plt.hist(test['sharpened_cosine_similarity'], bins=20);\n",
    "plt.title('Distribution of sharpened cosine similarity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean sharpened cosine similarity: {test[\"sharpened_cosine_similarity\"].mean():.3f} +- {test[\"sharpened_cosine_similarity\"].std()/np.sqrt(len(test))*1.96:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sharpened_cosine_similarity\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sharpened_cosine_similarity\"].std()/np.sqrt(len(test))*1.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sort_values('sharpened_cosine_similarity', ascending=False).head()[['rewrite_prompt', 'predicted_prompt', 'cosine_similarity', 'sharpened_cosine_similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test['sharpened_cosine_similarity'], bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Evaluate with T5 transformer\n",
    "- [ ] Select the best dataset for testing\n",
    "- [ ] Estimate uncertainty on evaluation\n",
    "- [ ] Save evaluation results to disk for traceability\n",
    "- [ ] Pairwise comparison to have more power to take decisions\n",
    "- [ ] Encapsulate the code to do a prompt evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
