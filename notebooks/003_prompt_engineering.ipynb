{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First steps with prompt engineering:\n",
    "\n",
    "- Choose a validation dataset\n",
    "- Implement a validation pipeline\n",
    "- Try different prompt strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text  # Registers the ops.\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/mnt/hdd0/Kaggle/llm_prompt_recovery/models/mixtral-8x7b-instruct-v0.1-hf'\n",
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    #device_map=\"auto\",\n",
    "    device_map=create_intertwined_device_map(),\n",
    "    #device_map=create_shared_device_map(16),\n",
    "    trust_remote_code=True,\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id # this is needed to do batch inference\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash attention speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/en/model_doc/mixtral#speeding-up-mixtral-by-using-flash-attention\n",
    "\n",
    "```bash\n",
    "export FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE  \n",
    "pip install -U flash-attn --no-build-isolation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    print(chat_with_mixtral('write a poem about real madrid', max_new_tokens=50))\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/bible.txt', 'r') as f:\n",
    "    bible = f.read()\n",
    "print(chat_with_mixtral(bible[:24000], max_new_tokens=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# flash-attention\n",
    "Execution Time : 5.4 s, tokens per second: 9.4\n",
    "Execution Time : 9.1 s, tokens per second: 5.6\n",
    "\n",
    "# baseline\n",
    "Execution Time : 5.1 s, tokens per second: 10.0\n",
    "Execution Time : 9.2 s, tokens per second: 5.5\n",
    "```\n",
    "\n",
    "Not clear difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that batch prediction works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [f'<s>[INST] What is the history of {country}? [/INST]' for country in ['Spain', 'France', 'Germany', 'Italy']]\n",
    "pipe(prompts, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[chat_with_mixtral(prompt, max_new_tokens=50) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "pipe_bs = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, batch_size=batch_size)\n",
    "pipe_bs(prompts, do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bs(prompts[:3] + [bible[:24000]], do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the duration when batching inputs is determined by the longest input and output. Thus it might have sense to order the inputs by length.\n",
    "\n",
    "All the predictions are the same, despite the method used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study public datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/481811"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I study the datasets? I have to read them\n",
    "\n",
    "I could read the prompts, this needs time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_dataset(filepath):\n",
    "    \"\"\" Search for sure and rewritten on the text to see the patterns \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    assert all([key in df.columns for key in ['original_text', 'rewritten_text', 'rewrite_prompt']])\n",
    "    if not 'id' in df.columns:\n",
    "        df['id'] = np.arange(len(df))\n",
    "    df['rewritten_text'] = df['rewritten_text'].apply(lambda x: x.lstrip())\n",
    "    remove_texts = [\n",
    "        \"Sure, here's the rewritten text:\",\n",
    "        \"**Rewritten Text:**\",\n",
    "        \"**Rewritten text:**\",\n",
    "        \"Certainly. Here's the rewritten text:\",\n",
    "    ]\n",
    "    for text in remove_texts:\n",
    "        df['rewritten_text'] = df['rewritten_text'].apply(lambda x: x.replace(text, \"\"))\n",
    "\n",
    "    patterns = [\n",
    "        r\"^Sure.*?:\",\n",
    "        r\"^Here.*?:\",\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        df['rewritten_text'] = df['rewritten_text'].apply(lambda x: re.sub(pattern, \"\", x))\n",
    "\n",
    "    df['rewritten_text'] = df['rewritten_text'].apply(lambda x: x.lstrip())\n",
    "    df.to_csv(filepath.replace('.csv', '_curated.csv'), index=False)\n",
    "\n",
    "curate_dataset('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v2.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/newtonbaba/gemma_data_set_prompt_recover_1.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/dipamc77_prompts_0_500_wiki_first_para_3000.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/alexxxsem/data_gemma_0_1000.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/galileo/gemma_v1_7b-it.csv',\n",
    "]\n",
    "for filepath in filepaths:\n",
    "    print(filepath)\n",
    "    curate_dataset(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = filepaths[3]\n",
    "df = pd.read_csv(filepath)\n",
    "df = pd.read_csv(filepath.replace('.csv', '_curated.csv'))\n",
    "\n",
    "df['rewritten_text'].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'\\n\\nHere is the text rewritten in the style of a Victorian gentlem'.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v1.csv').head(64)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROMPT = \"\"\"<s>[INST]\n",
    "# Guess the prompt\n",
    "\n",
    "Your task is to guess which prompt was given to transform the original text into the rewritten text.\n",
    "Read the two texts carefully and answer with the most likely prompt that was used to modify the original text.\n",
    "\n",
    "- The response should be as short and concise as possible. Whenever possible use a single short sentence prompt.\n",
    "- The prompt should be as generic as possible and content agnostic unless it is absolutely necessary to make reference to the content of the original text\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "## Prompt\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def truncate_txt(text, max_words=2000):\n",
    "    text_list = text.split()\n",
    "    return \" \".join(text_list[:max_words])\n",
    "\n",
    "prompts = []\n",
    "for _, row in test.iterrows():\n",
    "    prompts.append(MODEL_PROMPT.format(original_text=truncate_txt(row['original_text']),\n",
    "                                       rewritten_text=truncate_txt(row['rewritten_text'])))\n",
    "\n",
    "plt.hist([len(tokenizer.tokenize(prompt)) for prompt in prompts], bins=20);\n",
    "plt.title('Distribution of prompt token length')\n",
    "plt.xlabel('Prompt token length');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [chat_with_mixtral(prompt, max_new_tokens=25, verbose=False) for prompt in tqdm(prompts, smoothing=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the predictions one by one takes 3m2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "responses_bs = pipe(prompts, do_sample=False, return_full_text=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id, max_new_tokens=25,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lengths = [len(tokenizer.tokenize(prompt)) for prompt in prompts]\n",
    "sorted_prompts = [prompt for _, prompt in sorted(zip(prompt_lengths, prompts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "responses_bs = pipe(sorted_prompts, do_sample=False, return_full_text=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id, max_new_tokens=25,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class PromptsDataset(Dataset):\n",
    "    def __init__(self, prompts):\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "responses_bs = pipe(PromptsDataset(sorted_prompts), do_sample=False, return_full_text=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id, max_new_tokens=25,\n",
    "                    batch_size=batch_size)\n",
    "responses_bs = list(responses_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "responses_bs = pipe(sorted_prompts, do_sample=False, return_full_text=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id, max_new_tokens=25,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MODEL_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I cannot use a batch size higher than 8, which results in a very modest speedup of 2min vs 3min. And that is considering the current input tokens which may vary.\n",
    "- The batch affects to some of the responses, it might be due to numerical errors\n",
    "- Using a torch dataset does not speedup the inference\n",
    "\n",
    "Thus I believe the best option is to use a simple sequential inference. To be fast I should use a small evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some responses change when using batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "batch_size = 1\n",
    "print(pipe(prompts[idx: idx + batch_size], do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=25, batch_size=batch_size)[0][0]['generated_text'])\n",
    "\n",
    "batch_size = 4\n",
    "print(pipe(prompts[idx: idx + batch_size], do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=25, batch_size=batch_size)[0][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\",\n",
    "    padding='do_not_pad')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id # this is needed to do batch inference\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gc.collect()\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "pipe(prompts[:batch_size], do_sample=False, return_full_text=False, pad_token_id=tokenizer.eos_token_id, max_new_tokens=25, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_url = \"https://www.kaggle.com/models/google/sentence-t5/frameworks/TensorFlow2/variations/st5-base/versions/1\"\n",
    "encoder = hub.KerasLayer(hub_url)\n",
    "\n",
    "RESULTS_FILEPATH = '/mnt/hdd0/Kaggle/llm_prompt_recovery/evaluations/mixtral_prompt_engineering.yaml'\n",
    "RESULTS_FOLDER = '/mnt/hdd0/Kaggle/llm_prompt_recovery/evaluations/mixtral_prompt_engineering'\n",
    "\n",
    "def evaluate_prompt_template(prompt_template, max_new_tokens, test_filepath, n_samples,\n",
    "                             random_seed=7, verbose=False, filter_multiline_responses=True):\n",
    "    if is_already_evaluated(prompt_template, test_filepath, random_seed,\n",
    "                            max_new_tokens, n_samples, filter_multiline_responses):\n",
    "        test = load_results_dataframe(prompt_template, test_filepath, random_seed,\n",
    "                           max_new_tokens, n_samples, filter_multiline_responses)\n",
    "        return test\n",
    "\n",
    "    test = load_test_data(test_filepath, n_samples, random_seed)\n",
    "    prompts = create_prompts(test, prompt_template, verbose=verbose)\n",
    "    responses = [chat_with_mixtral(prompt, max_new_tokens=max_new_tokens, verbose=False)\n",
    "                 for prompt in tqdm(prompts, smoothing=0, desc='Inference')]\n",
    "    if filter_multiline_responses:\n",
    "        test['raw_predicted_prompt'] = responses\n",
    "        responses = [response.lstrip('\\n').split('\\n')[0] for response in responses]\n",
    "    test = evaluate_responses(test, responses, verbose=verbose)\n",
    "    save_results(test, prompt_template, test_filepath, random_seed,\n",
    "                 max_new_tokens, filter_multiline_responses)\n",
    "    return test\n",
    "\n",
    "def load_test_data(test_filepath, n_samples, random_seed=7):\n",
    "    test = pd.read_csv(test_filepath)\n",
    "    np.random.seed(random_seed)\n",
    "    keep_indices = np.random.choice(test.index, n_samples, replace=False)\n",
    "    test = test.loc[keep_indices]\n",
    "    return test\n",
    "\n",
    "def truncate_txt(text, max_words=2000):\n",
    "    text_list = str(text).split()\n",
    "    return \" \".join(text_list[:max_words])\n",
    "\n",
    "def create_prompts(test, prompt_template, verbose=True):\n",
    "    prompts = []\n",
    "    for _, row in test.iterrows():\n",
    "        prompts.append(prompt_template.format(original_text=truncate_txt(row['original_text']),\n",
    "                                              rewritten_text=truncate_txt(row['rewritten_text'])))\n",
    "    if verbose:\n",
    "        plt.hist([len(tokenizer.tokenize(prompt)) for prompt in prompts], bins=20);\n",
    "        plt.title('Distribution of prompt token length')\n",
    "        plt.xlabel('Prompt token length');\n",
    "        plt.show()\n",
    "    return prompts\n",
    "\n",
    "def evaluate_responses(test, responses, verbose=True):\n",
    "    test['predicted_prompt'] = responses\n",
    "    responses_embeddings = compute_t5_embeddings(responses)\n",
    "    ground_truth_embeddings =compute_t5_embeddings(test['rewrite_prompt'])\n",
    "    similarity = np.sum(responses_embeddings * ground_truth_embeddings, axis=1)\n",
    "    test['cosine_similarity'] = similarity\n",
    "    test['sharpened_cosine_similarity'] = similarity**3\n",
    "    print(f'Mean sharpened cosine similarity: {test[\"sharpened_cosine_similarity\"].mean():.3f} +- {test[\"sharpened_cosine_similarity\"].std()/np.sqrt(len(test))*1.96:.3f}')\n",
    "    if verbose:\n",
    "        plt.hist(test['sharpened_cosine_similarity'], bins=20);\n",
    "        plt.title('Distribution of sharpened cosine similarity');\n",
    "        plt.show()\n",
    "    return test\n",
    "\n",
    "def compute_t5_embeddings(texts):\n",
    "    \"\"\" I have verified that the embeddings have unit norm \"\"\"\n",
    "    embeddings = encoder(tf.constant(texts))[0].numpy()\n",
    "    return embeddings\n",
    "\n",
    "def save_results(test, prompt_template, test_filepath, random_seed,\n",
    "                 max_new_tokens, filter_multiline_responses,\n",
    "                 output_filepath=RESULTS_FILEPATH,\n",
    "                 output_folder=RESULTS_FOLDER):\n",
    "    results = load_results(output_filepath)\n",
    "    if not test_filepath in results:\n",
    "        results[test_filepath] = dict()\n",
    "\n",
    "    hash = get_evaluation_hash(prompt_template, test_filepath, random_seed,\n",
    "                               max_new_tokens, len(test), filter_multiline_responses)\n",
    "    print(f'Saving results to {hash}.csv')\n",
    "    results[test_filepath][hash] = dict(\n",
    "        prompt_template=prompt_template,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        random_seed=random_seed,\n",
    "        n_samples=len(test),\n",
    "        responses=test['predicted_prompt'].tolist(),\n",
    "        ids=test['id'].tolist(),\n",
    "        sharpened_cosine_similarity=test['sharpened_cosine_similarity'].tolist(),\n",
    "        mean_sharpened_cosine_similarity=round(float(test['sharpened_cosine_similarity'].mean()), 4),\n",
    "    )\n",
    "    with open(output_filepath, 'w') as f:\n",
    "        yaml.dump(results, f)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    test.to_csv(os.path.join(output_folder, f\"{hash}.csv\"), index=False)\n",
    "\n",
    "\n",
    "def load_results_dataframe(prompt_template, test_filepath, random_seed,\n",
    "                           max_new_tokens, n_samples, filter_multiline_responses,\n",
    "                           output_folder=RESULTS_FOLDER):\n",
    "    hash = get_evaluation_hash(prompt_template, test_filepath, random_seed, max_new_tokens, n_samples, filter_multiline_responses)\n",
    "    print(f'Loading results from {hash}.csv')\n",
    "    output_filepath = os.path.join(output_folder, f\"{hash}.csv\")\n",
    "    test = pd.read_csv(output_filepath)\n",
    "    print(f'Mean sharpened cosine similarity: {test[\"sharpened_cosine_similarity\"].mean():.3f} +- {test[\"sharpened_cosine_similarity\"].std()/np.sqrt(len(test))*1.96:.3f}')\n",
    "    return test\n",
    "\n",
    "\n",
    "def load_results(output_filepath):\n",
    "    if os.path.exists(output_filepath):\n",
    "        with open(output_filepath, 'r') as f:\n",
    "            results = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    else:\n",
    "        results = {}\n",
    "    return results\n",
    "\n",
    "\n",
    "def is_already_evaluated(prompt_template, test_filepath, random_seed, max_new_tokens,\n",
    "                         n_samples, filter_multiline_responses, output_filepath=RESULTS_FILEPATH):\n",
    "    hash = get_evaluation_hash(prompt_template, test_filepath, random_seed,\n",
    "                               max_new_tokens, n_samples, filter_multiline_responses)\n",
    "    results = load_results(output_filepath)\n",
    "    return hash in results.get(test_filepath, dict())\n",
    "\n",
    "\n",
    "def get_evaluation_hash(prompt_template, test_filepath, random_seed, max_new_tokens, n_samples, filter_multiline_responses):\n",
    "    input_string = f\"{prompt_template}{test_filepath}{random_seed}{max_new_tokens}{n_samples}{filter_multiline_responses}\"\n",
    "    return create_hash(input_string)\n",
    "\n",
    "def create_hash(input_string, algorithm='sha256'):\n",
    "    try:\n",
    "        hash_object = hashlib.new(algorithm)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Unsupported hashing algorithm: {algorithm}\")\n",
    "    hash_object.update(input_string.encode())\n",
    "    return hash_object.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_worse_predictions(test, n=5):\n",
    "    display_columns = ['original_text', 'rewritten_text', 'rewrite_prompt', 'predicted_prompt', 'sharpened_cosine_similarity']\n",
    "    display_columns = ['rewrite_prompt', 'predicted_prompt', 'sharpened_cosine_similarity']\n",
    "    return test[display_columns].sort_values('sharpened_cosine_similarity', ascending=True).head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gemma_suppl_rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to overfit to `'/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated.csv'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST]\n",
    "# Guess the instruction\n",
    "\n",
    "Your task is to guess which instruction was given to transform the original text into the rewritten text.\n",
    "Read the two texts carefully and answer with the most likely prompt that was used to modify the original text.\n",
    "\n",
    "- The response should be as short and concise as possible. Whenever possible use a single short sentence instruction.\n",
    "- The instruction should be as generic as possible and content agnostic unless it is absolutely necessary to make reference to the content of the original text\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "## Instruction\n",
    "\n",
    "\"\"\"\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_prompt = \"\"\"<s>[INST] Original Text: Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite![/INST]\n",
    "Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.</s>\n",
    "[INST] Re-written Text: Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.[/INST]\n",
    "The request was: Shift the narrative from offering an informal, comforting assurance to providing a more structured advisory note that emphasizes mindfulness and preparedness. This modification should mark a clear change in tone and purpose, aiming to responsibly inform and guide, while maintaining a positive and constructive approach.</s>\n",
    "<s>[INST] Original Text: A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.[/INST]\n",
    "Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.</s>\n",
    "[INST] Re-written Text: Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.[/INST]\n",
    "The request was: Transform your communication from an academic delivery to a dynamic, rhythm-infused presentation. Keep the essence of the information intact but weave in artistic elements, utilizing rhythm, rhyme, and a conversational style. This approach should make the content more relatable and enjoyable, aiming to both educate and entertain your audience.</s>\n",
    "<s>[INST] Original Text: {original_text} [/INST]\n",
    "Provide the modified text and I'll tell you a something general that changed about it.  I'll avoid any specifics though.  My tone will be neutral.</s>\n",
    "[INST] Re-written Text: {rewritten_text} [/INST]\n",
    "The request was: \"\"\"\n",
    "test = evaluate_prompt_template(\n",
    "    forum_prompt, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "examples = [\n",
    "    {\n",
    "        'original_text': \"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\",\n",
    "        'rewritten_text': \"(Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\",\n",
    "        'instruction': \"Convert this into a sea shanty\"\n",
    "    },\n",
    "    {\n",
    "        'original_text': \"Chocolate, a beloved treat worldwide, originates from the cacao tree's seeds, native to the Americas. Its history spans over 3,000 years, evolving from a bitter beverage to the sweet delicacies we relish today. Esteemed by ancient civilizations like the Maya and Aztecs, chocolate was more than a food; it symbolized wealth, power, and even had spiritual significance. The Spanish introduction of chocolate to Europe transformed it into a global phenomenon, leading to the creation of diverse chocolate products. Today, chocolate is celebrated for its rich flavors and mood-enhancing properties, making it a universal symbol of indulgence and affection.\",\n",
    "        'rewritten_text': \"Oh chocolate, thou sweet regale of tongues, birthed from the seeds of cacao's embrace, cradled in the Americas' bosom. Thine ancient roots weave back yon three millennia, transmuting from bitter draught to sweet ambrosia cherished in modern banquet. Held in high esteem by Maya and Aztec alike, thee was more than mere sustenance; a symbol of opulence, power, and the divine. When Spaniards didst bear thee across the seas, Europe's palate was forever changed, birthing a myriad of confections. Now, celebrated for thy velvety richness and spirit-lifting virtues, chocolate remains a beacon of delight and affection, universally adored.\",\n",
    "        'instruction': \"Rewrite this essay but do it using the writing style of William Shakespeare\"\n",
    "    },\n",
    "    {\n",
    "        'original_text': \"In a small town nestled between rolling hills and vast fields, there lived a cat named Oliver. Every evening, as the sun dipped below the horizon, painting the sky in shades of orange and purple, Oliver would embark on his nightly adventures. With the moon as his guide, he explored hidden nooks, greeted nocturnal friends, and chased fleeting shadows. His life was simple yet full of wonder, a testament to finding joy in the smallest of moments.\",\n",
    "        'rewritten_text': \"En un peque√±o pueblo encajado entre colinas ondulantes y vastos campos, viv√≠a un gato llamado Oliver. Cada atardecer, cuando el sol se sumerg√≠a bajo el horizonte, pintando el cielo de tonos naranjas y morados, Oliver emprend√≠a sus aventuras nocturnas. Con la luna como su gu√≠a, exploraba rincones ocultos, saludaba a amigos nocturnos y persegu√≠a sombras fugaces. Su vida era sencilla pero llena de maravillas, un testimonio de encontrar alegr√≠a en los momentos m√°s peque√±os.\",\n",
    "        'instruction': \"Translate to spanish\"\n",
    "    },\n",
    "    {\n",
    "        'original_text': \"Quantum mechanics is a fundamental theory in physics that describes the behavior of particles at the microscopic scale. It introduces concepts like wave-particle duality, quantization of energy, and uncertainty principles, fundamentally changing our understanding of matter and energy.\",\n",
    "        'rewritten_text': \"Quantum mechanics is like a magic rule book for tiny things like atoms and particles. It tells us that these tiny things can be in different places at the same time and can act like both balls and waves. It's how the smallest parts of our world work!\",\n",
    "        'instruction': \"Improve the text as if you were explaining it to a five-year-old\"\n",
    "    },\n",
    "    {\n",
    "        'original_text': \"In a quaint village, a forgotten ice cream recipe rekindled friendships long cooled. The shared delight of creamy sweetness under the sun's gaze melted barriers, spoonful by spoonful. Laughter echoed, as flavors of joy united the community, proving happiness is best served cold and shared.\",\n",
    "        'rewritten_text': \"A forgotten ice cream recipe brings a divided village together, rekindling friendships and joy.\",\n",
    "        'instruction': \"Summarize the story in a single short sentence\"\n",
    "    },\n",
    "    {\n",
    "        'original_text': \"In a quaint village, a forgotten ice cream recipe rekindled friendships long cooled. The shared delight of creamy sweetness under the sun's gaze melted barriers, spoonful by spoonful. Laughter echoed, as flavors of joy united the community, proving happiness is best served cold and shared.\",\n",
    "        'rewritten_text': \"Sweet Unity\",\n",
    "        'instruction': \"What is the title of the story?\"\n",
    "    },\n",
    "    {\n",
    "        'original_text': \"Hey! üòä Hope you're doing awesome today. I was thinking, it‚Äôs been a while since we caught up, and what better way to do that than over some good food? How about dinner tonight? Let‚Äôs pick a place we both love or try out that new spot we've been talking about. What do you say? Let me know what works for you! üçΩÔ∏èüëØ‚Äç‚ôÇÔ∏è\",\n",
    "        'rewritten_text': \"I hope this message finds you well. I am reaching out to propose an opportunity for us to catch up, as it has been some time since our last meeting. Considering the importance of our friendship and our shared love for good cuisine, I suggest we convene for dinner this evening.\\n\\nThere is the option of visiting a restaurant we both hold in high regard or exploring a new establishment that has recently caught our interest. Please let me know your availability for tonight and if there is a specific venue you would prefer.\\n\\nI look forward to your prompt response and to the possibility of an enjoyable evening together.\",\n",
    "        'instruction': \"Rewrite the text in a more formal tone.\"\n",
    "    },\n",
    "    {\n",
    "        'original_text': 'In a thrilling encounter, Liverpool faced off against Real Madrid in a match that showcased football at its finest. With both teams displaying exceptional skill and determination, the game was a testament to their storied rivalry, leaving fans on the edge of their seats until the final whistle.',\n",
    "        'rewritten_text': 'In an exhilarating clash, Liverpool went head-to-head with Real Madrid, highlighting the pinnacle of football excellence. Both squads demonstrated unparalleled prowess and resilience, underlining their legendary competition. Spectators remained captivated throughout, eagerly anticipating the outcome until the last moment.',\n",
    "        'instruction': 'Rewrite the text using a different word choice',\n",
    "    },\n",
    "    {\n",
    "        'original_text': 'Dear Bob,\\n\\nI am interested in purchasing a used Toyota car listed by you. Could you please provide more details, including the condition, year, and asking price?\\n\\nThank you,\\n',\n",
    "        'rewritten_text': \"Hey Bob,\\n\\nI'm looking to buy a used Toyota you've listed. Can you share more about it, like how it's holding up, its year, and how much you're asking?\\n\\nThanks,\",\n",
    "        'instruction': 'Rewrite the text in a more informal tone',\n",
    "    },\n",
    "    {\n",
    "        'original_text': \"{original_text}\",\n",
    "        'rewritten_text': \"{rewritten_text}\",\n",
    "        'instruction': '',\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = \"Analyze the original and rewritten text and answer with the most likely instruction that was given to rewrite or make stylistic changes to the original text.\"\n",
    "\n",
    "\n",
    "\n",
    "def format_messages_for_mixtral(messages, change_line_between_messages=True):\n",
    "    text = '<s>'\n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            text += f'[INST] {message[\"content\"]} [/INST]'\n",
    "        else:\n",
    "            text += f'{message[\"content\"]}</s>'\n",
    "        if change_line_between_messages:\n",
    "            text += '\\n'\n",
    "    return text\n",
    "\n",
    "example_template = \"\"\"\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': intro},\n",
    "    {'role': 'assistant', 'content': \"I understand the task, I'm going to take a deep breath, concentrate and we can start.\"},\n",
    "]\n",
    "for example in examples:\n",
    "    messages.append({'role': 'user', 'content': f'Original text: {example[\"original_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': 'Give me the rewritten text and I will give you the instruction'})\n",
    "    messages.append({'role': 'user', 'content': f'Rewritten text: {example[\"rewritten_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': f'The instruction was: {example[\"instruction\"]}'})\n",
    "prompt_template = format_messages_for_mixtral(messages, change_line_between_messages=False)[:-5]\n",
    "print(f'Prompt template tokens: {len(tokenizer.tokenize(prompt_template))}')\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"\n",
    "## Example {n}\n",
    "\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "### Instruction\n",
    "\n",
    "{instruction}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "task_template = \"\"\"\n",
    "## Task\n",
    "\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "### Instruction\n",
    "\n",
    "{instruction}\n",
    "\"\"\"\n",
    "\n",
    "def markdown_format_prompt(examples):\n",
    "    text = \"\"\"<s>[INST]\n",
    "Analyze the original and rewritten text and answer with the most likely instruction that was given\n",
    "to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "Below you can see some examples and finally the task you have to do.\n",
    "\"\"\"\n",
    "    for idx, example in enumerate(examples[:-1]):\n",
    "        text += example_template.format(n=idx+1, **example)\n",
    "    text += task_template.format(**examples[-1])\n",
    "    return text\n",
    "\n",
    "prompt_template = markdown_format_prompt(examples)\n",
    "print(f'Prompt template tokens: {len(tokenizer.tokenize(prompt_template))}')\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"\n",
    "## Example {n}\n",
    "\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "### Instruction\n",
    "\n",
    "The instruction given to rewrite this text is: {instruction}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "task_template = \"\"\"\n",
    "## Task\n",
    "\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "### Instruction\n",
    "\n",
    "The instruction given to rewrite this text is: {instruction}\n",
    "\"\"\"\n",
    "\n",
    "def markdown_format_prompt(examples):\n",
    "    text = \"\"\"<s>[INST]\n",
    "Analyze the original and rewritten text and answer with the most likely instruction that was given\n",
    "to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "Below you can see some examples and finally the task you have to do.\n",
    "\"\"\"\n",
    "    for idx, example in enumerate(examples[:-1]):\n",
    "        text += example_template.format(n=idx+1, **example)\n",
    "    text += task_template.format(**examples[-1])\n",
    "    return text\n",
    "\n",
    "prompt_template = markdown_format_prompt(examples)\n",
    "print(f'Prompt template tokens: {len(tokenizer.tokenize(prompt_template))}')\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_worse_predictions(test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluating the whole dataset takes 20 minutes.\n",
    "- If I evaluate a subset the result has too much uncertainty and it is not possible to measure small improvements, in fact the mean score can get worse. Even when using 300 samples the uncertainty is ~0.015. The public leaderboard uses 195 samples so the uncertainty will be around ~0.02\n",
    "- There are many different prompts that can lead to the same output\n",
    "- By looking at the \"errors\" I don't see more options to add examples to the input. On this dataset I don't think I'm able to improve the score. There is a tension between detailed and abstract prompts, I feel I cannot improve one at the cost of the other. My prompts are in the abstract zone.\n",
    "- The predictions of the model are very reasonable. Thus it is hard to understand why it scores just 0.59 on LB while submitting a generic string like \"Improve the text to this\" scores 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to evaluate other dataset to get more ideas to improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nbroad-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = \"Analyze the original and rewritten text and answer with the most likely instruction that was given to rewrite or make stylistic changes to the original text.\"\n",
    "\n",
    "\n",
    "\n",
    "def format_messages_for_mixtral(messages, change_line_between_messages=True):\n",
    "    text = '<s>'\n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            text += f'[INST] {message[\"content\"]} [/INST]'\n",
    "        else:\n",
    "            text += f'{message[\"content\"]}</s>'\n",
    "        if change_line_between_messages:\n",
    "            text += '\\n'\n",
    "    return text\n",
    "\n",
    "example_template = \"\"\"\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': intro},\n",
    "    {'role': 'assistant', 'content': \"I understand the task, I'm going to take a deep breath, concentrate and we can start.\"},\n",
    "]\n",
    "for example in examples:\n",
    "    messages.append({'role': 'user', 'content': f'Original text: {example[\"original_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': 'Give me the rewritten text and I will give you the instruction'})\n",
    "    messages.append({'role': 'user', 'content': f'Rewritten text: {example[\"rewritten_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': f'The instruction was: {example[\"instruction\"]}'})\n",
    "prompt_template = format_messages_for_mixtral(messages, change_line_between_messages=False)[:-5]\n",
    "print(f'Prompt template tokens: {len(tokenizer.tokenize(prompt_template))}')\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v2_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"\n",
    "## Example {n}\n",
    "\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "### Instruction\n",
    "\n",
    "The instruction given to rewrite this text is: {instruction}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "task_template = \"\"\"\n",
    "## Task\n",
    "\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "### Instruction\n",
    "\n",
    "The instruction given to rewrite this text is: {instruction}\n",
    "\"\"\"\n",
    "\n",
    "def markdown_format_prompt(examples):\n",
    "    text = \"\"\"<s>[INST]\n",
    "Analyze the original and rewritten text and answer with the most likely instruction that was given\n",
    "to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "Below you can see some examples and finally the task you have to do.\n",
    "\"\"\"\n",
    "    for idx, example in enumerate(examples[:-1]):\n",
    "        text += example_template.format(n=idx+1, **example)\n",
    "    text += task_template.format(**examples[-1])\n",
    "    return text\n",
    "\n",
    "prompt_template = markdown_format_prompt(examples)\n",
    "print(f'Prompt template tokens: {len(tokenizer.tokenize(prompt_template))}')\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v2_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_worse_predictions(test, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why some inferences start by `1.` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = \"Analyze the original and rewritten text and answer with the most likely instruction that was given to rewrite or make stylistic changes to the original text.\"\n",
    "\n",
    "\n",
    "\n",
    "def format_messages_for_mixtral(messages, change_line_between_messages=True):\n",
    "    text = '<s>'\n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            text += f'[INST] {message[\"content\"]} [/INST]'\n",
    "        else:\n",
    "            text += f'{message[\"content\"]}</s>'\n",
    "        if change_line_between_messages:\n",
    "            text += '\\n'\n",
    "    return text\n",
    "\n",
    "example_template = \"\"\"\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': intro},\n",
    "    {'role': 'assistant', 'content': \"I understand the task, I'm going to take a deep breath, concentrate and we can start.\"},\n",
    "]\n",
    "for example in examples:\n",
    "    messages.append({'role': 'user', 'content': f'Original text: {example[\"original_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': 'Give me the rewritten text and I will give you the instruction'})\n",
    "    messages.append({'role': 'user', 'content': f'Rewritten text: {example[\"rewritten_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': f'The instruction was: {example[\"instruction\"]}'})\n",
    "prompt_template = format_messages_for_mixtral(messages, change_line_between_messages=False)[:-5]\n",
    "print(f'Prompt template tokens: {len(tokenizer.tokenize(prompt_template))}')\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/newtonbaba/gemma_data_set_prompt_recover_1_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = \"Analyze the original and rewritten text and answer with the most likely instruction that was given to rewrite or make stylistic changes to the original text.\"\n",
    "\n",
    "\n",
    "\n",
    "def format_messages_for_mixtral(messages, change_line_between_messages=True):\n",
    "    text = '<s>'\n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            text += f'[INST] {message[\"content\"]} [/INST]'\n",
    "        else:\n",
    "            text += f'{message[\"content\"]}</s>'\n",
    "        if change_line_between_messages:\n",
    "            text += '\\n'\n",
    "    return text\n",
    "\n",
    "example_template = \"\"\"\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': intro},\n",
    "    {'role': 'assistant', 'content': \"I understand the task, I'm going to take a deep breath, concentrate and we can start.\"},\n",
    "]\n",
    "for example in examples:\n",
    "    messages.append({'role': 'user', 'content': f'Original text: {example[\"original_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': 'Give me the rewritten text and I will give you the instruction'})\n",
    "    messages.append({'role': 'user', 'content': f'Rewritten text: {example[\"rewritten_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': f'The instruction was: {example[\"instruction\"]}'})\n",
    "prompt_template = format_messages_for_mixtral(messages, change_line_between_messages=False)[:-5]\n",
    "print(f'Prompt template tokens: {len(tokenizer.tokenize(prompt_template))}')\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/dipamc77_prompts_0_500_wiki_first_para_3000_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = \"Analyze the original and rewritten text and answer with the most likely instruction that was given to rewrite or make stylistic changes to the original text.\"\n",
    "\n",
    "\n",
    "\n",
    "def format_messages_for_mixtral(messages, change_line_between_messages=True):\n",
    "    text = '<s>'\n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            text += f'[INST] {message[\"content\"]} [/INST]'\n",
    "        else:\n",
    "            text += f'{message[\"content\"]}</s>'\n",
    "        if change_line_between_messages:\n",
    "            text += '\\n'\n",
    "    return text\n",
    "\n",
    "example_template = \"\"\"\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': intro},\n",
    "    {'role': 'assistant', 'content': \"I understand the task, I'm going to take a deep breath, concentrate and we can start.\"},\n",
    "]\n",
    "for example in examples:\n",
    "    messages.append({'role': 'user', 'content': f'Original text: {example[\"original_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': 'Give me the rewritten text and I will give you the instruction'})\n",
    "    messages.append({'role': 'user', 'content': f'Rewritten text: {example[\"rewritten_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': f'The instruction was: {example[\"instruction\"]}'})\n",
    "prompt_template = format_messages_for_mixtral(messages, change_line_between_messages=False)[:-5]\n",
    "print(f'Prompt template tokens: {len(tokenizer.tokenize(prompt_template))}')\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/alexxxsem/data_gemma_0_1000_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = \"Analyze the original and rewritten text and answer with the most likely instruction that was given to rewrite or make stylistic changes to the original text.\"\n",
    "\n",
    "\n",
    "\n",
    "def format_messages_for_mixtral(messages, change_line_between_messages=True):\n",
    "    text = '<s>'\n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            text += f'[INST] {message[\"content\"]} [/INST]'\n",
    "        else:\n",
    "            text += f'{message[\"content\"]}</s>'\n",
    "        if change_line_between_messages:\n",
    "            text += '\\n'\n",
    "    return text\n",
    "\n",
    "example_template = \"\"\"\n",
    "### Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "### Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': intro},\n",
    "    {'role': 'assistant', 'content': \"I understand the task, I'm going to take a deep breath, concentrate and we can start.\"},\n",
    "]\n",
    "for example in examples:\n",
    "    messages.append({'role': 'user', 'content': f'Original text: {example[\"original_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': 'Give me the rewritten text and I will give you the instruction'})\n",
    "    messages.append({'role': 'user', 'content': f'Rewritten text: {example[\"rewritten_text\"]}'})\n",
    "    messages.append({'role': 'assistant', 'content': f'The instruction was: {example[\"instruction\"]}'})\n",
    "prompt_template = format_messages_for_mixtral(messages, change_line_between_messages=False)[:-5]\n",
    "print(f'Prompt template tokens: {len(tokenizer.tokenize(prompt_template))}')\n",
    "test = evaluate_prompt_template(\n",
    "    prompt_template, max_new_tokens=75,\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/galileo/gemma_v1_7b-it_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_worse_predictions(test, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some answers start by: Incorrect\n",
    "- Use shorter answers\n",
    "- Prompt, instruction, order... What is the best way to frame it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple prompts evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what score we get if we try simple prompts like `Improve the text to this`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_simple_prompt(prompt, test_filepath, n_samples,\n",
    "                             random_seed=7, verbose=False):\n",
    "    test = load_test_data(test_filepath, n_samples, random_seed)\n",
    "    test = evaluate_responses(test, [prompt]*len(test), verbose=verbose)\n",
    "\n",
    "def eval_simple_prompt_on_multiple_datasets(prompt, n_samples=300):\n",
    "    filepaths = [\n",
    "        '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite.csv',\n",
    "        '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v2.csv',\n",
    "        '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/newtonbaba/gemma_data_set_prompt_recover_1.csv',\n",
    "        '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/dipamc77_prompts_0_500_wiki_first_para_3000.csv',\n",
    "        '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/alexxxsem/data_gemma_0_1000.csv',\n",
    "        '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/galileo/gemma_v1_7b-it.csv',\n",
    "    ]\n",
    "    for filepath in filepaths:\n",
    "        print(filepath)\n",
    "        evaluate_simple_prompt(prompt, filepath, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_simple_prompt_on_multiple_datasets('Improve the text to this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = evaluate_simple_prompt(\n",
    "    'Improve the text to this',\n",
    "    test_filepath='/mnt/hdd0/Kaggle/llm_prompt_recovery/data/newtonbaba/gemma_data_set_prompt_recover_1_curated.csv', n_samples=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Evaluate with T5 transformer\n",
    "- [x] Select the best dataset for testing\n",
    "- [x] Estimate uncertainty on evaluation\n",
    "- [x] Save evaluation results to disk for traceability\n",
    "- [ ] Pairwise comparison to have more power to take decisions\n",
    "- [x] Encapsulate the code to do a prompt evaluation\n",
    "- [ ] Is it better to use a long instruction, or a conversation? Probably a conversation but verify it.\n",
    "- [x] Improve multiline cleaning\n",
    "- [ ] Evaluate simple prompt like \"Improve the text to this\"\n",
    "- [ ] Maybe force the model to make very short predictions: f.e. less than 10 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
