{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance Mixtral GPU Memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I have a more balanced GPU memory usage?\n",
    "\n",
    "It will allow to use a bigger input size or batch size on submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/mnt/hdd0/Kaggle/llm_prompt_recovery/models/mixtral-8x7b-instruct-v0.1-hf'\n",
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    #device_map=\"auto\",\n",
    "    device_map=create_intertwined_device_map(),\n",
    "    #device_map=create_shared_device_map(16),\n",
    "    trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id # this is needed to do batch inference\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    print(chat_with_mixtral('write a poem about real madrid', max_new_tokens=25))\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# device_map='auto'\n",
    "Execution Time : 2.9 s, tokens per second: 9.1\n",
    " In the heart of Spain, where the passion runs deep,\n",
    "Lies a team called Real Madrid, a treasure to keep\n",
    "GPU 0 memory allocated: 10.2 GB, max memory allocated: 10.3 GB\n",
    "GPU 1 memory allocated: 13.1 GB, max memory allocated: 13.2 GB\n",
    "\n",
    "# device_map=auto_device_map\n",
    "Execution Time : 2.7 s, tokens per second: 9.5\n",
    " In the heart of Spain, where the passion runs deep,\n",
    "Lies a team called Real Madrid, a treasure to keep\n",
    "GPU 0 memory allocated: 10.2 GB, max memory allocated: 10.3 GB\n",
    "GPU 1 memory allocated: 13.1 GB, max memory allocated: 13.2 GB\n",
    "\n",
    "# device_map=create_shared_device_map(16)\n",
    "Execution Time : 2.7 s, tokens per second: 9.6\n",
    " In the heart of Spain, where the passion runs deep,\n",
    "Lies a team called Real Madrid, a treasure to keep\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 11.8 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 11.8 GB\n",
    "Execution Time : 10.7 s, tokens per second: 9.4\n",
    "\n",
    "# device_map=create_intertwined_device_map(),\n",
    "Execution Time : 3.1 s, tokens per second: 8.5\n",
    " In the heart of Spain, where the passion runs deep,\n",
    "Lies a team called Real Madrid, a treasure to keep\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 11.8 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 11.8 GB\n",
    "Execution Time : 10.2 s, tokens per second: 9.9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/bible.txt', 'r') as f:\n",
    "    bible = f.read()\n",
    "print(bible[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens, max_memory, inference_time = [], [], []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    text = bible[:int(i*4000)]\n",
    "    input_tokens.append(len(tokenizer.tokenize(text)))\n",
    "    print(f'Input tokens: {input_tokens[-1]}')\n",
    "    t0 = time.time()\n",
    "    chat_with_mixtral(text, max_new_tokens=25)\n",
    "    inference_time.append(time.time() - t0)\n",
    "    print_gpu_memory()\n",
    "    max_memory.append(sum(torch.cuda.max_memory_allocated(device) for device in range(torch.cuda.device_count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# device_map=create_shared_device_map(16)\n",
    "Input tokens: 1129\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 3.6 s, tokens per second: 7.3\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 12.0 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 11.9 GB\n",
    "Input tokens: 2320\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 3.9 s, tokens per second: 6.7\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 12.3 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 12.2 GB\n",
    "Input tokens: 3543\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 4.4 s, tokens per second: 5.9\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 12.6 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 12.5 GB\n",
    "Input tokens: 4830\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 5.1 s, tokens per second: 5.1\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 12.9 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 12.9 GB\n",
    "Input tokens: 6046\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 5.8 s, tokens per second: 4.5\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 13.2 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 13.2 GB\n",
    "Input tokens: 7202\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 6.4 s, tokens per second: 4.1\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 13.5 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 13.4 GB\n",
    "Input tokens: 8389\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 7.0 s, tokens per second: 3.7\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 13.8 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 13.7 GB\n",
    "Input tokens: 9743\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 8.0 s, tokens per second: 3.3\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 14.2 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 14.1 GB\n",
    "Input tokens: 11073\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 8.7 s, tokens per second: 3.0\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 14.5 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 14.4 GB\n",
    "\n",
    "\n",
    "# device_map=create_intertwined_device_map(),\n",
    "Input tokens: 1129\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 3.9 s, tokens per second: 6.7\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 11.9 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 12.0 GB\n",
    "Input tokens: 2320\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 4.2 s, tokens per second: 6.2\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 12.2 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 12.3 GB\n",
    "Input tokens: 3543\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 4.7 s, tokens per second: 5.5\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 12.5 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 12.6 GB\n",
    "Input tokens: 4830\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 5.3 s, tokens per second: 4.9\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 12.9 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 12.9 GB\n",
    "Input tokens: 6046\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 6.0 s, tokens per second: 4.3\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 13.2 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 13.2 GB\n",
    "Input tokens: 7202\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 6.7 s, tokens per second: 3.9\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 13.4 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 13.5 GB\n",
    "Input tokens: 8389\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 7.4 s, tokens per second: 3.5\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 13.7 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 13.8 GB\n",
    "Input tokens: 9743\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 8.2 s, tokens per second: 3.2\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 14.1 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 14.2 GB\n",
    "Input tokens: 11073\n",
    "Formatting the prompt to Mixtral needs.\n",
    "Execution Time : 9.0 s, tokens per second: 2.9\n",
    "GPU 0 memory allocated: 11.7 GB, max memory allocated: 14.4 GB\n",
    "GPU 1 memory allocated: 11.7 GB, max memory allocated: 14.5 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods use the same memory, but the intertwined device map is slower. This suggests that the bottleneck of passing information througth the GPUs has a bigger weight than the more even GPU utilization (and likely lower temperature)\n",
    "\n",
    "However on Kaggle the result was the opposite, so maybe I'm just observing random variations on inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm loading a Mixtral model on two gpus using the following code:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,)\n",
    "```\n",
    "\n",
    "The problem is that the use of GPU memory is not balanced:\n",
    "\n",
    "- GPU 0 memory allocated: 10.2 GB, max memory allocated: 10.3 GB\n",
    "- GPU 1 memory allocated: 13.1 GB, max memory allocated: 13.2 GB\n",
    "\n",
    "How could I have a more even use of GPU memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Does using `gradient_checkpointing=True` improve memory usage? I don't believe that since I'm doing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
