{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore public datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a tool to explore public data and see if it can be useful for fine-tuning models.\n",
    "\n",
    "I want to know:\n",
    "\n",
    "- Text length: how big are the texts in the dataset, measured in tokens\n",
    "- Unique prompts: How many unique prompts each dataset has\n",
    "- Prompt sampling: Inspect how random prompts from the dataset look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/Mistral-7B-Instruct-v0.2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_length_distribution(df):\n",
    "    df['all_text'] = df['original_text'] + df['rewritten_text'] + df['rewrite_prompt']\n",
    "    df['text_length'] = df['all_text'].apply(lambda x: len(tokenizer.tokenize(str(x))))\n",
    "    plt.hist(df['text_length'].values, bins=np.linspace(0, 4000, 100))\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(filepath):\n",
    "    name = os.path.basename(filepath).split('.')[0]\n",
    "    folder = os.path.basename(os.path.dirname(filepath))\n",
    "    if folder != 'data':\n",
    "        name = f'{folder}/{name}'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_summary_table(datasets, dataset_names):\n",
    "    rows = []\n",
    "    for dataset in datasets:\n",
    "        n_prompts = len(dataset['rewrite_prompt'].unique())\n",
    "        rows.append(dict(\n",
    "                         n=len(dataset),\n",
    "                         n_prompts=n_prompts,\n",
    "                         ratio=round(n_prompts/len(dataset), 2),\n",
    "                         median_tokens=int(dataset['text_length'].median()),\n",
    "                         ))\n",
    "    return pd.DataFrame(rows, index=dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_prompts(dataset, n=5, random_seed=7):\n",
    "    unique_prompts = dataset['rewrite_prompt'].apply(lambda x: str(x).strip()).unique()\n",
    "    if n > len(unique_prompts):\n",
    "        return unique_prompts\n",
    "    np.random.seed(random_seed)\n",
    "    return sorted(np.random.choice(unique_prompts, n, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepaths = sorted(glob.glob('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/*_curated.csv'))\n",
    "dataset_filepaths += sorted(glob.glob('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/*/*_curated.csv'))\n",
    "dataset_names = [get_dataset_name(filepath) for filepath in dataset_filepaths]\n",
    "datasets = [pd.read_csv(filepath) for filepath in tqdm(dataset_filepaths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(dataset_filepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(dataset_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, dataset in zip(dataset_names, datasets):\n",
    "    text_length_distribution(dataset)\n",
    "    title = f'Token length distribution of {dataset_name} (n={len(dataset)})'\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique prompts vs dataset lenght, maybe a table with that data and also median token length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_datasets_summary_table(datasets, dataset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 9 trainings, 2 days of submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "for dataset_name, dataset in zip(dataset_names, datasets):\n",
    "    prompts = sample_prompts(dataset, n=n)\n",
    "    print(f'\\n\\n\\t\\t{dataset_name} ({len(dataset)} samples)\\n')\n",
    "    print('\\n'.join(prompts))\n",
    "    print('\\n' + '*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(df.sample(40).rewrite_prompt.values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
