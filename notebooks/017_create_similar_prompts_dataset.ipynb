{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create similar prompts datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPT4 API to create a diverse and high quality dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from prometeo.evaluation import get_sharpened_cosine_similarity, estimate_mean\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt4(prompt, temperature=0.7):\n",
    "    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'], organization=os.environ['OPENAI_API_ORG'])\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_progress(submits):\n",
    "    progress = 0\n",
    "    with tqdm(total=len(submits), smoothing=0) as progress_bar:\n",
    "        while 1:\n",
    "            time.sleep(1)\n",
    "            current_progress = np.sum([submit.done() for submit in submits])\n",
    "            if current_progress > progress:\n",
    "                progress_bar.update(current_progress - progress)\n",
    "                progress = current_progress\n",
    "            if progress == len(submits):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create superdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a first step let's concat all the training datasets in a single one for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_imitation_of_leaked_v1.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v2.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v3.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v4_with_hints.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v5_gpt4.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_multi_instruction_v1.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4_v2.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv',\n",
    "]\n",
    "df = pd.concat([pd.read_csv(filepath) for filepath in filepaths], ignore_index=True)\n",
    "df = df[['original_text', 'rewritten_text', 'rewrite_prompt']]\n",
    "print(df.shape)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/super_dataset_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/super_dataset_v1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_prompts = df['rewrite_prompt'].unique()\n",
    "print(len(unique_prompts), len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some prompts are repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Your task is to rewrite the following text prompt while preserving the meaning.\n",
    "You can reword the text, use synonyms, or change the structure of the sentences.\n",
    "Please provide 10 different rewrites using this json format:\n",
    "{\"rewrite_1\": \"...\", \"rewrite_2\": \"...\", ...., \"rewrite_10\": \"...\"}\n",
    "\n",
    "```text\n",
    "PLACEHOLDER\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have made a first naive generation with 10 prompts and it has taken 90 seconds, that means that it would take 3.5 hours to generate prompts for the whole dataset. But at the same time I know that there is a limit of 500RPM for gpt-4-turbo, so ideally I could do the task in 2 minutes. I have to parallelize the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for unique_prompt in tqdm(unique_prompts[:10]):\n",
    "    formatted_prompt = prompt_template.replace('PLACEHOLDER', unique_prompt)\n",
    "    print(formatted_prompt)\n",
    "    responses.append(chat_with_gpt4(formatted_prompt))\n",
    "    print(responses[-1])\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor(max_workers=40) as pool:\n",
    "    submits = []\n",
    "    for unique_prompt in tqdm(unique_prompts, desc='Creating submits'):\n",
    "        formatted_prompt = prompt_template.replace('PLACEHOLDER', unique_prompt)\n",
    "        submits.append(pool.submit(chat_with_gpt4, formatted_prompt))\n",
    "    monitor_progress(submits)\n",
    "    results = [submit.result() for submit in submits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In just 5 minutes the work is done! I could have probably done it faster using 80 workers but I didn't want to risk hitting the limit. This is already pretty fast.\n",
    "\n",
    "The cost of generating this new prompts was 11$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_results = [json.loads(result) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([len(parsed_result) for parsed_result in parsed_results], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results seem to have 10 prompts as required, that is very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_variations = dict(zip(unique_prompts, parsed_results))\n",
    "with open('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/prompt_to_variations.json', 'w') as f:\n",
    "    json.dump(prompt_to_variations, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure similarity with T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the similarity of the prompts using T5 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_similarity = dict()\n",
    "for prompt, variations in tqdm(prompt_to_variations.items(), total=len(prompt_to_variations)):\n",
    "    keys = list(variations.keys())\n",
    "    prompt_variations = [variations[key] for key in keys]\n",
    "    similarity = get_sharpened_cosine_similarity([prompt], prompt_variations)\n",
    "    prompt_to_similarity[prompt] = dict(zip(keys, similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize an histogram with the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_values = np.concatenate([list(similarity.values()) for similarity in prompt_to_similarity.values()])\n",
    "plt.hist(similarity_values, bins=100);\n",
    "plt.title('Distribution of SCS between prompts and their variations')\n",
    "plt.xlabel('Sharpened cosine similarity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(similarity_values > 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is pretty good, 85% of the prompt variations score above `0.8`.\n",
    "I believe that is a good threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.8\n",
    "prompt_to_variations_filtered = dict()\n",
    "for prompt, similarity in prompt_to_similarity.items():\n",
    "    keys = list(similarity.keys())\n",
    "    values = list(similarity.values())\n",
    "    prompt_variations = [prompt_to_variations[prompt][key] for key, value in zip(keys, values) if value > similarity_threshold]\n",
    "    prompt_to_variations_filtered[prompt] = prompt_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(variations) for variations in prompt_to_variations_filtered.values()], bins=np.arange(0.5, 11));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([len(variations) for variations in prompt_to_variations_filtered.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the superdataset v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_dataset = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    prompt = row['rewrite_prompt']\n",
    "    variations = prompt_to_variations_filtered[prompt]\n",
    "    for variation in variations:\n",
    "        super_dataset.append([row['original_text'], row['rewritten_text'], variation])\n",
    "# convert to pandas dataframe\n",
    "super_df = pd.DataFrame(super_dataset, columns=['original_text', 'rewritten_text', 'rewrite_prompt'])\n",
    "print(super_df.shape)\n",
    "super_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_df.sample(frac=1).to_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/super_dataset_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
