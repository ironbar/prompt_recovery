{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT4 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPT4 API to guess the prompts of the leaked dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from prometeo.evaluation import get_sharpened_cosine_similarity, estimate_mean\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'], organization=os.environ['OPENAI_API_ORG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_gpt4(prompt, temperature=0):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "## Output format\n",
    "\n",
    "Let's do the task step by step:\n",
    "\n",
    "1. On a first step analyze the differences of the texts in less than 30 words.\n",
    "2. On a second step write the most likely prompt using json format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prompt(response):\n",
    "    if '{' not in response:\n",
    "        print(f'Response does not contain json: {response}')\n",
    "        return response\n",
    "\n",
    "    json_text = '{' + response.split('{')[1].split('}')[0] + '}'\n",
    "    data = eval(json_text)\n",
    "    if isinstance(data, dict):\n",
    "        return list(data.values())[0]\n",
    "    elif isinstance(data, set):\n",
    "        return data.pop()\n",
    "    raise ValueError(f'Could not parse prompt from {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_response(response, prompt):\n",
    "    normalized_response = response.split('{')[0]\n",
    "    normalized_response += '{\"prompt\": \"' + prompt + '\"}'\n",
    "    normalized_response = normalized_response.replace('\\n\\n', '\\n')\n",
    "    return normalized_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling to GPT4 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n",
    "- https://platform.openai.com/docs/api-reference/chat/create?lang=python\n",
    "- https://platform.openai.com/docs/api-reference/introduction\n",
    "\n",
    "The most recent model is `gpt-4-0125-preview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tqdm(zip(df['original_text'].values, df['rewritten_text'].values), total=len(df))\n",
    "responses = []\n",
    "for original_text, rewritten_text in iterator:\n",
    "    prompt = prompt_template.format(original_text=original_text, rewritten_text=rewritten_text)\n",
    "    responses.append(chat_with_gpt4(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 20 minutes to compute all the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gpt4_prompt'] = responses\n",
    "df.to_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocess the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [parse_prompt(response) for response in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4 works very well and I had no problem in parsing all the responses.\n",
    "\n",
    "We might want to train later with the data, so let's normalize the responses to use always the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_responses = [normalize_response(response, prompt) for response, prompt in zip(responses, prompts)]\n",
    "df['gpt4_response'] = responses\n",
    "df['gpt4_prompt'] = prompts\n",
    "df['gpt4_normalized_response'] = normalized_responses\n",
    "df.to_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the number of tokens of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv')\n",
    "plt.hist([len(tokenizer.tokenize(response)) for response in df.gpt4_normalized_response], bins=50);\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.title('Distribution of GPT4 responses token length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean number of tokens is around 50, and the max is below 75. Thus we should be able to generate text of that lenght on submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate GPT4 on rewritten supplementary material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute GPT4 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tqdm(zip(df['original_text'].values, df['rewritten_text'].values), total=len(df))\n",
    "responses = []\n",
    "for original_text, rewritten_text in iterator:\n",
    "    prompt = prompt_template.format(original_text=original_text, rewritten_text=rewritten_text)\n",
    "    responses.append(chat_with_gpt4(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [parse_prompt(response) for response in responses]\n",
    "normalized_responses = [normalize_response(response, prompt) for response, prompt in zip(responses, prompts)]\n",
    "df['gpt4_response'] = responses\n",
    "df['gpt4_prompt'] = prompts\n",
    "df['gpt4_normalized_response'] = normalized_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the responses did not have json format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scs = get_sharpened_cosine_similarity(df['rewrite_prompt'].values, df['gpt4_prompt'].values)\n",
    "estimate_mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scs'] = scs\n",
    "df.to_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4 only scores 0.7036 +- 0.014 on the rewritten supplementary material. My current best score is 0.724\n",
    "\n",
    "However if I look at GPT4 \"errors\" they are not errors at all:\n",
    "\n",
    "- Many times the prompt given by GPT4 is better than the original prompt, the problem is that Gemma does not follow the given instruction\n",
    "- The other big problem is that Gemma is not an [injective function](https://en.wikipedia.org/wiki/Injective_function), many different prompts can lead to the same answer. How to choose among the space of possible prompts? We could inject some\n",
    "bias if we know the style of the host.\n",
    "\n",
    "I judge that GPT4 answers are very good. I believe that a combination of GPT4 analisys and Gemma could lead to a very strong model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the similarity I get if I compare my best model with GPT4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/evaluations/mixtral_prompt_engineering/e1bcc54b4fcff31eeafed9bc0a0933a0a8d7b3c1955c55baf3d57c045bb433b3.csv')\n",
    "print(estimate_mean(mixtral['sharpened_cosine_similarity'].values))\n",
    "mixtral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scs = get_sharpened_cosine_similarity(mixtral['predicted_prompt'].values, df['gpt4_prompt'].values)\n",
    "estimate_mean(scs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score rises to 0.755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
