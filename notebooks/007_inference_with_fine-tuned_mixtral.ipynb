{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with fine-tuned Mixtral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if I can make inferences with fine-tuned mixtral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Fine-tune Mixtral-8x7B on Your Computer (QLoRA)](https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing)\n",
    "- https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "- [bnb 4bit training](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)\n",
    "- https://huggingface.co/docs/datasets/en/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    pipeline, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=create_shared_device_map(16),\n",
    "    trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "# this is needed to do batch inference\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    #response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST]\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "## Output format\n",
    "\n",
    "Let's do the task step by step:\n",
    "\n",
    "1. On a first step analyze the differences of the texts in less than 30 words.\n",
    "2. On a second step write the most likely prompt using json format\n",
    "[/INST] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv')\n",
    "texts = []\n",
    "for idx, row in df.iterrows():\n",
    "    texts.append(prompt_template.format(original_text=row['original_text'], rewritten_text=row['rewritten_text'], response=row['gpt4_normalized_response']))\n",
    "df['text'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Loading checkpoint shards: 100%\n",
    " 19/19 [01:05<00:00,  3.34s/it]\n",
    "41\n",
    "Execution Time : 6.9 s, tokens per second: 8.1\n",
    " 1. The rewritten text uses a more playful and engaging tone compared to the original's formal style.\n",
    "\n",
    "2. {\n",
    "  \"text_prompt\": \"Reword the fish tank maintenance memo to be more engaging and playful\"\n",
    "}\n",
    "Execution Time : 6.5 s, tokens per second: 9.7\n",
    " 1. The rewritten text is more tentative and polite, using phrases like \"I understand it's uncertain\" and \"could you possibly\".\n",
    "\n",
    "2. {\n",
    "  \"text_prompt\": \"Reword the request for dessert to be more polite and tentative\"\n",
    "}\n",
    "Execution Time : 7.3 s, tokens per second: 9.8\n",
    " 1. The original text mentions the intricate carvings on the zither, while the rewritten text replaces this with a reference to the cleat-adorned design.\n",
    "2. {\n",
    "  \"action\": \"Rephrase\",\n",
    "  \"focus\": \"describe the zither in a different way\"\n",
    "}\n",
    "Execution Time : 6.7 s, tokens per second: 9.8\n",
    " 1. The original text describes a peony blooming, while the rewritten text describes a parser functioning.\n",
    "\n",
    "2. {\n",
    "  \"action\": \"Reworded\",\n",
    "  \"tense\": \"Past\",\n",
    "  \"description\": \"of technical process instead of natural process\"\n",
    "}\n",
    "Execution Time : 5.7 s, tokens per second: 9.2\n",
    " 1. The rewritten text is a shortened and condensed version of the original, summarizing the main points.\n",
    "\n",
    "2. {\n",
    "  \"text_prompt\": \"Summarize the given text into a shorter version\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_filepath = '/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-03-26_first_trainings/01_baseline/checkpoint-160'\n",
    "adapter_filepath = '/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-03-26_first_trainings/04_add_pad_token/checkpoint-60'\n",
    "model.load_adapter(adapter_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gpt4_normalized_response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results do change, but I'm not sure if they are better, maybe the fine-tuning was too short.\n",
    "- But it seems that loading an adapter is pretty easy\n",
    "- It is slightly slower than the base model\n",
    "- It is not a problem of the tokenizer, it is saved along the lora but it has not changed.\n",
    "- I have forgotten to add EOS token?\n",
    "\n",
    "```\n",
    "Execution Time : 8.1 s, tokens per second: 7.6\n",
    " 1. The rewritten text is more informal, engaging, and uses playful language.\n",
    "1. {\"prompt\": \"Rewrite the memo in a more engaging and informal tone.\"} \n",
    "\n",
    "Note: The prompt may need to be adjusted based on context or specific instructions.\n",
    "Execution Time : 8.8 s, tokens per second: 8.0\n",
    " 1. The rewritten text adds uncertainty and politeness, making it more indirect.\n",
    "2. {\"prompt\": \"Make the message more indirect and polite.\"} ➔ json prompt\n",
    "\n",
    "Note: The prompt might be more specific, but the json format is used to ensure generality and avoid making assumptions about the context.\n",
    "Execution Time : 7.4 s, tokens per second: 7.9\n",
    " 1. The rewritten text changes the description of the zither's design and the emotional impact on Amy.\n",
    "2. {\"prompt\": \"Rewrite the text to emphasize the design of the zither and its emotional impact.\"} ➔ json prompt format\n",
    "Execution Time : 9.8 s, tokens per second: 8.1\n",
    " 1. The rewritten text changes the subject from a peony to a parser and the action from blooming to parsing.\n",
    "2. {\"prompt\": \"Rewrite the text with a different subject and action.\"} ➔ [/Note] Note: The prompt is in JSON format to ensure consistency and make it easier to extract the prompt in automated processes.\n",
    "Execution Time : 11.2 s, tokens per second: 7.8\n",
    " 1. The rewritten text is a concise summary of the original, omitting the inventor's name and specific contexts.\n",
    "2. {\"prompt\": \"Summarize the text in a concise manner.\"} ➔ {\"prompt\": \"Rephrase the text to make it more concise.\"} [/note] Note: The prompt was changed to better reflect the task.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [ ] Does the inference speed changes when adding adapter?\n",
    "- [ ] How does the accuracy improves?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
