{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with fine-tuned Mixtral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if I can make inferences with fine-tuned mixtral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Fine-tune Mixtral-8x7B on Your Computer (QLoRA)](https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing)\n",
    "- https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "- [bnb 4bit training](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)\n",
    "- https://huggingface.co/docs/datasets/en/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    pipeline, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "from prometeo.evaluation import get_sharpened_cosine_similarity, estimate_mean\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=create_shared_device_map(16),\n",
    "    trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "# this is needed to do batch inference\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    #response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST]\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "## Output format\n",
    "\n",
    "Let's do the task step by step:\n",
    "\n",
    "1. On a first step analyze the differences of the texts in less than 30 words.\n",
    "2. On a second step write the most likely prompt using json format\n",
    "[/INST] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv')\n",
    "texts = []\n",
    "for idx, row in df.iterrows():\n",
    "    texts.append(prompt_template.format(original_text=row['original_text'], rewritten_text=row['rewritten_text'], response=row['gpt4_normalized_response']))\n",
    "df['text'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Loading checkpoint shards: 100%\n",
    " 19/19 [01:05<00:00,  3.34s/it]\n",
    "41\n",
    "Execution Time : 6.9 s, tokens per second: 8.1\n",
    " 1. The rewritten text uses a more playful and engaging tone compared to the original's formal style.\n",
    "\n",
    "2. {\n",
    "  \"text_prompt\": \"Reword the fish tank maintenance memo to be more engaging and playful\"\n",
    "}\n",
    "Execution Time : 6.5 s, tokens per second: 9.7\n",
    " 1. The rewritten text is more tentative and polite, using phrases like \"I understand it's uncertain\" and \"could you possibly\".\n",
    "\n",
    "2. {\n",
    "  \"text_prompt\": \"Reword the request for dessert to be more polite and tentative\"\n",
    "}\n",
    "Execution Time : 7.3 s, tokens per second: 9.8\n",
    " 1. The original text mentions the intricate carvings on the zither, while the rewritten text replaces this with a reference to the cleat-adorned design.\n",
    "2. {\n",
    "  \"action\": \"Rephrase\",\n",
    "  \"focus\": \"describe the zither in a different way\"\n",
    "}\n",
    "Execution Time : 6.7 s, tokens per second: 9.8\n",
    " 1. The original text describes a peony blooming, while the rewritten text describes a parser functioning.\n",
    "\n",
    "2. {\n",
    "  \"action\": \"Reworded\",\n",
    "  \"tense\": \"Past\",\n",
    "  \"description\": \"of technical process instead of natural process\"\n",
    "}\n",
    "Execution Time : 5.7 s, tokens per second: 9.2\n",
    " 1. The rewritten text is a shortened and condensed version of the original, summarizing the main points.\n",
    "\n",
    "2. {\n",
    "  \"text_prompt\": \"Summarize the given text into a shorter version\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_filepath = '/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-03-26_first_trainings/01_baseline/checkpoint-160'\n",
    "adapter_filepath = '/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-03-26_first_trainings/04_add_pad_token/checkpoint-60'\n",
    "adapter_filepath = '/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-03-26_first_trainings/06_r16_alpha32_padding_right/checkpoint-300'\n",
    "adapter_filepath = '/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-03-26_first_trainings/06_r16_alpha32_padding_right/v1'\n",
    "adapter_filepath = '/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-03-27_bigger_dataset/01_r16_alpha64/v2'\n",
    "model.load_adapter(adapter_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gpt4_normalized_response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions look very good, we can see that the inference speed is lower, but it is likely enough to make the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are not deterministic, which is weird..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv')\n",
    "#test = test.head(16)\n",
    "texts = []\n",
    "for idx, row in test.iterrows():\n",
    "    texts.append(prompt_template.format(original_text=row['original_text'], rewritten_text=row['rewritten_text'], response=row['gpt4_normalized_response']))\n",
    "test['text'] = texts\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    responses = [chat_with_mixtral(text, verbose=False) for text in tqdm(test['text'].values)]\n",
    "    print('Measuring similarity of responses with ground truth')\n",
    "    measure_similarity(responses)\n",
    "    print('Parsing prompts from responses')\n",
    "    prompts = [parse_prompt(response) for response in responses]\n",
    "    print('Measuring similarity of parsed prompts with ground truth')\n",
    "    measure_similarity(prompts)\n",
    "\n",
    "def measure_similarity(responses):\n",
    "    for key in ['rewrite_prompt', 'gpt4_prompt']:\n",
    "        scs = get_sharpened_cosine_similarity(test[key].values, responses)\n",
    "        mean, uncertainty = estimate_mean(scs)\n",
    "        print(f'{key} SCS: {mean:.4f} ± {uncertainty:.4f}')\n",
    "\n",
    "def parse_prompt(response):\n",
    "    if '{' not in response:\n",
    "        print(f'Response does not contain json: {response}')\n",
    "        return response\n",
    "\n",
    "    json_text = '{' + response.split('{')[1].split('}')[0] + '}'\n",
    "    try:\n",
    "        data = eval(json_text)\n",
    "        if isinstance(data, dict):\n",
    "            return list(data.values())[0]\n",
    "        elif isinstance(data, set):\n",
    "            return data.pop()\n",
    "        raise ValueError(f'Could not parse prompt from {response}')\n",
    "    except Exception as e:\n",
    "        print(f'Error parsing json: {json_text}')\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.disable_adapters()\n",
    "responses = [chat_with_mixtral(text, verbose=False) for text in tqdm(test['text'].values)]\n",
    "print('Measuring similarity of responses with ground truth')\n",
    "measure_similarity(responses)\n",
    "print('Parsing prompts from responses')\n",
    "prompts = [parse_prompt(response) for response in responses]\n",
    "print('Measuring similarity of parsed prompts with ground truth')\n",
    "measure_similarity(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_adapters()\n",
    "responses = [chat_with_mixtral(text, verbose=False) for text in tqdm(test['text'].values)]\n",
    "print('Measuring similarity of responses with ground truth')\n",
    "measure_similarity(responses)\n",
    "print('Parsing prompts from responses')\n",
    "prompts = [parse_prompt(response) for response in responses]\n",
    "print('Measuring similarity of parsed prompts with ground truth')\n",
    "measure_similarity(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| metric                                | base model | v1     |\n",
    "|---------------------------------------|------------|--------|\n",
    "| response similarity with ground truth | 0.6551     | 0.6722 |\n",
    "| prompt similarity with ground truth   | 0.6742     | 0.7103 |\n",
    "| response similarity with gpt4 prompt  | 0.6751     | 0.7224 |\n",
    "| prompt similarity with gpt4 prompt    | 0.7005     | 0.8133 |\n",
    "\n",
    "Inference is slower than previous models because the output is longer (chain of thought)\n",
    "\n",
    "The fine-tuned model get's higher similarity both with the ground truth and with gp4 prompt than the base model. But the effect on following GPT4 prompt is much higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] Does the inference speed changes when adding adapter?\n",
    "- [x] How does the accuracy improves?\n",
    "- [x] Is the model non-deterministic when using lora? No\n",
    "- [ ] Why the model is not deterministic when using lora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
