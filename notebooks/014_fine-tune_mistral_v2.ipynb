{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Mistral v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Mistral with different datasets to see what is the best training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import yaml\n",
    "import os\n",
    "import hashlib\n",
    "import glob\n",
    "\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    pipeline, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_path = {\n",
    "    'mistral-7b': '/home/gbarbadillo/data/Mistral-7B-Instruct-v0.2',\n",
    "    'mixtral-8x7b': '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf',\n",
    "    'llama-13b': '/home/gbarbadillo/data/llama2-13b-chat-hf',\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    logging.info(f'Loading {model_name}...')\n",
    "    model_path = model_to_path[model_name]\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit= True,\n",
    "        bnb_4bit_quant_type= \"nf4\",\n",
    "        bnb_4bit_compute_dtype= torch.float16,\n",
    "        bnb_4bit_use_double_quant= True,\n",
    "        llm_int8_enable_fp32_cpu_offload= True,\n",
    "        llm_int8_skip_modules=['gate', 'lm_head'],\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=get_device_map(model_name),\n",
    "        trust_remote_code=True,)\n",
    "    if model_name == 'llama-13b':\n",
    "        # quirks to update the model default configuration\n",
    "        model.config.pretraining_tp = 1 # otherwise the quantized model does not work\n",
    "        model.generation_config.temperature = None\n",
    "        model.generation_config.top_p = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True)\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "    tokenizer.padding_side = 'right' # by default is left, for training right seems to be better\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    gc.collect()\n",
    "    print_gpu_memory()\n",
    "    return model, tokenizer\n",
    "\n",
    "def empty_gpu_vram():\n",
    "    logging.info('Emptying GPU VRAM...')\n",
    "    global model, tokenizer, pipe, trainer\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del pipe\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory()\n",
    "\n",
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')\n",
    "\n",
    "\n",
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map\n",
    "\n",
    "\n",
    "def get_device_map(model_name):\n",
    "    if model_name == 'mixtral-8x7b':\n",
    "        return create_intertwined_device_map()\n",
    "    else:\n",
    "        return 'auto'\n",
    "\n",
    "assert get_device_map('foo') == 'auto'\n",
    "assert get_device_map('mixtral-8x7b') != 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    #response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST][prompt-recovery]\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, Update etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST] The most likely text prompt given to transform the original text into the rewritten text was: {response} </s>\"\"\"\n",
    "response_template = \"The most likely text prompt given to transform the original text into the rewritten text was:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there is a less constrained prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST][prompt-recovery]\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST] The most likely text prompt given to transform the original text into the rewritten text was: {response} </s>\"\"\"\n",
    "response_template = \"The most likely text prompt given to transform the original text into the rewritten text was:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_training(filepath, target_col='rewrite_prompt'):\n",
    "    df = pd.read_csv(filepath)\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        texts.append(prompt_template.format(original_text=row['original_text'],\n",
    "                                            rewritten_text=row['rewritten_text'],\n",
    "                                            response=row[target_col]))\n",
    "    df['text'] = texts\n",
    "    df['n_tokens'] = df['text'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_too_long_samples(train_df, eval_df, max_seq_length, safe_margin = 10):\n",
    "    logging.info(f'Filtering samples with more than {max_seq_length} tokens.')\n",
    "    print(f'There were {len(train_df)} samples for training and {len(eval_df)} samples for evaluation.')\n",
    "    train_df = train_df[train_df['n_tokens'] <= max_seq_length - safe_margin]\n",
    "    eval_df = eval_df[eval_df['n_tokens'] <= max_seq_length - safe_margin]\n",
    "    print(f'There are {len(train_df)} samples for training and {len(eval_df)} samples for evaluation.')\n",
    "    return train_df, eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_for_training(model, tokenizer):\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_datasets(model_name, dataset_filepaths, experiment_name, peft_config, max_seq_length=640):\n",
    "    global model, tokenizer, pipe, trainer\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    train_df = pd.concat([prepare_dataframe_for_training(filepath) for filepath in dataset_filepaths])\n",
    "    eval_df = prepare_dataframe_for_training('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v1.csv')\n",
    "    train_df, eval_df = filter_too_long_samples(train_df, eval_df, max_seq_length)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    eval_dataset = Dataset.from_pandas(eval_df)\n",
    "    print(f'Train sample:\\n{train_df.text.values[0]}')\n",
    "    print(f'Test sample:\\n{eval_df.text.values[0]}')\n",
    "    logging.info('Making a few sample inferences')\n",
    "    for text in train_df['text'].values[:5]:\n",
    "        print(chat_with_mixtral(text.split(response_template)[0] + response_template))\n",
    "\n",
    "    prepare_model_for_training(model, tokenizer)\n",
    "    logging_steps = 25\n",
    "    training_arguments = TrainingArguments(\n",
    "            output_dir=f\"/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-04-11_datacentric_mistral/{experiment_name}\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            do_eval=True,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            per_device_train_batch_size=16, # 4-16 should be fine for lora.\n",
    "            gradient_accumulation_steps=1,\n",
    "            per_device_eval_batch_size=16,\n",
    "            log_level=\"debug\",\n",
    "            save_steps=logging_steps, #50,\n",
    "            logging_steps=logging_steps//4, #50,\n",
    "            learning_rate=2e-5, # maybe we can increase this\n",
    "            eval_steps=logging_steps, #50,\n",
    "            max_steps=250, #300,\n",
    "            warmup_steps=30,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "    )\n",
    "    # https://docs.wandb.ai/guides/track/environment-variables\n",
    "    # https://stackoverflow.com/questions/71106179/log-two-model-runs-with-keras-wandb\n",
    "    os.environ['WANDB_PROJECT'] = 'datacentric_mistral'\n",
    "    os.environ['WANDB_NAME'] = experiment_name\n",
    "    w = wandb.init(reinit=True, project='datacentric_mistral', name=experiment_name)\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, response_template=response_template)\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        data_collator=data_collator,\n",
    "        args=training_arguments,\n",
    "    )\n",
    "    trainer.train()\n",
    "    w.finish()\n",
    "\n",
    "    logging.info('Making a few sample inferences')\n",
    "    for text in train_df['text'].values[:5]:\n",
    "        print(chat_with_mixtral(text.split(response_template)[0] + response_template))\n",
    "    empty_gpu_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    # lora_alpha: LoRA scaling factor.\n",
    "    lora_alpha=64, #64,\n",
    "    lora_dropout=0.1, # 0.1, althought Vaca suggested to use 0.05 for big models\n",
    "    # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    r=1, #16\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    # lora_alpha: LoRA scaling factor.\n",
    "    lora_alpha=64, #64,\n",
    "    lora_dropout=0.1, # 0.1, althought Vaca suggested to use 0.05 for big models\n",
    "    # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    r=16, #16\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/dipamc77_prompts_0_500_wiki_first_para_3000_curated.csv'],\n",
    "    'dipamc77',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated.csv'],\n",
    "    'gemma_suppl_rewrite_debug_3',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v1_curated.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/nbroad-v2_curated.csv'],\n",
    "    'nbroad',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/aishaalmahmoud/llm_dataset_1_curated.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/aishaalmahmoud/llm_dataset_20k_curated.csv'],\n",
    "    'aishaalmahmoud',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/winddude_70k_gemma_template_built_curated.csv'],\n",
    "    'winddude',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/alexxxsem/data_gemma_0_1000_curated.csv'],\n",
    "    'alexxxsem',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/galileo/gemma_v1_7b-it_curated.csv'],\n",
    "    'galileo',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/newtonbaba/gemini_data_set_prompt_recover_3_curated.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/newtonbaba/gemma_data_set_prompt_recover_1_curated.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/newtonbaba/gemma_data_set_prompt_recover_2_curated.csv'],\n",
    "    'newtonbaba',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://wandb.ai/guillermobarbadillo/datacentric_mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On 500 steps with batch size 16, a total of 8k samples are seen.\n",
    "- Each train should take around 2 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More experiments with my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_imitation_of_leaked_v1.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v2.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv',\n",
    "     ],\n",
    "    'add_imitation_of_leaked_v1',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v4_with_hints.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v2.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv',\n",
    "     ],\n",
    "    'add_hints',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_multi_instruction_v1.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v2.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv',\n",
    "     ],\n",
    "    'add_multi_instructions',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    ['/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v5_gpt4.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v2.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv',\n",
    "     '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv',\n",
    "     ],\n",
    "    'add_new_prompts',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_imitation_of_leaked_v1.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v2.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v3.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v4_with_hints.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v5_gpt4.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_multi_instruction_v1.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4_v2.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv',\n",
    " ]\n",
    "train_model_on_datasets(\n",
    "    'mistral-7b',\n",
    "    filepaths,\n",
    "    'all_new_data_together',\n",
    "    peft_config,\n",
    "    max_seq_length=640,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] Verify that I can clean the memory at the end of the training\n",
    "- [x] Weird message about too long texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
