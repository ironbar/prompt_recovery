{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Mixtral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn to fine-tune Mixtral and make predictions with fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Fine-tune Mixtral-8x7B on Your Computer (QLoRA)](https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing)\n",
    "- https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "- [bnb 4bit training](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)\n",
    "- https://huggingface.co/docs/datasets/en/index\n",
    "- https://huggingface.co/docs/transformers/en/peft\n",
    "- https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\n",
    "- https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload\n",
    "- [Taller + AMA: Entrenamiento de LLMs, Alejandro Vaca](https://www.youtube.com/watch?v=458UWBlBdtI&t=3494s)\n",
    "- https://github.com/somosnlp/recursos/blob/main/hackathon_2024/entrenamiento_llm_instrucciones.ipynb\n",
    "- https://gathnex.medium.com/mistral-7b-fine-tuning-a-step-by-step-guide-52122cdbeca8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LoRA does not add any inference latency because adapter weights can be merged with the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    pipeline, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=create_shared_device_map(16),\n",
    "    trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this solve the problem of not ending the generations\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "# this is needed to do batch inference\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "tokenizer.padding_side = 'right' # by default is left, for training right seems to be better\n",
    "gc.collect()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    #response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST]\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "## Output format\n",
    "\n",
    "Let's do the task step by step:\n",
    "\n",
    "1. On a first step analyze the differences of the texts in less than 30 words.\n",
    "2. On a second step write the most likely prompt using json format\n",
    "[/INST] {response} </s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_training(filepath, target_col='gpt4_normalized_response'):\n",
    "    df = pd.read_csv(filepath)\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        texts.append(prompt_template.format(original_text=row['original_text'],\n",
    "                                            rewritten_text=row['rewritten_text'],\n",
    "                                            response=row[target_col]))\n",
    "    df['text'] = texts\n",
    "    df['n_tokens'] = df['text'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_dataframe_for_training('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4.csv')\n",
    "eval_df = prepare_dataframe_for_training('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_indices = [164, 181, 235]\n",
    "print(train_df.loc[bad_indices].rewritten_text.values)\n",
    "train_df = train_df.drop(bad_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_df['n_tokens'], bins=20, alpha=0.5, label='train')\n",
    "plt.hist(eval_df['n_tokens'], bins=20, alpha=0.5, label='eval')\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.title('Token distribution of the texts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could train with just 512 tokens of context if I remove ~18% of the data, seems like a good deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['n_tokens'] <= 512]\n",
    "eval_df = eval_df[eval_df['n_tokens'] <= 512]\n",
    "print(f'There are {len(train_df)} samples for training and {len(eval_df)} samples for evaluation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first experiments let's simply split the data in two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in train_df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text.split('[/INST]')[0] + '[/INST]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in eval_df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text.split('[/INST]')[0] + '[/INST]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    # lora_alpha: LoRA scaling factor.\n",
    "    lora_alpha=64, #64,\n",
    "    lora_dropout=0.1, # 0.1, althought Vaca suggested to use 0.05 for big models\n",
    "    # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "    r=16, #16\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-03-27_bigger_dataset/01_r16_alpha64\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        per_device_train_batch_size=8, # 4-16 should be fine for lora.\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=8,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=50, #50,\n",
    "        logging_steps=50, #50,\n",
    "        learning_rate=2e-5, # maybe we can increase this\n",
    "        eval_steps=50, #50,\n",
    "        max_steps=1000, #300,\n",
    "        warmup_steps=30,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First train with default parameters\n",
    "\n",
    "- I have registered to Weights & Biases because it has a free personal account.\n",
    "- The model is training and it is expected to take 3.5 hours for 300 steps, each GPU is using 17GB of memory.\n",
    "- The batch size is 16 with the initial configuration\n",
    "\n",
    "Second train with updated parameters (50 epochs, 1024 max_seq_length)\n",
    "\n",
    "- Now the model is using 23 and 24 GB of GPU memory\n",
    "- Training speed was halved due to duplicating the max_seq_length, this implies that if I filter and train only on short samples I could train much faster.\n",
    "- The model has been trained succesfully for 50 steps in less than one hour\n",
    "- https://wandb.ai/guillermobarbadillo/huggingface/runs/u6az7ac1\n",
    "\n",
    "Third train: limit the max seq lenght again to 512 by removing part of the dataset.\n",
    "\n",
    "- https://wandb.ai/guillermobarbadillo/huggingface/runs/eqx2bm11\n",
    "- The train is expected to take around 3 hours\n",
    "\n",
    "4 train. 02_r8_alpha32\n",
    "\n",
    "- Val loss is slightly higher\n",
    "- https://wandb.ai/guillermobarbadillo/huggingface/runs/kb27pqnb\n",
    "\n",
    "Add new pad token\n",
    "\n",
    "- Is it training faster? Yes, it has trained in 1h25m\n",
    "- https://wandb.ai/guillermobarbadillo/huggingface/runs/ywc27wou\n",
    "\n",
    "Final train with all the train data\n",
    "\n",
    "- Estimated train time is around 4 hours, I believe it will overfit earlier.\n",
    "- https://wandb.ai/guillermobarbadillo/huggingface/runs/jn7oiuqp?nw=nwuserguillermobarbadillo\n",
    "- With this setup the best validation loss is achieved around epoch 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a few inferences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in train_df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text.split('[/INST]')[0] + '[/INST] '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in eval_df['text'].values[:5]:\n",
    "    print(chat_with_mixtral(text.split('[/INST]')[0] + '[/INST] '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversation does not end correctly, that is weird.\n",
    "\n",
    "```\n",
    "Execution Time : 6.5 s, tokens per second: 5.7\n",
    " 1. The rewritten text uses more playful and engaging language.\n",
    "2. {prompt: Rewrite the memo with a more engaging tone.}  // json format\n",
    "Execution Time : 10.1 s, tokens per second: 6.4\n",
    " 1. The rewritten text is more tentative and polite, using possibly and uncertain.\n",
    "2. {prompt: Rewrite the message to be more polite and tentative.} \n",
    "\n",
    "Note: This prompt is inferred from the changes in the rewritten text and may not be explicitly stated.\n",
    "Execution Time : 10.1 s, tokens per second: 6.3\n",
    " 1. The rewritten text changes intricate carvings to cleat-adorned design and emphasizes balance and stability.\n",
    "2. {prompt: Rewrite the text to emphasize a different aspect.}  // hint: use Rewrite the text to for stylistic changes\n",
    "Execution Time : 9.8 s, tokens per second: 6.8\n",
    " 1. The rewritten text changes the subject from a peony to a parser and the action from blooming to parsing.\n",
    "2. {prompt: Rewrite the sentence with a different subject and action.}  OR  {prompt: Change the subject and action of the sentence.}  [generic prompt]\n",
    "Execution Time : 6.7 s, tokens per second: 6.3\n",
    " 1. The rewritten text condenses information, removing some details and making it more concise.\n",
    "2. {prompt: Summarize the information about the cornet.}  // json format\n",
    "```\n",
    "\n",
    "\n",
    "When using pad_token=unk_token it does end the conversation correctly:\n",
    "\n",
    "```\n",
    "Execution Time : 5.9 s, tokens per second: 7.4\n",
    " 1. The rewritten text uses a playful tone and metaphors related to fish and water.\n",
    "2. {\"prompt\": \"Rewrite the memo in a playful and engaging tone.\"}\n",
    "Execution Time : 5.0 s, tokens per second: 6.9\n",
    " 1. The rewritten text is more formal and uncertain.\n",
    "2. {\"prompt\": \"Rewrite the text to be more formal and uncertain.\"}\n",
    "Execution Time : 8.1 s, tokens per second: 7.8\n",
    " 1. The rewritten text removes the description of the zither's intricate carvings and replaces it with a reference to its cleat-adorned design.\n",
    "2. {\"prompt\": \"Rewrite the text to focus on the zither's design.\"}\n",
    "Execution Time : 6.1 s, tokens per second: 7.7\n",
    " 1. The original text is about a peony blooming, while the rewritten text is about a parser working efficiently.\n",
    "2. {\"prompt\": \"Rewrite the text to be about technology and efficiency.\"}\n",
    "Execution Time : 4.6 s, tokens per second: 7.4\n",
    " 1. The rewritten text is a shorter summary of the original text.\n",
    "2. {\"text prompt\": \"Rewrite the text as a short summary\"}\n",
    "```\n",
    "\n",
    "Using a new pad token also results in good generations.\n",
    "```\n",
    "Execution Time : 4.7 s, tokens per second: 7.3\n",
    " 1. The rewritten text is more formal and indirect.\n",
    "2. {\"prompt\": \"Rewrite the text in a formal and indirect way.\"}\n",
    "Execution Time : 5.4 s, tokens per second: 7.6\n",
    " 1. The original text describes a natural event, while the rewritten text describes a technical process.\n",
    "2. {\"prompt\": \"Rewrite the text to describe a technical process.\"}\n",
    "Execution Time : 5.5 s, tokens per second: 7.5\n",
    " 1. The rewritten text uses more abstract language and replaces negative words with positive ones.\n",
    "2. {\"prompt\": \"Rewrite the text using positive language and abstract concepts.\"}\n",
    "Execution Time : 7.0 s, tokens per second: 7.6\n",
    " 1. The rewritten text is more elaborate and descriptive, using metaphors and adjectives to create a vivid image.\n",
    "2. {\"prompt\": \"Rewrite the text to be more descriptive and imaginative.\"}\n",
    "Execution Time : 6.8 s, tokens per second: 7.7\n",
    " 1. The rewritten text focuses on the transformation of the mural and the captivation of passersby.\n",
    "2. {\"prompt\": \"Rewrite the text to focus on the mural's impact on the urban landscape.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] How to format the dataset? Apparently it is as easy as creating a dataset from a pandas dataframe\n",
    "- [x] Study token length distribution of the data, how it relates to the max_seq_length?\n",
    "- [x] Verify the predictions of the model before and after training\n",
    "- [x] How to save the lora model? The training saves checkpoints\n",
    "- [x] How to load the lora model? Done on a following notebook\n",
    "- [ ] rslora? https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#common-lora-parameters-in-peft\n",
    "- [ ] Reduce alpha, or reduce r as well\n",
    "- [x] What is the optimal train duration? Probably around 15 epochs\n",
    "- [ ] Try with Vaca's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
