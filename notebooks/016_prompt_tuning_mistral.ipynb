{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt tuning Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of fine-tuning the model, let's use prompt tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import yaml\n",
    "import os\n",
    "import hashlib\n",
    "import glob\n",
    "\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    pipeline, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_path = {\n",
    "    'mistral-7b': '/home/gbarbadillo/data/Mistral-7B-Instruct-v0.2',\n",
    "    'mixtral-8x7b': '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf',\n",
    "    'llama-13b': '/home/gbarbadillo/data/llama2-13b-chat-hf',\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    logging.info(f'Loading {model_name}...')\n",
    "    model_path = model_to_path[model_name]\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit= True,\n",
    "        bnb_4bit_quant_type= \"nf4\",\n",
    "        bnb_4bit_compute_dtype= torch.float16,\n",
    "        bnb_4bit_use_double_quant= True,\n",
    "        llm_int8_enable_fp32_cpu_offload= True,\n",
    "        llm_int8_skip_modules=['gate', 'lm_head'],\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=get_device_map(model_name),\n",
    "        trust_remote_code=True,)\n",
    "    if model_name == 'llama-13b':\n",
    "        # quirks to update the model default configuration\n",
    "        model.config.pretraining_tp = 1 # otherwise the quantized model does not work\n",
    "        model.generation_config.temperature = None\n",
    "        model.generation_config.top_p = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True)\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "    tokenizer.padding_side = 'right' # by default is left, for training right seems to be better\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    gc.collect()\n",
    "    print_gpu_memory()\n",
    "    return model, tokenizer\n",
    "\n",
    "def empty_gpu_vram():\n",
    "    logging.info('Emptying GPU VRAM...')\n",
    "    global model, tokenizer, pipe, trainer\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del pipe\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory()\n",
    "\n",
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')\n",
    "\n",
    "\n",
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer=16):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map\n",
    "\n",
    "\n",
    "def get_device_map(model_name):\n",
    "    if model_name == 'mixtral-8x7b':\n",
    "        return create_shared_device_map()\n",
    "    else:\n",
    "        return 'auto'\n",
    "\n",
    "assert get_device_map('foo') == 'auto'\n",
    "assert get_device_map('mixtral-8x7b') != 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    #response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST][prompt-recovery]\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, Update etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST] The most likely text prompt given to transform the original text into the rewritten text was: {response} </s>\"\"\"\n",
    "response_template = \"The most likely text prompt given to transform the original text into the rewritten text was:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there is a less constrained prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<s>[INST][prompt-recovery]\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST] The most likely text prompt given to transform the original text into the rewritten text was: {response} </s>\"\"\"\n",
    "response_template = \"The most likely text prompt given to transform the original text into the rewritten text was:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_training(filepath, target_col='rewrite_prompt'):\n",
    "    df = pd.read_csv(filepath)\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        texts.append(prompt_template.format(original_text=row['original_text'],\n",
    "                                            rewritten_text=row['rewritten_text'],\n",
    "                                            response=row[target_col]))\n",
    "    df['text'] = texts\n",
    "    df['n_tokens'] = df['text'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_too_long_samples(train_df, eval_df, max_seq_length, safe_margin = 10):\n",
    "    logging.info(f'Filtering samples with more than {max_seq_length} tokens.')\n",
    "    print(f'There were {len(train_df)} samples for training and {len(eval_df)} samples for evaluation.')\n",
    "    train_df = train_df[train_df['n_tokens'] <= max_seq_length - safe_margin]\n",
    "    eval_df = eval_df[eval_df['n_tokens'] <= max_seq_length - safe_margin]\n",
    "    print(f'There are {len(train_df)} samples for training and {len(eval_df)} samples for evaluation.')\n",
    "    return train_df, eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_for_training(model, tokenizer):\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.use_cache = False # Gradient checkpointing is used by default but not compatible with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_datasets(model_name, dataset_filepaths, experiment_name, peft_config,\n",
    "                            max_seq_length=640, learning_rate=2e-5, shuffle=True,\n",
    "                            batch_size=16, max_steps=250):\n",
    "    global model, tokenizer, pipe, trainer\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    train_df = pd.concat([prepare_dataframe_for_training(filepath) for filepath in dataset_filepaths])\n",
    "    eval_df = prepare_dataframe_for_training('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v1.csv')\n",
    "    train_df, eval_df = filter_too_long_samples(train_df, eval_df, max_seq_length)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    if shuffle:\n",
    "        logging.info('Shuffling the training dataset.')\n",
    "        train_dataset = train_dataset.shuffle(seed=42)\n",
    "    eval_dataset = Dataset.from_pandas(eval_df)\n",
    "    print(f'Train sample:\\n{train_df.text.values[0]}')\n",
    "    print(f'Test sample:\\n{eval_df.text.values[0]}')\n",
    "    logging.info('Making a few sample inferences')\n",
    "    for text in train_df['text'].values[:5]:\n",
    "        print(chat_with_mixtral(text.split(response_template)[0] + response_template))\n",
    "\n",
    "    prepare_model_for_training(model, tokenizer)\n",
    "    logging_steps = 25\n",
    "    output_dir = f\"/mnt/hdd0/Kaggle/llm_prompt_recovery/trainings/2024-04-14_prompt_tuning/{experiment_name}\"\n",
    "    training_arguments = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            do_eval=True,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            per_device_train_batch_size=batch_size, # 4-16 should be fine for lora.\n",
    "            gradient_accumulation_steps=1,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            log_level=\"debug\",\n",
    "            save_steps=logging_steps, #50,\n",
    "            logging_steps=logging_steps//4, #50,\n",
    "            learning_rate=learning_rate, # maybe we can increase this\n",
    "            eval_steps=logging_steps, #50,\n",
    "            max_steps=max_steps, #300,\n",
    "            warmup_steps=0, #30 since we are training the prompt I don't believe we need warmup\n",
    "            lr_scheduler_type=\"linear\",\n",
    "    )\n",
    "    # https://docs.wandb.ai/guides/track/environment-variables\n",
    "    # https://stackoverflow.com/questions/71106179/log-two-model-runs-with-keras-wandb\n",
    "    w = wandb.init(reinit=True, project='prompt_tuning', name=experiment_name)\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(tokenizer=tokenizer, response_template=response_template)\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        data_collator=data_collator,\n",
    "        args=training_arguments,\n",
    "    )\n",
    "    trainer.train()\n",
    "    w.finish()\n",
    "\n",
    "    # logging.info('Making a few sample inferences')\n",
    "    # for text in train_df['text'].values[:5]:\n",
    "    #     print(chat_with_mixtral(text.split(response_template)[0] + response_template))\n",
    "    empty_gpu_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More experiments with my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_imitation_of_leaked_v1.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v2.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v3.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v4_with_hints.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v5_gpt4.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_multi_instruction_v1.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/mooney_test_with_gpt4_v2.csv',\n",
    "    '/mnt/hdd0/Kaggle/llm_prompt_recovery/data/gemma_suppl_rewrite_curated_with_gpt4.csv',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistral-7b'\n",
    "num_virtual_tokens = 8\n",
    "prompt_tuning_conf = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
    "    num_virtual_tokens=num_virtual_tokens, #Number of virtual tokens to be added and trained.\n",
    "    tokenizer_name_or_path=model_to_path[model_name] #The pre-trained model.\n",
    ")\n",
    "learning_rate = 1e-1\n",
    "experiment_name = f'{model_name}_prompt_tuning_vtk{num_virtual_tokens}_lr{learning_rate:.0e}_1000steps'\n",
    "train_model_on_datasets(\n",
    "    model_name,\n",
    "    filepaths,\n",
    "    experiment_name,\n",
    "    prompt_tuning_conf,\n",
    "    max_seq_length=640,\n",
    "    learning_rate=learning_rate,\n",
    "    max_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mixtral-8x7b'\n",
    "num_virtual_tokens = 8\n",
    "prompt_tuning_conf = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
    "    num_virtual_tokens=num_virtual_tokens, #Number of virtual tokens to be added and trained.\n",
    "    tokenizer_name_or_path=model_to_path[model_name] #The pre-trained model.\n",
    ")\n",
    "learning_rate = 5e-2\n",
    "experiment_name = f'{model_name}_prompt_tuning_vtk{num_virtual_tokens}_lr{learning_rate:.0e}_1000steps'\n",
    "train_model_on_datasets(\n",
    "    model_name,\n",
    "    filepaths,\n",
    "    experiment_name,\n",
    "    prompt_tuning_conf,\n",
    "    max_seq_length=640,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=8,\n",
    "    max_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e-1 gives better results than 2e-2 learning rate and 1, so it is close to optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just 32k parameters. 131kB!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference with Mistral-7B\n",
    "\n",
    "```\n",
    " \"Find a way to motivate the team to exceed the quarterly goals by emphasizing the incentives and the team effort.\"\n",
    " \"Make the language more urgent and emphasize the importance of addressing any discrepancies or issues related to the acre allocations for the project as soon as possible to prevent any delays or setbacks.\"\n",
    " \"Rewrite this sentence in a more playful and engaging way, using metaphors and exaggeration to make the task of doing the dishes sound more exciting and adventurous.\"\n",
    " \"Rewrite this statement to focus on the positive aspects of the project timeline extension and frame it as an opportunity rather than a challenge.\"\n",
    " \"Critique the optimistic view on sustainability and discuss the challenges in implementing it effectively.\" This prompt would have encouraged the writer to explore the limitations and complexities of sustainability, as well as the need for systemic changes beyond technological solutions.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Predictions from `mistral-7b_prompt_tuning_vtk8_lr2e-03`, does not look good, the format is different and we can see a note at the end.\n",
    "\n",
    "```\n",
    " \"Find a way to motivate the team to exceed the quarterly goals by emphasizing the incentives and the team effort.\"\n",
    " \"Make the language more urgent and emphasize the importance of addressing any discrepancies or issues related to the acre allocations for the project as soon as possible to prevent any delays or setbacks.\"\n",
    " \"Rewrite this sentence in a more playful and engaging way, using metaphors and exaggeration to make the task of doing the dishes sound like an exciting adventure.\"\n",
    " \"Rewrite this statement to focus on the positive aspects of the project timeline extension and frame it as an opportunity rather than a challenge.\"\n",
    " \"Critique the optimistic view on sustainability and discuss the challenges in implementing it effectively.\" This prompt would have encouraged the writer to explore the limitations and complexities of sustainability, as well as the need for systemic changes beyond technological solutions.\n",
    "```\n",
    "\n",
    "Maybe I'm not using the model correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [x] Verify that I can clean the memory at the end of the training\n",
    "- [x] Weird message about too long texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
