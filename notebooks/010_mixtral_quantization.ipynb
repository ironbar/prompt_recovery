{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixtral quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I make Mixtral to work better if I don't quantize the gate layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] What if MoE does not deal correctly with quantization and should I leave some layers as they are?\n",
    "  - [ ] https://github.com/mobiusml/hqq/issues/2\n",
    "  - [ ] https://github.com/vllm-project/vllm/issues/2243\n",
    "  - [ ] I could try this on the forum fork\n",
    "  - [ ] Here we see again the Mixtral gates suggestion, although it is not the bit and bytes config.\n",
    "  - [ ] https://huggingface.co/docs/transformers/main_classes/quantization#transformers.QuantoConfig.modules_to_not_convert\n",
    "  - [ ] https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig\n",
    "  - [ ] llm_int8_skip_modules might be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 10:07:57.697332: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-09 10:07:58.111997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-09 10:07:59.484901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    pipeline, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    "    llm_int8_enable_fp32_cpu_offload= True,\n",
    "    llm_int8_skip_modules=['gate', 'lm_head'],\n",
    "    #llm_int8_skip_modules=['gate', 'lm_head', 'w1'],\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2a2708388f46ecb629fd7399a4359b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=create_shared_device_map(16),\n",
    "    trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def chat_with_mixtral(prompt, max_new_tokens=200, verbose=True, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if not prompt.startswith('<s>[INST]'):\n",
    "        print('Formatting the prompt to Mixtral needs.')\n",
    "        prompt = f'<s>[INST] {prompt} [/INST]'\n",
    "    start = time.time()\n",
    "\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    #response = re.sub(r'[\\'\"]', '', response)\n",
    "    if verbose:\n",
    "        stop = time.time()\n",
    "        time_taken = stop-start\n",
    "        n_tokens = len(tokenizer.tokenize(response))\n",
    "        print(f\"Execution Time : {time_taken:.1f} s, tokens per second: {n_tokens/time_taken:.1f}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters(model, key=None):\n",
    "    for module_name, module in model.named_modules():\n",
    "        if key is not None and key not in module_name:\n",
    "            continue\n",
    "        total_params = 0\n",
    "        for param in module.parameters(recurse=False):\n",
    "            # Multiply dimensions of the parameter\n",
    "            total_params += param.numel()\n",
    "        if total_params > 0:\n",
    "            print(f\"{module_name} ({type(module).__name__}): {total_params:.1e} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 memory allocated: 11.7 GB, max memory allocated: 11.7 GB\n",
      "GPU 1 memory allocated: 11.7 GB, max memory allocated: 11.7 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe we should skip the quantization of the `(gate): Linear4bit(in_features=4096, out_features=8, bias=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to skip quantization of gate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 memory allocated: 11.7 GB, max memory allocated: 11.7 GB\n",
      "GPU 1 memory allocated: 11.5 GB, max memory allocated: 11.5 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear4bit(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to have worked! `(gate): Linear(in_features=4096, out_features=8, bias=False)`\n",
    "The weird thing is that the used memory is almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.1.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.2.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.3.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.4.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.5.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.6.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.7.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.8.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.9.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.10.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.11.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.12.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.13.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.14.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.15.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.16.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.17.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.18.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.19.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.20.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.21.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.22.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.23.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.24.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.25.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.26.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.27.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.28.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.29.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.30.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n",
      "model.layers.31.block_sparse_moe.gate (Linear): 3.3e+04 parameters\n"
     ]
    }
   ],
   "source": [
    "print_parameters(model, 'gate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.056"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(3.3e+04*32)/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all the gate layers have around 1M of parameters, (32 layers with 33k parameters each), that is tiny compared to the whole model. That is we don't see a big change on GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not quantize the head also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 memory allocated: 11.7 GB, max memory allocated: 11.7 GB\n",
      "GPU 1 memory allocated: 11.7 GB, max memory allocated: 11.7 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head (Linear): 1.3e+08 parameters\n"
     ]
    }
   ],
   "source": [
    "print_parameters(model, 'lm_head')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head has around 1e8 parameters, that will take 0.2GB of memory when using float16 and 0.05 when using float4. Thus the difference is just 0.15GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not quantize some heavy layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.424"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.9e+07*32*8/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.0.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.0.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.0.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.0.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.0.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.0.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.0.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.1.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.1.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.1.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.1.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.1.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.1.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.1.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.1.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.2.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.2.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.2.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.2.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.2.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.2.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.2.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.2.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.3.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.3.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.3.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.3.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.3.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.3.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.3.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.3.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.4.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.4.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.4.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.4.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.4.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.4.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.4.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.4.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.5.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.5.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.5.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.5.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.5.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.5.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.5.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.5.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.6.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.6.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.6.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.6.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.6.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.6.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.6.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.6.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.7.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.7.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.7.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.7.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.7.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.7.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.7.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.7.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.8.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.8.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.8.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.8.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.8.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.8.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.8.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.8.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.9.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.9.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.9.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.9.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.9.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.9.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.9.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.9.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.10.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.10.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.10.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.10.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.10.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.10.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.10.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.10.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.11.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.11.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.11.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.11.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.11.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.11.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.11.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.11.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.12.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.12.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.12.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.12.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.12.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.12.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.12.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.12.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.13.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.13.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.13.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.13.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.13.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.13.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.13.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.13.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.14.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.14.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.14.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.14.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.14.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.14.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.14.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.14.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.15.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.15.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.15.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.15.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.15.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.15.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.15.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.15.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.16.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.16.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.16.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.16.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.16.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.16.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.16.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.16.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.17.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.17.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.17.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.17.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.17.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.17.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.17.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.17.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.18.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.18.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.18.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.18.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.18.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.18.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.18.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.18.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.19.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.19.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.19.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.19.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.19.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.19.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.19.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.19.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.20.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.20.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.20.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.20.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.20.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.20.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.20.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.20.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.21.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.21.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.21.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.21.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.21.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.21.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.21.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.21.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.22.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.22.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.22.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.22.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.22.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.22.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.22.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.22.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.23.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.23.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.23.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.23.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.23.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.23.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.23.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.23.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.24.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.24.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.24.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.24.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.24.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.24.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.24.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.24.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.25.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.25.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.25.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.25.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.25.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.25.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.25.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.25.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.26.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.26.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.26.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.26.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.26.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.26.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.26.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.26.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.27.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.27.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.27.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.27.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.27.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.27.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.27.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.27.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.28.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.28.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.28.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.28.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.28.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.28.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.28.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.28.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.29.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.29.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.29.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.29.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.29.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.29.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.29.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.29.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.30.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.30.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.30.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.30.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.30.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.30.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.30.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.30.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.31.block_sparse_moe.experts.0.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.31.block_sparse_moe.experts.1.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.31.block_sparse_moe.experts.2.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.31.block_sparse_moe.experts.3.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.31.block_sparse_moe.experts.4.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.31.block_sparse_moe.experts.5.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.31.block_sparse_moe.experts.6.w1 (Linear4bit): 2.9e+07 parameters\n",
      "model.layers.31.block_sparse_moe.experts.7.w1 (Linear4bit): 2.9e+07 parameters\n"
     ]
    }
   ],
   "source": [
    "print_parameters(model, 'w1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0 memory allocated: 22.0 GB, max memory allocated: 22.0 GB\n",
      "GPU 1 memory allocated: 22.0 GB, max memory allocated: 22.0 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_gpu_memory()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works correctly! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have verified that it is possible to skip the quantization of some layers.\n",
    "\n",
    "If we don't quantize the gates and the lm_head of Mixtral the memory usage is almost the same since they are small layers.\n",
    "\n",
    "Just add `llm_int8_skip_modules=['gate', 'lm_head'],` to the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
