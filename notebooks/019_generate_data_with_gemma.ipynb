{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data with Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train other models such as Phi-2 and Gemma and see the validation loss and leaderboard score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import yaml\n",
    "import os\n",
    "import hashlib\n",
    "import glob\n",
    "\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
    "    pipeline, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_path = {\n",
    "    'mistral-7b': '/home/gbarbadillo/data/Mistral-7B-Instruct-v0.2',\n",
    "    'mixtral-8x7b': '/home/gbarbadillo/data/mixtral-8x7b-instruct-v0.1-hf',\n",
    "    'llama-13b': '/home/gbarbadillo/data/llama2-13b-chat-hf',\n",
    "    'phi-2': '/home/gbarbadillo/data/phi-2',\n",
    "    'gemma-2b': '/home/gbarbadillo/data/gemma-2b-it',\n",
    "    'gemma-7b': '/home/gbarbadillo/data/gemma-7b-it',\n",
    "    'mistral-22b': '/home/gbarbadillo/data/Mistral-22B-v0.2',\n",
    "}\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    logging.info(f'Loading {model_name}...')\n",
    "    model_path = model_to_path[model_name]\n",
    "\n",
    "    if model_name in ['phi-2', 'gemma-2b', 'gemma-7b']:\n",
    "        bnb_config = None\n",
    "    else:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit= True,\n",
    "            bnb_4bit_quant_type= \"nf4\",\n",
    "            bnb_4bit_compute_dtype= torch.float16,\n",
    "            bnb_4bit_use_double_quant= True,\n",
    "            llm_int8_enable_fp32_cpu_offload= True,\n",
    "            llm_int8_skip_modules=['gate', 'lm_head'],\n",
    "        )\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=get_device_map(model_name),\n",
    "        trust_remote_code=True,)\n",
    "    if model_name == 'llama-13b':\n",
    "        # quirks to update the model default configuration\n",
    "        model.config.pretraining_tp = 1 # otherwise the quantized model does not work\n",
    "        model.generation_config.temperature = None\n",
    "        model.generation_config.top_p = None\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True)\n",
    "    if 'gemma' not in model_name:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.padding_side = 'right' # by default is left, for training right seems to be better\n",
    "\n",
    "    gc.collect()\n",
    "    print_gpu_memory()\n",
    "    return model, tokenizer\n",
    "\n",
    "def empty_gpu_vram():\n",
    "    logging.info('Emptying GPU VRAM...')\n",
    "    global model, tokenizer, pipe\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print_gpu_memory()\n",
    "\n",
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')\n",
    "\n",
    "\n",
    "auto_device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "    'model.layers.0': 0,\n",
    "    'model.layers.1': 0,\n",
    "    'model.layers.2': 0,\n",
    "    'model.layers.3': 0,\n",
    "    'model.layers.4': 0,\n",
    "    'model.layers.5': 0,\n",
    "    'model.layers.6': 0,\n",
    "    'model.layers.7': 0,\n",
    "    'model.layers.8': 0,\n",
    "    'model.layers.9': 0,\n",
    "    'model.layers.10': 0,\n",
    "    'model.layers.11': 0,\n",
    "    'model.layers.12': 0,\n",
    "    'model.layers.13': 0,\n",
    "    'model.layers.14': 1,\n",
    "    'model.layers.15': 1,\n",
    "    'model.layers.16': 1,\n",
    "    'model.layers.17': 1,\n",
    "    'model.layers.18': 1,\n",
    "    'model.layers.19': 1,\n",
    "    'model.layers.20': 1,\n",
    "    'model.layers.21': 1,\n",
    "    'model.layers.22': 1,\n",
    "    'model.layers.23': 1,\n",
    "    'model.layers.24': 1,\n",
    "    'model.layers.25': 1,\n",
    "    'model.layers.26': 1,\n",
    "    'model.layers.27': 1,\n",
    "    'model.layers.28': 1,\n",
    "    'model.layers.29': 1,\n",
    "    'model.layers.30': 1,\n",
    "    'model.layers.31': 1,\n",
    "    'model.norm': 1,\n",
    "    'lm_head': 1\n",
    " }\n",
    "\n",
    "def create_shared_device_map(transition_layer):\n",
    "    shared_device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx <= transition_layer:\n",
    "            shared_device_map[key] = 0\n",
    "        else:\n",
    "            shared_device_map[key] = 1\n",
    "    return shared_device_map\n",
    "\n",
    "def create_intertwined_device_map():\n",
    "    device_map = {}\n",
    "    for idx, key in enumerate(auto_device_map):\n",
    "        if idx == 0:\n",
    "            device_map[key] = 1\n",
    "        elif idx >= 33:\n",
    "            device_map[key] = 0\n",
    "        else:\n",
    "            device_map[key] = idx % 2\n",
    "    return device_map\n",
    "\n",
    "\n",
    "def get_device_map(model_name):\n",
    "    if model_name == 'mixtral-8x7b':\n",
    "        return create_intertwined_device_map()\n",
    "    else:\n",
    "        return 'auto'\n",
    "\n",
    "assert get_device_map('foo') == 'auto'\n",
    "assert get_device_map('mixtral-8x7b') != 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_mixtral(prompt, max_new_tokens=400, do_sample=False, temperature=0.7, top_p=0.95):\n",
    "    if do_sample:\n",
    "        sampling_kwargs = dict(do_sample=True, temperature=temperature, top_p=top_p)\n",
    "    else:\n",
    "        sampling_kwargs = dict(do_sample=False)\n",
    "\n",
    "    sequences = pipe(\n",
    "        prompt ,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/184g120/mistral_fine_tuning_eos_and_padding/\n",
    "        # https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/106\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **sampling_kwargs,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    response = sequences[0]['generated_text']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens from different models:\n",
    "\n",
    "```python\n",
    "# gemma\n",
    "{'bos_token': '<bos>',\n",
    " 'eos_token': '<eos>',\n",
    " 'unk_token': '<unk>',\n",
    " 'pad_token': '<pad>',\n",
    " 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}\n",
    "# phi-2\n",
    "{'bos_token': '<|endoftext|>',\n",
    " 'eos_token': '<|endoftext|>',\n",
    " 'unk_token': '<|endoftext|>'}\n",
    "# mistral-7b\n",
    "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
    "\n",
    "# llama-13b\n",
    "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"The most likely text prompt given to transform the original text into the rewritten text was:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_template_v1 = \"\"\"<start_of_turn>user\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, Update etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "The most likely text prompt given to transform the original text into the rewritten text was: {response}\n",
    "<end_of_turn><eos>\"\"\"\n",
    "assert response_template in gemma_template_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_template_v1 = \"\"\"Instruct: Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, Update etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "Output: The most likely text prompt given to transform the original text into the rewritten text was: {response} <|endoftext|>\"\"\"\n",
    "assert response_template in phi_template_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_template = \"\"\"<s>[INST][prompt-recovery]\n",
    "Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, Update etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "[/INST] The most likely text prompt given to transform the original text into the rewritten text was: {response} </s>\"\"\"\n",
    "assert response_template in mistral_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guanaco_template = \"\"\"<s>### System: You are a helpful assistant.\n",
    "### Human: Analyze the original and rewritten text and answer with the most likely text prompt that was given to rewrite or make stylistic changes to the original text.\n",
    "\n",
    "- The text prompt should be a single sentence. Reply just with a short sentence and do not add any notes or comments.\n",
    "- Sometimes the rewritten text will have hints about the text prompt. For example if it starts by\n",
    "  Reworded, Rephrased, Translated, Update etc. you should include that word in the text prompt.\n",
    "- Unless necessary do not make reference to details in the original text and keep the text prompt abstract and generic.\n",
    "\n",
    "## Original text\n",
    "\n",
    "{original_text}\n",
    "\n",
    "## Rewritten text\n",
    "\n",
    "{rewritten_text}\n",
    "\n",
    "### Assistant: The most likely text prompt given to transform the original text into the rewritten text was: {response} </s>\"\"\"\n",
    "assert response_template in mistral_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_template = \"\"\"<start_of_turn>user\n",
    "{rewrite_prompt}\n",
    "\n",
    "{original_text}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer('gemma-7b')\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gemma_dataset(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    responses = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        prompt = gemma_template.format(rewrite_prompt=row['rewrite_prompt'], original_text=row['original_text'])\n",
    "        print(prompt)\n",
    "        response = chat_with_mixtral(prompt)\n",
    "        responses.append(response)\n",
    "        print('-'*80)\n",
    "        print(response)\n",
    "        print('-'*80)\n",
    "        print(row['rewritten_text'])\n",
    "        print('='*80 + '\\n\\n')\n",
    "    df['rewritten_text'] = responses\n",
    "    df.to_csv(filepath.replace('.csv', '_gemma.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gemma_dataset('/mnt/hdd0/Kaggle/llm_prompt_recovery/data/high_quality_dataset_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incredibly slow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
